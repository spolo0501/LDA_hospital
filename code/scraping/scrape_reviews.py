#!/usr/bin/env python3
"""
Google Maps è©•è«–æ™ºèƒ½æŠ“å–å™¨ - åŒ…è£å™¨
æ•´åˆåˆ° LDA_hospital å°ˆæ¡ˆï¼Œè‡ªå‹•ç®¡ç†ç›®éŒ„çµæ§‹

ä½¿ç”¨ç¯„ä¾‹ï¼š
    python3 scrape_reviews.py \\
        --url "Google Maps URL" \\
        --name "Hospital_Name" \\
        --category hospitals \\
        --region usa \\
        --max-pages 100
"""

import os
import sys
import argparse
from datetime import datetime
from pathlib import Path
import json

# åŠ å…¥æ ¸å¿ƒçˆ¬èŸ²æ¨¡çµ„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from google_review_scraper import GoogleReviewsScraper


class IntelligentReviewScraper:
    """æ™ºèƒ½è©•è«–æŠ“å–å™¨ - èˆ‡å°ˆæ¡ˆç›®éŒ„çµæ§‹æ•´åˆ"""

    # å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼ˆè‡ªå‹•åµæ¸¬ï¼‰
    PROJECT_ROOT = Path(__file__).parent.parent.parent

    # æ”¯æ´çš„è³‡æ–™é¡å‹
    CATEGORIES = [
        'hospitals', 'museums', 'airports', 'restaurants',
        'hotels', 'universities', 'shopping_malls', 'tourist_attractions'
    ]

    # æ”¯æ´çš„åœ°å€
    REGIONS = [
        'taiwan', 'usa', 'uk', 'japan', 'china',
        'asia', 'europe', 'north_america'
    ]

    # èªè¨€å°æ‡‰è¡¨
    LANGUAGE_MAP = {
        'taiwan': 'zh-TW',
        'china': 'zh-CN',
        'japan': 'ja',
        'usa': 'en',
        'uk': 'en',
        'asia': 'en',
        'europe': 'en',
        'north_america': 'en'
    }

    # åœ°å€ä»£ç¢¼å°æ‡‰è¡¨
    REGION_CODE_MAP = {
        'taiwan': 'tw',
        'china': 'cn',
        'japan': 'jp',
        'usa': 'us',
        'uk': 'uk',
        'asia': 'us',
        'europe': 'uk',
        'north_america': 'us'
    }

    def __init__(self, category: str, region: str, place_name: str, output_dir: str = None):
        """
        åˆå§‹åŒ–æ™ºèƒ½æŠ“å–å™¨

        Args:
            category: è³‡æ–™é¡å‹ (hospitals, museums, ç­‰)
            region: åœ°å€ (taiwan, usa, uk, ç­‰)
            place_name: åœ°é»åç¨± (ç”¨æ–¼æª”å)
            output_dir: è‡ªå®šç¾©è¼¸å‡ºç›®éŒ„ï¼ˆå¯é¸ï¼Œé è¨­ç‚ºå°ˆæ¡ˆçµæ§‹ï¼‰
        """
        if category not in self.CATEGORIES:
            raise ValueError(f"ä¸æ”¯æ´çš„ category: {category}ã€‚æ”¯æ´çš„é¡å‹: {', '.join(self.CATEGORIES)}")

        if region not in self.REGIONS:
            raise ValueError(f"ä¸æ”¯æ´çš„ region: {region}ã€‚æ”¯æ´çš„åœ°å€: {', '.join(self.REGIONS)}")

        self.category = category
        self.region = region
        self.place_name = self._sanitize_filename(place_name)

        # è¨­å®šèªè¨€å’Œåœ°å€ä»£ç¢¼
        self.language = self.LANGUAGE_MAP.get(region, 'en')
        self.region_code = self.REGION_CODE_MAP.get(region, 'us')

        # åˆå§‹åŒ–æ ¸å¿ƒçˆ¬èŸ²
        self.scraper = GoogleReviewsScraper(language=self.language, region=self.region_code)

        # è¨­å®šè¼¸å‡ºè·¯å¾‘
        if output_dir:
            # ä½¿ç”¨è‡ªå®šç¾©è¼¸å‡ºç›®éŒ„
            self.output_dir = Path(output_dir) / category / region
        else:
            # ä½¿ç”¨å°ˆæ¡ˆé è¨­çµæ§‹
            self.output_dir = self.PROJECT_ROOT / "data" / "raw" / category / region

        self.output_dir.mkdir(parents=True, exist_ok=True)

    def _sanitize_filename(self, name: str) -> str:
        """æ¸…ç†æª”æ¡ˆåç¨±ï¼Œç§»é™¤éæ³•å­—å…ƒ"""
        # ç§»é™¤æˆ–æ›¿æ›éæ³•å­—å…ƒ
        name = name.replace('/', '_').replace('\\', '_')
        name = name.replace(':', '_').replace('*', '_')
        name = name.replace('?', '_').replace('"', '_')
        name = name.replace('<', '_').replace('>', '_')
        name = name.replace('|', '_').replace(' ', '_')
        return name

    def _generate_output_paths(self) -> dict:
        """ç”Ÿæˆè¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆä¸å«æ™‚é–“æˆ³ï¼Œé¿å…é‡è¤‡æŠ“å–ï¼‰"""
        base_filename = self.place_name

        return {
            'csv': self.output_dir / f"{base_filename}.csv",
            'json': self.output_dir / f"{base_filename}.json",
            'stats': self.output_dir / f"{base_filename}_stats.csv",
            'report': self.output_dir / f"{base_filename}_report.txt"
        }

    def scrape(self, url: str, max_pages: int = 100, per_page: int = 20, delay: float = 2.0) -> dict:
        """
        åŸ·è¡ŒæŠ“å–

        Args:
            url: Google Maps URL æˆ– Place ID
            max_pages: æœ€å¤§æŠ“å–é æ•¸
            per_page: æ¯é è©•è«–æ•¸
            delay: æ¯é é–“å»¶é²ç§’æ•¸

        Returns:
            æŠ“å–çµæœå­—å…¸
        """
        print("\n" + "=" * 70)
        print("ğŸ¥ æ™ºèƒ½è©•è«–æŠ“å–å™¨")
        print("=" * 70)
        print(f"ğŸ“‚ è³‡æ–™é¡å‹: {self.category}")
        print(f"ğŸŒ åœ°å€: {self.region}")
        print(f"ğŸ“ åœ°é»: {self.place_name}")
        print(f"ğŸ—‚ï¸  è¼¸å‡ºç›®éŒ„: {self.output_dir}")
        print(f"ğŸŒ èªè¨€: {self.language}, åœ°å€ä»£ç¢¼: {self.region_code}")
        print("=" * 70 + "\n")

        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
        paths = self._generate_output_paths()
        if paths['csv'].exists():
            file_size = paths['csv'].stat().st_size
            print(f"âš ï¸  æª”æ¡ˆå·²å­˜åœ¨: {paths['csv'].name}")
            print(f"ğŸ“¦ æª”æ¡ˆå¤§å°: {file_size / 1024:.1f} KB")
            print(f"â­ï¸  è·³éæŠ“å–ï¼ˆé¿å…é‡è¤‡ï¼‰\n")
            return {
                "status": "skipped",
                "reason": "file_already_exists",
                "paths": {k: str(v) for k, v in paths.items()}
            }

        # åŸ·è¡ŒæŠ“å–
        results = self.scraper.scrape_all_reviews(
            place_identifier=url,
            max_pages=max_pages,
            reviews_per_page=per_page,
            delay=delay
        )

        if "error" in results:
            print(f"\nâŒ æŠ“å–å¤±æ•—: {results['error']}")
            return results

        # å„²å­˜æª”æ¡ˆ
        print(f"\nğŸ’¾ æ­£åœ¨å„²å­˜æª”æ¡ˆ...")
        self.scraper.save_to_csv(results, str(paths['csv']))
        self.scraper.save_to_json(results, str(paths['json']))

        # ç”ŸæˆæŠ“å–å ±å‘Š
        self._generate_report(results, paths['report'])

        print(f"\nâœ… æŠ“å–å®Œæˆï¼")
        print(f"\nğŸ“ ç”Ÿæˆçš„æª”æ¡ˆ:")
        print(f"   â€¢ CSV:    {paths['csv']}")
        print(f"   â€¢ JSON:   {paths['json']}")
        print(f"   â€¢ Stats:  {paths['stats']}")
        print(f"   â€¢ Report: {paths['report']}")

        return {
            "status": "success",
            "results": results,
            "paths": {k: str(v) for k, v in paths.items()}
        }

    def _generate_report(self, results: dict, report_path: Path):
        """ç”ŸæˆæŠ“å–å ±å‘Š"""
        reviews = results.get("reviews", [])
        avg_rating = sum(r.get("rating", 0) for r in reviews) / len(reviews) if reviews else 0

        report = f"""
{'='*70}
Google Maps è©•è«–æŠ“å–å ±å‘Š
{'='*70}

ğŸ“Š åŸºæœ¬è³‡è¨Š
{'â”€'*70}
è³‡æ–™é¡å‹:      {self.category}
åœ°å€:          {self.region}
åœ°é»:          {self.place_name}
æŠ“å–æ™‚é–“:      {results.get('timestamp', 'N/A')}

ğŸ“ˆ æŠ“å–çµ±è¨ˆ
{'â”€'*70}
ç¸½è©•è«–æ•¸:      {results.get('total_reviews', 0)}
æˆåŠŸé æ•¸:      {results.get('pages_fetched', 0)}
è€—æ™‚:          {results.get('scraping_duration', 0):.2f} ç§’
å¹³å‡è©•åˆ†:      {avg_rating:.2f} æ˜Ÿ

ğŸ“ è©•è«–å…§å®¹çµ±è¨ˆ
{'â”€'*70}
æœ‰æ–‡å­—è©•è«–:    {len([r for r in reviews if r.get('review_text', '')])}
æœ‰ç…§ç‰‡è©•è«–:    {len([r for r in reviews if r.get('photos_count', 0) > 0])}

â­ è©•åˆ†åˆ†å¸ƒ
{'â”€'*70}
5æ˜Ÿ: {len([r for r in reviews if r.get('rating') == 5])} ({len([r for r in reviews if r.get('rating') == 5])/len(reviews)*100:.1f}%)
4æ˜Ÿ: {len([r for r in reviews if r.get('rating') == 4])} ({len([r for r in reviews if r.get('rating') == 4])/len(reviews)*100:.1f}%)
3æ˜Ÿ: {len([r for r in reviews if r.get('rating') == 3])} ({len([r for r in reviews if r.get('rating') == 3])/len(reviews)*100:.1f}%)
2æ˜Ÿ: {len([r for r in reviews if r.get('rating') == 2])} ({len([r for r in reviews if r.get('rating') == 2])/len(reviews)*100:.1f}%)
1æ˜Ÿ: {len([r for r in reviews if r.get('rating') == 1])} ({len([r for r in reviews if r.get('rating') == 1])/len(reviews)*100:.1f}%)

ğŸ“ æª”æ¡ˆä½ç½®
{'â”€'*70}
è¼¸å‡ºç›®éŒ„:      {self.output_dir}
CSVæª”æ¡ˆ:       {self.place_name}_*.csv
JSONæª”æ¡ˆ:      {self.place_name}_*.json

{'='*70}
æŠ“å–å®Œæˆ âœ…
{'='*70}
"""

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"ğŸ“„ å ±å‘Šå·²ç”Ÿæˆ: {report_path}")


def main():
    """å‘½ä»¤åˆ—ä»‹é¢"""
    parser = argparse.ArgumentParser(
        description="æ™ºèƒ½ Google Maps è©•è«–æŠ“å–å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¯„ä¾‹:

  # æŠ“å–ç¾åœ‹é†«é™¢è©•è«–
  python3 scrape_reviews.py \\
      --url "https://www.google.com/maps/place/..." \\
      --name "Mayo_Clinic" \\
      --category hospitals \\
      --region usa \\
      --max-pages 100

  # æŠ“å–å°ç£åšç‰©é¤¨è©•è«–
  python3 scrape_reviews.py \\
      --url "https://www.google.com/maps/place/..." \\
      --name "National_Palace_Museum" \\
      --category museums \\
      --region taiwan \\
      --max-pages 50

æ”¯æ´çš„è³‡æ–™é¡å‹ (category):
  hospitals, museums, airports, restaurants, hotels, universities,
  shopping_malls, tourist_attractions

æ”¯æ´çš„åœ°å€ (region):
  taiwan, usa, uk, japan, china, asia, europe, north_america
        """
    )

    parser.add_argument("--url", required=True, help="Google Maps URL æˆ– Place ID")
    parser.add_argument("--name", required=True, help="åœ°é»åç¨± (ç”¨æ–¼æª”å)")
    parser.add_argument("--category", required=True, choices=IntelligentReviewScraper.CATEGORIES,
                       help="è³‡æ–™é¡å‹")
    parser.add_argument("--region", required=True, choices=IntelligentReviewScraper.REGIONS,
                       help="åœ°å€")
    parser.add_argument("--output-dir", type=str, default=None,
                       help="è‡ªå®šç¾©è¼¸å‡ºç›®éŒ„ï¼ˆå¯é¸ï¼Œé è¨­ç‚º ./google_reviews_output/ï¼‰")
    parser.add_argument("--max-pages", type=int, default=100, help="æœ€å¤§æŠ“å–é æ•¸ (é è¨­: 100)")
    parser.add_argument("--per-page", type=int, default=20, help="æ¯é è©•è«–æ•¸ (é è¨­: 20)")
    parser.add_argument("--delay", type=float, default=2.0, help="æ¯é é–“å»¶é²ç§’æ•¸ (é è¨­: 2.0)")

    args = parser.parse_args()

    try:
        # å¦‚æœæ²’æœ‰æŒ‡å®šè¼¸å‡ºç›®éŒ„ï¼Œä½¿ç”¨ç•¶å‰ç›®éŒ„çš„ google_reviews_output
        output_dir = args.output_dir if args.output_dir else "./google_reviews_output"

        # åˆå§‹åŒ–æ™ºèƒ½æŠ“å–å™¨
        scraper = IntelligentReviewScraper(
            category=args.category,
            region=args.region,
            place_name=args.name,
            output_dir=output_dir
        )

        # åŸ·è¡ŒæŠ“å–
        result = scraper.scrape(
            url=args.url,
            max_pages=args.max_pages,
            per_page=args.per_page,
            delay=args.delay
        )

        if result.get("status") == "success":
            print("\nğŸ‰ æ‰€æœ‰ä»»å‹™å®Œæˆï¼")
            sys.exit(0)
        else:
            print("\nâŒ æŠ“å–å¤±æ•—")
            sys.exit(1)

    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
