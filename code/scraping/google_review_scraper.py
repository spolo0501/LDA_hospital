#!/usr/bin/env python3
"""
Google Maps è©•è«–æŠ“å–å™¨ - æ ¸å¿ƒæ¨¡çµ„
æ”¹ç·¨è‡ª /Users/simon/Downloads/Claude_code/GoogleReviews/csv_google_reviews_scraper.py

ç”¨æ–¼æŠ“å– Google Maps è©•è«–ä¸¦å„²å­˜ç‚º CSV å’Œ JSON æ ¼å¼
"""

import requests
import json
import time
import re
import csv
import base64
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import unquote
from langdetect import detect, LangDetectException


class GoogleReviewsScraper:
    """Google Maps è©•è«–æŠ“å–å™¨æ ¸å¿ƒé¡åˆ¥"""

    def __init__(self, language: str = "en", region: str = "us"):
        """
        åˆå§‹åŒ–æŠ“å–å™¨

        Args:
            language: èªè¨€è¨­å®š (zh-TW, en, ja ç­‰)
            region: åœ°å€è¨­å®š (tw, us, uk ç­‰)
        """
        self.base_url = "https://www.google.com/maps/rpc/listugcposts"
        self.language = language
        self.region = region
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "*/*",
            "Accept-Language": f"{language},zh;q=0.9,en;q=0.8",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache",
            "Referer": "https://www.google.com/maps/",
            "X-Requested-With": "XMLHttpRequest"
        }

    def extract_place_id_from_url(self, google_maps_url: str) -> Optional[str]:
        """
        å¾ Google Maps URL æå– Place ID

        Args:
            google_maps_url: Google Maps URL (æ”¯æ´æ¨™æº– URL å’Œ cid æ ¼å¼)

        Returns:
            Place ID (æ ¼å¼: 0x123:0x456) æˆ– None
        """
        print(f"ğŸ” åˆ†æ URL...")
        decoded_url = unquote(google_maps_url)

        # æª¢æŸ¥æ˜¯å¦ç‚º cid æ ¼å¼
        cid_pattern = r'[?&]cid=(\d+)'
        cid_match = re.search(cid_pattern, decoded_url)
        if cid_match:
            cid = cid_match.group(1)
            # å°‡ cid è½‰æ›ç‚ºåå…­é€²ä½æ ¼å¼
            cid_hex = hex(int(cid))[2:]  # ç§»é™¤ '0x' å‰ç¶´
            # Google Maps ä½¿ç”¨çš„æ ¼å¼é€šå¸¸æ˜¯å…©éƒ¨åˆ†ï¼Œä½† cid åªæœ‰ä¸€å€‹å€¼
            # æˆ‘å€‘å¯ä»¥å˜—è©¦ç›´æ¥ä½¿ç”¨ cid å€¼
            place_id = f"0x0:0x{cid_hex}"
            print(f"âœ… å¾ cid æå–åœ°é» ID: {place_id} (cid={cid})")
            return place_id

        # æ¨™æº– Place ID æ ¼å¼
        patterns = [
            r'!1s(0x[0-9a-f]+:0x[0-9a-f]+)',
            r'1s(0x[0-9a-f]+:0x[0-9a-f]+)',
            r'!3m8!1s(0x[0-9a-f]+:0x[0-9a-f]+)',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, decoded_url)
            if matches:
                place_id = matches[0]
                print(f"âœ… æˆåŠŸæå–åœ°é» ID: {place_id}")
                return place_id

        print("âŒ ç„¡æ³•å¾ URL æå–åœ°é» ID")
        return None

    def build_params(self, place_id: str, page_token: str = "", review_count: int = 10) -> dict:
        """æ§‹å»º API è«‹æ±‚åƒæ•¸"""
        pb_param = f"!1m6!1s{place_id}!6m4!4m1!1e1!4m1!1e3!2m2!1i{review_count}!2s{page_token}!5m2!1s-kDaaLrLL8a_vr0PnrbygAI!7e81!8m9!2b1!3b1!5b1!7b1!12m4!1b1!2b1!4m1!1e1!11m0!13m1!1e2"

        return {
            "authuser": "0",
            "hl": self.language,
            "gl": self.region,
            "pb": pb_param
        }

    def fetch_reviews_page(self, place_id: str, page_token: str = "", review_count: int = 10) -> Optional[Dict]:
        """
        ç²å–å–®é è©•è«–æ•¸æ“š

        Args:
            place_id: åœ°é» ID
            page_token: åˆ†é  token
            review_count: æ¯é è©•è«–æ•¸

        Returns:
            åŒ…å«è©•è«–æ•¸æ“šçš„å­—å…¸æˆ– None
        """
        params = self.build_params(place_id, page_token, review_count)

        try:
            response = requests.get(self.base_url, params=params, headers=self.headers, timeout=30)

            if response.status_code == 200:
                response_text = response.text
                if response_text.startswith(')]}\''):
                    json_text = response_text[4:]
                    data = json.loads(json_text)

                    # æª¢æŸ¥éŒ¯èª¤éŸ¿æ‡‰
                    if isinstance(data, list) and len(data) > 0:
                        first_item = data[0]
                        if isinstance(first_item, list) and len(first_item) > 0 and first_item[0] == "er":
                            return {
                                "status": "error",
                                "message": "API è¿”å›éŒ¯èª¤ï¼Œå¯èƒ½æ˜¯åœ°é» ID ç„¡æ•ˆæˆ–ç„¡è©•è«–æ•¸æ“š"
                            }

                    return {
                        "status": "success",
                        "next_page_token": data[1] if len(data) > 1 and data[1] else None,
                        "reviews": data[2] if len(data) > 2 else [],
                        "raw_response": data
                    }

            return {
                "status": "error",
                "message": f"HTTP {response.status_code}"
            }

        except Exception as e:
            return {
                "status": "error",
                "message": str(e)
            }

    def find_in_data(self, data, target_type, condition=None):
        """åœ¨åµŒå¥—æ•¸æ“šä¸­æŸ¥æ‰¾ç¬¦åˆæ¢ä»¶çš„å€¼"""
        results = []

        def search_recursive(obj):
            if isinstance(obj, target_type):
                if condition is None or condition(obj):
                    results.append(obj)
            elif isinstance(obj, (list, tuple)):
                for item in obj:
                    search_recursive(item)
            elif isinstance(obj, dict):
                for value in obj.values():
                    search_recursive(value)

        search_recursive(data)
        return results

    def parse_review_data(self, review_raw: List) -> Dict:
        """
        è§£æè©•è«–æ•¸æ“š

        Args:
            review_raw: åŸå§‹è©•è«–æ•¸æ“šï¼ˆåµŒå¥—åˆ—è¡¨ï¼‰

        Returns:
            è§£æå¾Œçš„è©•è«–å­—å…¸
        """
        review = {
            "review_id": "",
            "author_name": "Unknown User",
            "rating": 0,
            "review_text": "",
            "review_date": "",
            "photos_count": 0,
            "likes_count": 0,
            "language": "unknown",
        }

        if not review_raw or not isinstance(review_raw, list) or len(review_raw) == 0:
            return review

        try:
            # Google Maps API è³‡æ–™çµæ§‹ï¼š
            # review_raw[0] - ä¸»è¦è©•è«–è³‡æ–™
            # review_raw[1] - null
            # review_raw[2] - ä¸‹ä¸€é  token

            main_data = review_raw[0] if len(review_raw) > 0 else None
            if not main_data or not isinstance(main_data, list):
                return review

            # æå–è©•è«– ID
            if len(main_data) > 0 and isinstance(main_data[0], str):
                review["review_id"] = f"r_{hash(main_data[0]) % 1000000}"

            # æå–ä½œè€…å’Œæ™‚é–“è³‡è¨Š - main_data[1]
            if len(main_data) > 1 and isinstance(main_data[1], list):
                author_data = main_data[1]

                # ä½œè€…åç¨± - author_data[4][5][0]
                try:
                    if (len(author_data) > 4 and isinstance(author_data[4], list) and
                        len(author_data[4]) > 5 and isinstance(author_data[4][5], list) and
                        len(author_data[4][5]) > 0 and isinstance(author_data[4][5][0], str)):
                        review["author_name"] = author_data[4][5][0]
                except (IndexError, TypeError):
                    pass

                # è©•è«–æ—¥æœŸ - author_data[6]
                try:
                    if len(author_data) > 6 and isinstance(author_data[6], str):
                        review["review_date"] = author_data[6]
                except (IndexError, TypeError):
                    pass

            # æå–è©•è«–å…§å®¹å’Œè©•åˆ† - main_data[2]
            if len(main_data) > 2 and isinstance(main_data[2], list):
                content_data = main_data[2]

                # è©•åˆ† - content_data[0][0]
                try:
                    if (len(content_data) > 0 and isinstance(content_data[0], list) and
                        len(content_data[0]) > 0 and isinstance(content_data[0][0], int)):
                        rating = content_data[0][0]
                        if 1 <= rating <= 5:
                            review["rating"] = rating
                except (IndexError, TypeError):
                    pass

                # è©•è«–æ–‡å­— - content_data[15][0][0]
                try:
                    if (len(content_data) > 15 and isinstance(content_data[15], list) and
                        len(content_data[15]) > 0 and isinstance(content_data[15][0], list) and
                        len(content_data[15][0]) > 0 and isinstance(content_data[15][0][0], str)):
                        review["review_text"] = content_data[15][0][0]
                except (IndexError, TypeError):
                    pass

            # è¨ˆç®—ç…§ç‰‡æ•¸é‡ï¼ˆæŸ¥æ‰¾ googleusercontent æˆ– ggpht.comï¼‰
            photo_count = len(self.find_in_data(
                review_raw,
                str,
                lambda s: 'googleusercontent' in s or 'ggpht.com' in s
            ))
            review["photos_count"] = photo_count

            # æŸ¥æ‰¾æŒ‰è®šæ•¸ï¼ˆæŸ¥æ‰¾å°çš„æ­£æ•´æ•¸ï¼‰
            like_candidates = self.find_in_data(review_raw, int, lambda x: 0 <= x <= 10000 and x not in [1,2,3,4,5])
            if like_candidates:
                review["likes_count"] = like_candidates[0]

            # èªè¨€æª¢æ¸¬
            if review["review_text"]:
                try:
                    detected_lang = detect(review["review_text"])
                    review["language"] = detected_lang
                except LangDetectException:
                    review["language"] = "unknown"
            else:
                review["language"] = "no_text"

        except Exception as e:
            print(f"âš ï¸ è§£æè©•è«–æ™‚é‡åˆ°éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()

        return review

    def scrape_all_reviews(self, place_identifier: str, max_pages: int = 10,
                          reviews_per_page: int = 10, delay: float = 2.0) -> Dict:
        """
        ä¸»è¦æŠ“å–å‡½æ•¸

        Args:
            place_identifier: Google Maps URL æˆ– Place ID
            max_pages: æœ€å¤§æŠ“å–é æ•¸
            reviews_per_page: æ¯é è©•è«–æ•¸
            delay: æ¯é é–“å»¶é²ç§’æ•¸

        Returns:
            åŒ…å«æ‰€æœ‰è©•è«–çš„å­—å…¸
        """
        # æå– Place ID
        if place_identifier.startswith("http"):
            place_id = self.extract_place_id_from_url(place_identifier)
            if not place_id:
                return {"error": "ç„¡æ³•å¾ URL æå–åœ°é» ID"}
        else:
            place_id = place_identifier

        print(f"ğŸš€ é–‹å§‹æŠ“å–åœ°é»è©•è«–...")
        print(f"ğŸ“ åœ°é» ID: {place_id}")
        print(f"ğŸ“Š é…ç½®: æœ€å¤§ {max_pages} é ï¼Œæ¯é  {reviews_per_page} æ¢ï¼Œå»¶é² {delay}s")

        all_reviews = []
        page_token = ""
        successful_pages = 0
        start_time = time.time()

        for page_count in range(1, max_pages + 1):
            print(f"ğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {page_count} é ...")

            result = self.fetch_reviews_page(place_id, page_token, reviews_per_page)

            if result["status"] == "error":
                print(f"âŒ ç¬¬ {page_count} é æŠ“å–å¤±æ•—: {result['message']}")
                if page_count == 1:
                    break
                continue

            reviews_data = result["reviews"]
            if not reviews_data:
                print(f"âœ… ç¬¬ {page_count} é ç„¡æ›´å¤šè©•è«–ï¼ŒæŠ“å–å®Œæˆ")
                break

            page_reviews = []
            for review_raw in reviews_data:
                if review_raw:
                    parsed_review = self.parse_review_data(review_raw)
                    if not parsed_review.get("error"):
                        page_reviews.append(parsed_review)

            all_reviews.extend(page_reviews)
            successful_pages += 1
            print(f"âœ… ç¬¬ {page_count} é å®Œæˆï¼Œç²å¾— {len(page_reviews)} æ¢è©•è«–")

            page_token = result["next_page_token"]
            if not page_token:
                print("âœ… å·²åˆ°é”æœ€å¾Œä¸€é ")
                break

            time.sleep(delay)

        end_time = time.time()
        duration = end_time - start_time

        return {
            "place_id": place_id,
            "total_reviews": len(all_reviews),
            "pages_fetched": successful_pages,
            "reviews": all_reviews,
            "scraping_duration": duration,
            "timestamp": datetime.now().isoformat()
        }

    def save_to_json(self, data: Dict, filename: str):
        """ä¿å­˜åˆ° JSON"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ JSON æ–‡ä»¶å·²ä¿å­˜: {filename}")

    def save_to_csv(self, data: Dict, filename: str):
        """
        ä¿å­˜åˆ° CSV æ–‡ä»¶

        Args:
            data: åŒ…å«è©•è«–çš„å­—å…¸
            filename: CSV æª”æ¡ˆè·¯å¾‘
        """
        try:
            reviews = data.get("reviews", [])
            if not reviews:
                print("âŒ æ²’æœ‰è©•è«–æ•¸æ“šå¯ä»¥ä¿å­˜åˆ° CSV")
                return

            # å®šç¾© CSV æ¬„ä½
            fieldnames = [
                "åºè™Ÿ",
                "è©•è«–ID",
                "ä½œè€…å§“å",
                "è©•åˆ†",
                "è©•è«–å…§å®¹",
                "è©•è«–æ—¥æœŸ",
                "ç…§ç‰‡æ•¸é‡",
                "æŒ‰è®šæ•¸",
                "èªè¨€"
            ]

            # å¯«å…¥ CSV
            with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                for i, review in enumerate(reviews, 1):
                    writer.writerow({
                        "åºè™Ÿ": i,
                        "è©•è«–ID": review.get("review_id", ""),
                        "ä½œè€…å§“å": review.get("author_name", ""),
                        "è©•åˆ†": review.get("rating", 0),
                        "è©•è«–å…§å®¹": review.get("review_text", ""),
                        "è©•è«–æ—¥æœŸ": review.get("review_date", ""),
                        "ç…§ç‰‡æ•¸é‡": review.get("photos_count", 0),
                        "æŒ‰è®šæ•¸": review.get("likes_count", 0),
                        "èªè¨€": review.get("language", "unknown")
                    })

            print(f"ğŸ“Š CSV æ–‡ä»¶å·²ä¿å­˜: {filename}")

            # çµ±è¨ˆè³‡è¨Š
            avg_rating = sum(r.get("rating", 0) for r in reviews) / len(reviews) if reviews else 0
            has_text_count = len([r for r in reviews if r.get("review_text", "")])
            has_photos_count = len([r for r in reviews if r.get("photos_count", 0) > 0])

            print(f"   åŒ…å« {len(reviews)} æ¢è©•è«–æ•¸æ“š")
            print(f"   å¹³å‡è©•åˆ†: {avg_rating:.2f} æ˜Ÿ")
            print(f"   æœ‰æ–‡å­—è©•è«–: {has_text_count} æ¢")
            print(f"   æœ‰ç…§ç‰‡è©•è«–: {has_photos_count} æ¢")

            # å‰µå»ºçµ±è¨ˆæª”æ¡ˆ
            stats_filename = filename.replace('.csv', '_stats.csv')
            with open(stats_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(["çµ±è¨ˆé …ç›®", "æ•¸å€¼"])
                writer.writerow(["åœ°é»ID", data.get("place_id", "")])
                writer.writerow(["ç¸½è©•è«–æ•¸", data.get("total_reviews", 0)])
                writer.writerow(["æŠ“å–é æ•¸", data.get("pages_fetched", 0)])
                writer.writerow(["æŠ“å–æ™‚é–“(ç§’)", f"{data.get('scraping_duration', 0):.2f}"])
                writer.writerow(["å¹³å‡è©•åˆ†", f"{avg_rating:.2f}"])
                writer.writerow(["æœ‰æ–‡å­—è©•è«–æ•¸", has_text_count])
                writer.writerow(["æœ‰ç…§ç‰‡è©•è«–æ•¸", has_photos_count])
                writer.writerow(["æŠ“å–æ—¥æœŸ", data.get("timestamp", "").split('T')[0]])

            print(f"ğŸ“ˆ çµ±è¨ˆæ–‡ä»¶å·²ä¿å­˜: {stats_filename}")

        except Exception as e:
            print(f"âŒ ä¿å­˜ CSV æ–‡ä»¶å¤±æ•—: {e}")
