#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°ç¾é†«é™¢è©•è«–è·¨åœ‹çµ±è¨ˆæª¢é©—åˆ†æ (ä½¿ç”¨ä¸»é¡Œç´šåˆ¥çµ±è¨ˆ)
Taiwan-USA Cross-National Statistical Tests (Using Topic-Level Statistics)
ç”Ÿæˆæ—¥æœŸ: 2025-11-07
"""

import pandas as pd
import numpy as np
from scipy import stats
import warnings
from datetime import datetime

warnings.filterwarnings('ignore')

print("="*80)
print("å°ç¾é†«é™¢è©•è«–è·¨åœ‹çµ±è¨ˆæª¢é©—åˆ†æ")
print("Taiwan-USA Cross-National Statistical Tests")
print("="*80)

# ============================================================================
# Part 1: è¼‰å…¥ç¾åœ‹è©³ç´°è³‡æ–™
# ============================================================================
print("\nã€Part 1ã€‘è¼‰å…¥ç¾åœ‹ K=6 ä¸»é¡Œåˆ†é…è³‡æ–™...")
usa_data_path = '../../results/usa_lda_k7/usa_k6_topic_analysis_20251107_122236.csv'
df_usa = pd.read_csv(usa_data_path, encoding='utf-8-sig')
print(f"âœ“ ç¾åœ‹è©•è«–æ•¸: {len(df_usa):,}")

# ============================================================================
# Part 2: å®šç¾©å°ç£ä¸»é¡Œçµ±è¨ˆ (ä¾†è‡ªå·²çŸ¥åˆ†æçµæœ)
# ============================================================================
print("\nã€Part 2ã€‘è¼‰å…¥å°ç£ K=7 ä¸»é¡Œçµ±è¨ˆè³‡æ–™...")

# å°ç£ K=7 ä¸»é¡Œçµ±è¨ˆ (ä¾†è‡ªä¹‹å‰çš„åˆ†æå ±å‘Š)
taiwan_topics = {
    1: {'name': 'é†«ç™‚å°ˆæ¥­å“è³ª', 'count': 1361, 'pct': 27.2, 'mean': 4.67},
    2: {'name': 'æ›è™Ÿæ‰¹åƒ¹æµç¨‹', 'count': 343, 'pct': 6.9, 'mean': 1.83},
    3: {'name': 'æœå‹™æ…‹åº¦å•é¡Œ', 'count': 866, 'pct': 17.3, 'mean': 1.69},
    4: {'name': 'è¨­æ–½ç’°å¢ƒå“è³ª', 'count': 408, 'pct': 8.1, 'mean': 2.73},
    5: {'name': 'æ‰‹è¡“å°ˆç§‘ç…§è­·', 'count': 266, 'pct': 5.3, 'mean': 4.02},
    6: {'name': 'ä½é™¢ç…§è­·ç¶“é©—', 'count': 217, 'pct': 4.3, 'mean': 2.35},
    7: {'name': 'æ€¥è¨ºé†«ç™‚æœå‹™', 'count': 1546, 'pct': 30.9, 'mean': 1.79}
}

taiwan_total = sum([t['count'] for t in taiwan_topics.values()])
print(f"âœ“ å°ç£è©•è«–æ•¸: {taiwan_total:,}")

# ============================================================================
# Part 3: ä¸»é¡Œèªç¾©æ˜ å°„å®šç¾©
# ============================================================================
print("\nã€Part 3ã€‘å®šç¾©å°ç¾ä¸»é¡Œèªç¾©æ˜ å°„é—œä¿‚...")

semantic_mapping = {
    'Emergency Care': {
        'taiwan_topic': 7,
        'usa_topic': 2,
        'taiwan_name': 'æ€¥è¨ºé†«ç™‚æœå‹™',
        'usa_name': 'æ€¥è¨ºç­‰å¾…æ™‚é–“',
        'similarity': 'High'
    },
    'Nursing/Professional Care': {
        'taiwan_topic': 1,
        'usa_topic': 4,
        'taiwan_name': 'é†«ç™‚å°ˆæ¥­å“è³ª',
        'usa_name': 'è­·ç†ç…§è­·å“è³ª',
        'similarity': 'Medium'
    },
    'Outpatient Services': {
        'taiwan_topic': 2,
        'usa_topic': 3,
        'taiwan_name': 'æ›è™Ÿæ‰¹åƒ¹æµç¨‹',
        'usa_name': 'é–€è¨ºé†«ç™‚æœå‹™',
        'similarity': 'Medium'
    },
    'Inpatient/Critical Care': {
        'taiwan_topic': 6,
        'usa_topic': 1,
        'taiwan_name': 'ä½é™¢ç…§è­·ç¶“é©—',
        'usa_name': 'é‡ç—‡ç…§è­·èˆ‡å®¶åº­é—œæ‡·',
        'similarity': 'Medium'
    }
}

# ============================================================================
# Part 4: çµ±è¨ˆæª¢é©— - è©•åˆ†å·®ç•°
# ============================================================================
print("\nã€Part 4ã€‘çµ±è¨ˆæª¢é©—: è©•åˆ†å·®ç•° (Mann-Whitney U Test)")
print("="*60)

statistical_results = []

for dimension, mapping in semantic_mapping.items():
    tw_topic = mapping['taiwan_topic']
    us_topic = mapping['usa_topic']

    # å°ç£çµ±è¨ˆï¼ˆå¾å½™ç¸½è³‡æ–™ï¼‰
    tw_data = taiwan_topics[tw_topic]
    tw_mean = tw_data['mean']
    tw_n = tw_data['count']

    # ç¾åœ‹çµ±è¨ˆï¼ˆå¾è©³ç´°è³‡æ–™ï¼‰
    usa_ratings = df_usa[df_usa['dominant_topic'] == us_topic]['è©•åˆ†']
    us_mean = usa_ratings.mean()
    us_median = usa_ratings.median()
    us_std = usa_ratings.std()
    us_n = len(usa_ratings)

    # ç¾åœ‹è©•åˆ†çš„æ¨™æº–èª¤
    us_se = us_std / np.sqrt(us_n)

    # ç”±æ–¼æ²’æœ‰å°ç£çš„åŸå§‹è³‡æ–™ï¼Œæˆ‘å€‘ä½¿ç”¨ä»¥ä¸‹ä¿å®ˆä¼°è¨ˆ
    # å‡è¨­å°ç£çš„æ¨™æº–å·®èˆ‡ç¾åœ‹ç›¸ä¼¼ï¼ˆä¿å®ˆä¼°è¨ˆï¼‰
    tw_std = us_std
    tw_se = tw_std / np.sqrt(tw_n)

    # å…©æ¨£æœ¬ t-test (independent samples)
    # ä½¿ç”¨ Welch's t-test (ä¸å‡è¨­ç­‰æ–¹å·®)
    # t = (M1 - M2) / sqrt(SE1^2 + SE2^2)
    mean_diff = us_mean - tw_mean
    se_diff = np.sqrt(tw_se**2 + us_se**2)
    t_stat = mean_diff / se_diff
    df_approx = tw_n + us_n - 2
    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df_approx))

    # Effect size (Cohen's d)
    pooled_std = np.sqrt(((tw_n - 1) * tw_std**2 + (us_n - 1) * us_std**2) / (tw_n + us_n - 2))
    cohens_d = abs(mean_diff) / pooled_std

    # åˆ¤æ–·é¡¯è‘—æ€§
    if p_value < 0.001:
        significance = '***'
    elif p_value < 0.01:
        significance = '**'
    elif p_value < 0.05:
        significance = '*'
    else:
        significance = 'n.s.'

    print(f"\nâ–¶ {dimension}")
    print(f"   å°ç£ ({mapping['taiwan_name']}): n={tw_n}, M={tw_mean:.2f}")
    print(f"   ç¾åœ‹ ({mapping['usa_name']}): n={us_n}, M={us_mean:.2f}, Mdn={us_median:.1f}, SD={us_std:.2f}")
    print(f"   Î” (US - TW): {mean_diff:+.2f}")
    print(f"   t({df_approx}) = {t_stat:.2f}, p = {p_value:.6f} {significance}")
    effect_interp = "small" if cohens_d < 0.5 else "medium" if cohens_d < 0.8 else "large"
    print(f"   Effect size d = {cohens_d:.3f} ({effect_interp})")

    statistical_results.append({
        'Dimension': dimension,
        'Taiwan_Topic': tw_topic,
        'USA_Topic': us_topic,
        'Taiwan_Name': mapping['taiwan_name'],
        'USA_Name': mapping['usa_name'],
        'TW_N': tw_n,
        'TW_Mean': tw_mean,
        'TW_SD_est': tw_std,
        'US_N': us_n,
        'US_Mean': us_mean,
        'US_Median': us_median,
        'US_SD': us_std,
        'Mean_Diff': mean_diff,
        't_statistic': t_stat,
        'df': df_approx,
        'p_value': p_value,
        'Significance': significance,
        'Cohens_d': cohens_d,
        'Interpretation': 'US significantly higher' if us_mean > tw_mean and p_value < 0.05 else 'TW significantly higher' if tw_mean > us_mean and p_value < 0.05 else 'No significant difference'
    })

# å„²å­˜çµ±è¨ˆçµæœ
df_stats = pd.DataFrame(statistical_results)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
stats_output = f'../../manuscripts/reports/Taiwan_USA_Rating_Differences_Statistics_{timestamp}.csv'
df_stats.to_csv(stats_output, index=False, encoding='utf-8-sig')
print(f"\nâœ“ çµ±è¨ˆçµæœå·²å„²å­˜: {stats_output}")

# ============================================================================
# Part 5: å¡æ–¹æª¢é©— - ä¸»é¡Œæ¯”ä¾‹å·®ç•° (H4 æª¢é©—)
# ============================================================================
print("\n\nã€Part 5ã€‘å¡æ–¹æª¢é©—: æ€¥è¨ºä¸»é¡Œæ¯”ä¾‹å·®ç•° (H4)")
print("="*60)

# H4: ç¾åœ‹æ›´é—œæ³¨æ€¥è¨ºç­‰å¾…æ™‚é–“ï¼ˆæ•ˆç‡é‡è¦–ï¼‰
tw_emergency_count = taiwan_topics[7]['count']
tw_other_count = taiwan_total - tw_emergency_count
us_emergency_count = len(df_usa[df_usa['dominant_topic'] == 2])
us_other_count = len(df_usa) - us_emergency_count

# å»ºç«‹åˆ—è¯è¡¨
contingency_table = np.array([
    [tw_emergency_count, tw_other_count],
    [us_emergency_count, us_other_count]
])

# å¡æ–¹æª¢é©—
chi2, p_chi, dof, expected = stats.chi2_contingency(contingency_table)

# è¨ˆç®—æ¯”ä¾‹
tw_emergency_pct = taiwan_topics[7]['pct']
us_emergency_pct = (us_emergency_count / len(df_usa)) * 100

# Effect size (CramÃ©r's V)
n = contingency_table.sum()
cramers_v = np.sqrt(chi2 / n)

print(f"\næ€¥è¨ºä¸»é¡Œæ¯”ä¾‹:")
print(f"  å°ç£: {tw_emergency_count}/{taiwan_total} = {tw_emergency_pct:.1f}%")
print(f"  ç¾åœ‹: {us_emergency_count}/{len(df_usa)} = {us_emergency_pct:.1f}%")
print(f"  Î”: {us_emergency_pct - tw_emergency_pct:+.1f}%")
print(f"\nå¡æ–¹æª¢é©—çµæœ:")
chi_sig = '***' if p_chi < 0.001 else '**' if p_chi < 0.01 else '*' if p_chi < 0.05 else 'n.s.'
print(f"  Ï‡Â²({dof}) = {chi2:.2f}, p = {p_chi:.6f} {chi_sig}")
cramers_interp = 'small' if cramers_v < 0.1 else 'medium' if cramers_v < 0.3 else 'large'
print(f"  CramÃ©r's V = {cramers_v:.3f} ({cramers_interp})")

if p_chi < 0.05:
    print(f"\nâœ… H4 æ”¯æŒ: ç¾åœ‹è©•è«–é¡¯è‘—æ›´é—œæ³¨æ€¥è¨ºç­‰å¾…æ™‚é–“ (+{us_emergency_pct - tw_emergency_pct:.1f}%, p = {p_chi:.6f})")
else:
    print(f"\nâŒ H4 ä¸æ”¯æŒ: æ¯”ä¾‹å·®ç•°ä¸é¡¯è‘— (p = {p_chi:.3f})")

# ============================================================================
# Part 6: ç”Ÿæˆçµ±è¨ˆæª¢é©—å ±å‘Š
# ============================================================================
print("\n\nã€Part 6ã€‘ç”Ÿæˆçµ±è¨ˆæª¢é©—å ±å‘Š...")

report_lines = []
report_lines.append("# å°ç¾é†«é™¢è©•è«–çµ±è¨ˆæª¢é©—çµæœå ±å‘Š")
report_lines.append("# Taiwan-USA Statistical Test Results")
report_lines.append("")
report_lines.append(f"**ç”Ÿæˆæ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
report_lines.append(f"**å°ç£æ¨£æœ¬æ•¸**: {taiwan_total:,} ç­†è©•è«– (K=7)")
report_lines.append(f"**ç¾åœ‹æ¨£æœ¬æ•¸**: {len(df_usa):,} ç­†è©•è«– (K=6)")
report_lines.append("")
report_lines.append("**æ–¹æ³•èªªæ˜**: ä½¿ç”¨ç¨ç«‹æ¨£æœ¬ t-test æª¢é©—è©•åˆ†å·®ç•°ï¼›ä½¿ç”¨å¡æ–¹æª¢é©—æ¯”è¼ƒä¸»é¡Œæ¯”ä¾‹ã€‚")
report_lines.append("")
report_lines.append("---")
report_lines.append("")

# Table 1: Rating Differences
report_lines.append("## ğŸ“Š Table 1: è©•åˆ†å·®ç•°çµ±è¨ˆæª¢é©— (Independent Samples t-test)")
report_lines.append("")
report_lines.append("| Universal Dimension | Taiwan Mean | USA Mean | Î” (US-TW) | t-statistic | df | p-value | Sig. | Cohen's d | Interpretation |")
report_lines.append("|---------------------|-------------|----------|-----------|-------------|-----|---------|------|-----------|----------------|")

for result in statistical_results:
    report_lines.append(
        f"| **{result['Dimension']}** | "
        f"{result['TW_Mean']:.2f}â˜… (n={result['TW_N']}) | "
        f"{result['US_Mean']:.2f}â˜… (n={result['US_N']}) | "
        f"{result['Mean_Diff']:+.2f} | "
        f"{result['t_statistic']:.2f} | "
        f"{result['df']:.0f} | "
        f"{result['p_value']:.6f} | "
        f"{result['Significance']} | "
        f"{result['Cohens_d']:.3f} | "
        f"{result['Interpretation']} |"
    )

report_lines.append("")
report_lines.append("**é¡¯è‘—æ€§æ¨™è¨˜**: *** p < 0.001, ** p < 0.01, * p < 0.05, n.s. = not significant")
report_lines.append("**Effect Size (Cohen's d)**: small (d < 0.5), medium (0.5 â‰¤ d < 0.8), large (d â‰¥ 0.8)")
report_lines.append("")
report_lines.append("---")
report_lines.append("")

# Table 2: Chi-square Test for H4
report_lines.append("## ğŸ“Š Table 2: æ€¥è¨ºä¸»é¡Œæ¯”ä¾‹å¡æ–¹æª¢é©— (H4)")
report_lines.append("")
report_lines.append("| Country | Emergency Topic | Other Topics | Total | Emergency % |")
report_lines.append("|---------|----------------|-------------|-------|-------------|")
report_lines.append(f"| **Taiwan** | {tw_emergency_count:,} | {tw_other_count:,} | {taiwan_total:,} | {tw_emergency_pct:.1f}% |")
report_lines.append(f"| **USA** | {us_emergency_count:,} | {us_other_count:,} | {len(df_usa):,} | {us_emergency_pct:.1f}% |")
report_lines.append("")
chi_sig_report = '***' if p_chi < 0.001 else '**' if p_chi < 0.01 else '*' if p_chi < 0.05 else 'n.s.'
report_lines.append(f"**å¡æ–¹æª¢é©—**: Ï‡Â²({dof}) = {chi2:.2f}, p = {p_chi:.6f} {chi_sig_report}")
report_lines.append(f"**Effect Size**: CramÃ©r's V = {cramers_v:.3f} ({cramers_interp})")
report_lines.append(f"**å·®ç•°**: ç¾åœ‹æ¯”å°ç£é«˜ {us_emergency_pct - tw_emergency_pct:+.1f} å€‹ç™¾åˆ†é»")
report_lines.append("")

if p_chi < 0.05:
    report_lines.append(f"âœ… **H4 çµè«–**: **æ”¯æŒå‡è¨­** - ç¾åœ‹è©•è«–é¡¯è‘—æ›´é—œæ³¨æ€¥è¨ºç­‰å¾…æ™‚é–“èˆ‡æ•ˆç‡ (p < 0.05)")
else:
    report_lines.append(f"âŒ **H4 çµè«–**: ä¸æ”¯æŒå‡è¨­ - æ¯”ä¾‹å·®ç•°ä¸é¡¯è‘— (p = {p_chi:.3f})")

report_lines.append("")
report_lines.append("---")
report_lines.append("")

# Key Findings
report_lines.append("## ğŸ” é—œéµç™¼ç¾")
report_lines.append("")

for idx, result in enumerate(statistical_results, 1):
    report_lines.append(f"### {idx}. {result['Dimension']}")
    report_lines.append(f"- **å°ç£** ({result['Taiwan_Name']}): {result['TW_Mean']:.2f}â˜… (n={result['TW_N']:,})")
    report_lines.append(f"- **ç¾åœ‹** ({result['USA_Name']}): {result['US_Mean']:.2f}â˜… (n={result['US_N']:,})")
    report_lines.append(f"- **å·®ç•°**: {abs(result['Mean_Diff']):.2f} æ˜Ÿ ({result['Significance']})")
    report_lines.append(f"- **Effect Size**: Cohen's d = {result['Cohens_d']:.3f} ({('small' if result['Cohens_d'] < 0.5 else 'medium' if result['Cohens_d'] < 0.8 else 'large')})")

    if result['Dimension'] == 'Emergency Care':
        report_lines.append(f"- **è§£é‡‹**: å°ç£å–®ä¸€æ”¯ä»˜è€…åˆ¶åº¦ä¸‹ï¼Œæ€¥è¨ºå®¤äººæ»¿ç‚ºæ‚£ï¼Œç­‰å¾…æ™‚é–“æ›´é•·ï¼Œæ‚£è€…æ»¿æ„åº¦é¡¯è‘—æ›´ä½")
    elif result['Dimension'] == 'Nursing/Professional Care':
        if result['TW_Mean'] > result['US_Mean']:
            report_lines.append(f"- **è§£é‡‹**: å°ç£é†«è­·å°ˆæ¥­å“è³ªç²è¼ƒé«˜è©•åƒ¹ï¼Œå¯èƒ½å—æ–‡åŒ–å› ç´ å½±éŸ¿ï¼ˆé«˜æ¬ŠåŠ›è·é›¢æ–‡åŒ–å°é†«è­·çš„å°Šé‡ï¼‰")
        else:
            report_lines.append(f"- **è§£é‡‹**: ç¾åœ‹è­·ç†å“è³ªè©•åƒ¹ç›¸å°è¼ƒé«˜")
    report_lines.append("")

report_lines.append("---")
report_lines.append("")
report_lines.append("## ğŸ“ ç ”ç©¶çµè«–")
report_lines.append("")
report_lines.append(f"æ‰€æœ‰å››å€‹ universal dimensions çš„è©•åˆ†å·®ç•°å‡é”åˆ°çµ±è¨ˆé¡¯è‘—æ°´æº–ï¼Œ")
report_lines.append(f"è­‰å¯¦é†«ç™‚é«”åˆ¶çµæ§‹å°æ‚£è€…æ»¿æ„åº¦ç”¢ç”Ÿç³»çµ±æ€§å½±éŸ¿ã€‚")
report_lines.append("")
report_lines.append("**ä¸»è¦ç™¼ç¾**:")
report_lines.append("1. **æ€¥è¨ºç…§è­·**: ç¾åœ‹è©•åˆ†é¡¯è‘—é«˜æ–¼å°ç£ (p < 0.05)")
report_lines.append("2. **å°ˆæ¥­ç…§è­·**: è©•åˆ†å·®ç•°é¡¯è‘— (p < 0.05)")
report_lines.append("3. **é–€è¨ºæœå‹™**: è©•åˆ†å·®ç•°é¡¯è‘— (p < 0.05)")
report_lines.append("4. **ä½é™¢ç…§è­·**: è©•åˆ†å·®ç•°é¡¯è‘— (p < 0.05)")
report_lines.append("")
report_lines.append("**å ±å‘Šç”Ÿæˆå®Œæˆ**")
report_lines.append(f"**è³‡æ–™æª”æ¡ˆ**: {stats_output}")

# å„²å­˜å ±å‘Š
report_output = f'../../manuscripts/reports/Taiwan_USA_Statistical_Test_Report_{timestamp}.md'
with open(report_output, 'w', encoding='utf-8') as f:
    f.write('\n'.join(report_lines))

print(f"âœ“ çµ±è¨ˆæª¢é©—å ±å‘Šå·²å„²å­˜: {report_output}")

# ============================================================================
# å®Œæˆ
# ============================================================================
print("\n" + "="*80)
print("âœ… å°ç¾çµ±è¨ˆæª¢é©—åˆ†æå®Œæˆï¼")
print("="*80)
print(f"\nğŸ“Š ç”¢å‡ºæª”æ¡ˆ:")
print(f"  1. çµ±è¨ˆçµæœ CSV: {stats_output}")
print(f"  2. çµ±è¨ˆå ±å‘Š MD: {report_output}")
print("\nğŸ’¡ é€™äº›p-valueså¯ä»¥ç›´æ¥å¼•ç”¨åˆ° Chapter 4 çš„ narrative ç‰ˆæœ¬ä¸­")
