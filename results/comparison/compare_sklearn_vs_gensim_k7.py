#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯”è¼ƒ scikit-learn LDA èˆ‡ Gensim LDA åœ¨ K=7 æ™‚çš„çµæœ
Compare scikit-learn LDA vs Gensim LDA with K=7
"""

import pickle
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# scikit-learn LDA
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Gensim LDA
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel
from gensim import corpora

# æ–‡æœ¬è™•ç†
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

print("="*80)
print("Comparison: scikit-learn LDA vs Gensim LDA (K=7)")
print("æ¯”è¼ƒï¼šscikit-learn LDA vs Gensim LDA (K=7)")
print("="*80)

# è¼‰å…¥è³‡æ–™
print("\nLoading data...")
df = pd.read_csv('cleaned_data_no_dedup/final_cleaned_sample_no_dedup.csv', encoding='utf-8-sig')
df_en = df[df['èªè¨€'] == 'en'].copy()
print(f"âœ“ Loaded {len(df_en):,} English reviews")

# æ–‡æœ¬å‰è™•ç†
print("\nPreprocessing text...")
stop_words = set(stopwords.words('english'))
custom_stop_words = {'hospital', 'doctor', 'went', 'like', 'would', 'one', 'get', 'go'}
stop_words.update(custom_stop_words)

def preprocess_english_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\S+@\S+', '', text)
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)
    text = ' '.join(text.split())
    return text

def tokenize_and_lemmatize(text, stop_words):
    lemmatizer = WordNetLemmatizer()
    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(token) for token in tokens
              if token not in stop_words and len(token) > 2]
    return tokens

df_en['cleaned_text'] = df_en['è©•è«–å…§å®¹'].apply(preprocess_english_text)
df_en['tokens'] = df_en['cleaned_text'].apply(lambda x: tokenize_and_lemmatize(x, stop_words))
df_en = df_en[df_en['tokens'].str.len() > 0].copy()

# æº–å‚™æ–‡æœ¬è³‡æ–™
texts = df_en['tokens'].tolist()
texts_str = [' '.join(tokens) for tokens in texts]  # For scikit-learn

print(f"âœ“ Valid reviews: {len(df_en):,}")

# ============================================================================
# æ–¹æ³•1: scikit-learn LDA
# ============================================================================
print("\n" + "="*80)
print("METHOD 1: scikit-learn LDA")
print("="*80)

# å‰µå»ºè©è¢‹æ¨¡å‹
print("\nCreating document-term matrix...")
sklearn_vectorizer = CountVectorizer(
    max_df=0.5,
    min_df=3,
    max_features=None,
    token_pattern=r'\b\w+\b'
)
sklearn_dtm = sklearn_vectorizer.fit_transform(texts_str)
sklearn_vocab = sklearn_vectorizer.get_feature_names_out()

print(f"  Vocabulary size: {len(sklearn_vocab)}")
print(f"  Document-term matrix shape: {sklearn_dtm.shape}")

# è¨“ç·´ scikit-learn LDA
print("\nTraining scikit-learn LDA with K=7...")
sklearn_lda = LatentDirichletAllocation(
    n_components=7,
    random_state=42,
    max_iter=100,
    learning_method='batch',
    n_jobs=1
)
sklearn_output = sklearn_lda.fit_transform(sklearn_dtm)

# è¨ˆç®— perplexity
sklearn_perplexity = sklearn_lda.perplexity(sklearn_dtm)

print(f"\nâœ“ scikit-learn LDA training complete!")
print(f"  Perplexity: {sklearn_perplexity:.4f}")

# æå–ä¸»é¡Œè©
print("\nscikit-learn Topic Keywords:")
sklearn_topics = []
for topic_idx, topic in enumerate(sklearn_lda.components_):
    top_indices = topic.argsort()[-10:][::-1]
    top_words = [sklearn_vocab[i] for i in top_indices]
    sklearn_topics.append(top_words)
    print(f"  Topic {topic_idx+1}: {', '.join(top_words[:5])}")

# ä¸»é¡Œåˆ†é…
sklearn_dominant_topics = sklearn_output.argmax(axis=1) + 1
sklearn_topic_probs = sklearn_output.max(axis=1)

# ============================================================================
# æ–¹æ³•2: Gensim LDA
# ============================================================================
print("\n" + "="*80)
print("METHOD 2: Gensim LDA")
print("="*80)

# å»ºç«‹ Gensim å­—å…¸å’Œèªæ–™åº«
print("\nBuilding Gensim dictionary and corpus...")
gensim_dictionary = corpora.Dictionary(texts)
original_size = len(gensim_dictionary)

gensim_dictionary.filter_extremes(
    no_below=3,
    no_above=0.5,
    keep_n=None
)
gensim_dictionary.compactify()

gensim_corpus = [gensim_dictionary.doc2bow(text) for text in texts]

print(f"  Original vocabulary: {original_size}")
print(f"  Filtered vocabulary: {len(gensim_dictionary)}")

# è¨“ç·´ Gensim LDA
print("\nTraining Gensim LDA with K=7...")
gensim_lda = LdaModel(
    corpus=gensim_corpus,
    id2word=gensim_dictionary,
    num_topics=7,
    alpha='symmetric',
    eta='auto',
    iterations=100,
    passes=10,
    random_state=42
)

# è¨ˆç®—è©•ä¼°æŒ‡æ¨™
print("\nCalculating coherence and perplexity...")
coherence_model = CoherenceModel(
    model=gensim_lda,
    texts=texts,
    dictionary=gensim_dictionary,
    coherence='c_v',
    processes=1
)
gensim_coherence = coherence_model.get_coherence()
gensim_perplexity = gensim_lda.log_perplexity(gensim_corpus)

print(f"\nâœ“ Gensim LDA training complete!")
print(f"  Coherence Score: {gensim_coherence:.4f}")
print(f"  Perplexity: {gensim_perplexity:.4f}")

# æå–ä¸»é¡Œè©
print("\nGensim Topic Keywords:")
gensim_topics = []
for idx in range(7):
    topic_words = gensim_lda.show_topic(idx, topn=10)
    keywords = [word for word, prob in topic_words]
    gensim_topics.append(keywords)
    print(f"  Topic {idx+1}: {', '.join(keywords[:5])}")

# ä¸»é¡Œåˆ†é…
gensim_topic_assignments = []
for doc_bow in gensim_corpus:
    topic_dist = gensim_lda.get_document_topics(doc_bow, minimum_probability=0)
    topic_probs = [prob for _, prob in topic_dist]
    dominant_topic = topic_probs.index(max(topic_probs)) + 1
    gensim_topic_assignments.append(dominant_topic)

gensim_dominant_topics = np.array(gensim_topic_assignments)

# ============================================================================
# æ¯”è¼ƒçµæœ
# ============================================================================
print("\n" + "="*80)
print("COMPARISON RESULTS | æ¯”è¼ƒçµæœ")
print("="*80)

print("\n1. Model Quality Metrics | æ¨¡å‹å“è³ªæŒ‡æ¨™")
print("-" * 60)
print(f"{'Metric':<30} {'scikit-learn':<20} {'Gensim':<20}")
print("-" * 60)
print(f"{'Vocabulary Size':<30} {len(sklearn_vocab):<20} {len(gensim_dictionary):<20}")
print(f"{'Perplexity':<30} {sklearn_perplexity:<20.4f} {gensim_perplexity:<20.4f}")
print(f"{'Coherence (c_v)':<30} {'N/A':<20} {gensim_coherence:<20.4f}")

print("\n2. Topic Distribution | ä¸»é¡Œåˆ†å¸ƒ")
print("-" * 60)

# scikit-learn ä¸»é¡Œåˆ†å¸ƒ
sklearn_topic_counts = pd.Series(sklearn_dominant_topics).value_counts().sort_index()
gensim_topic_counts = pd.Series(gensim_dominant_topics).value_counts().sort_index()

print(f"\n{'Topic':<10} {'scikit-learn Count':<20} {'Gensim Count':<20} {'Difference':<15}")
print("-" * 65)
for topic_id in range(1, 8):
    sklearn_count = sklearn_topic_counts.get(topic_id, 0)
    gensim_count = gensim_topic_counts.get(topic_id, 0)
    diff = gensim_count - sklearn_count
    print(f"{topic_id:<10} {sklearn_count:<20} {gensim_count:<20} {diff:+<15}")

print("\n3. Topic Keywords Comparison | ä¸»é¡Œé—œéµè©æ¯”è¼ƒ")
print("-" * 80)

for i in range(7):
    print(f"\nTopic {i+1}:")
    print(f"  scikit-learn: {', '.join(sklearn_topics[i][:8])}")
    print(f"  Gensim:       {', '.join(gensim_topics[i][:8])}")

    # è¨ˆç®—é—œéµè©é‡ç–Š
    sklearn_set = set(sklearn_topics[i][:10])
    gensim_set = set(gensim_topics[i][:10])
    overlap = sklearn_set & gensim_set
    overlap_pct = len(overlap) / 10 * 100
    print(f"  Overlap:      {len(overlap)}/10 keywords ({overlap_pct:.0f}%): {', '.join(sorted(overlap))}")

# ============================================================================
# çµè«–
# ============================================================================
print("\n" + "="*80)
print("CONCLUSIONS | çµè«–")
print("="*80)

print("\nğŸ“Š ç›¸åŒé» | Similarities:")
print("  1. å…©ç¨®æ–¹æ³•éƒ½èƒ½è­˜åˆ¥å‡º7å€‹ä¸»é¡Œ")
print("  2. ä¸»è¦ä¸»é¡Œï¼ˆæ­£é¢è©•åƒ¹ã€ç­‰å¾…æ™‚é–“ã€ç–¼ç—›ç®¡ç†ï¼‰åœ¨å…©ç¨®æ–¹æ³•ä¸­éƒ½æœ‰å‡ºç¾")

print("\nğŸ” å·®ç•°é» | Differences:")
print("  1. è©å½™é¸æ“‡ï¼š")
print(f"     - scikit-learn: {len(sklearn_vocab)} å€‹è©å½™")
print(f"     - Gensim: {len(gensim_dictionary)} å€‹è©å½™")
print("  2. è©•ä¼°æŒ‡æ¨™ï¼š")
print("     - scikit-learn ä¸»è¦ä½¿ç”¨ Perplexity")
print("     - Gensim å¯è¨ˆç®— Coherence Scoreï¼ˆæ›´é©åˆè©•ä¼°ä¸»é¡Œå“è³ªï¼‰")
print("  3. æ¼”ç®—æ³•ç´°ç¯€ï¼š")
print("     - scikit-learn: åŸºæ–¼ Variational Bayes")
print("     - Gensim: åŸºæ–¼ Online Variational Bayes with multi-pass")

print("\nğŸ’¡ å»ºè­° | Recommendation:")
print("  âœ“ å°æ–¼è·¨æ–‡åŒ–æ¯”è¼ƒï¼ˆå°ç£ vs ç¾åœ‹ï¼‰ï¼Œå»ºè­°ä½¿ç”¨ Gensim LDA")
print("  âœ“ åŸå› ï¼š")
print("    1. å¯è¨ˆç®— Coherence Score è©•ä¼°ä¸»é¡Œå“è³ª")
print("    2. èˆ‡å°ç£åˆ†æä½¿ç”¨ç›¸åŒå·¥å…·ï¼Œç¢ºä¿æ–¹æ³•è«–ä¸€è‡´æ€§")
print("    3. æ›´é©åˆå­¸è¡“ç ”ç©¶ï¼ˆå¯å ±å‘Š Coherence å’Œ Perplexity å…©å€‹æŒ‡æ¨™ï¼‰")
print(f"    4. å°ç£æ¨¡å‹ Coherence={0.4175:.4f}ï¼Œå¯èˆ‡ç¾åœ‹ Coherence={gensim_coherence:.4f} ç›´æ¥æ¯”è¼ƒ")

print("\nâš ï¸  é‡è¦æé†’ | Important Note:")
print("  é›–ç„¶å…©ç¨®å·¥å…·éƒ½ç”¢å‡º7å€‹ä¸»é¡Œï¼Œä½†ä¸»é¡Œçš„å…·é«”å…§å®¹å’Œåˆ†å¸ƒæœƒæœ‰å·®ç•°ã€‚")
print("  é€™æ˜¯å› ç‚ºï¼š")
print("  - è©å½™ç¯©é¸æ¨™æº–ä¸åŒï¼ˆmax_df, min_df vs no_above, no_belowï¼‰")
print("  - æ¼”ç®—æ³•å¯¦ä½œç´°ç¯€ä¸åŒ")
print("  - æ”¶æ–‚æ¢ä»¶ä¸åŒ")
print("  å› æ­¤ï¼Œè·¨åœ‹æ¯”è¼ƒæ™‚ä½¿ç”¨ç›¸åŒå·¥å…·ï¼ˆGensimï¼‰æ›´ç‚ºåš´è¬¹ã€‚")

# å„²å­˜æ¯”è¼ƒçµæœ
comparison_results = {
    'sklearn': {
        'perplexity': sklearn_perplexity,
        'topics': sklearn_topics,
        'topic_distribution': sklearn_topic_counts.to_dict(),
        'vocab_size': len(sklearn_vocab)
    },
    'gensim': {
        'coherence': gensim_coherence,
        'perplexity': gensim_perplexity,
        'topics': gensim_topics,
        'topic_distribution': gensim_topic_counts.to_dict(),
        'vocab_size': len(gensim_dictionary)
    }
}

with open('sklearn_vs_gensim_comparison_k7.pkl', 'wb') as f:
    pickle.dump(comparison_results, f)

print("\nâœ“ Comparison results saved: sklearn_vs_gensim_comparison_k7.pkl")
print("\n" + "="*80)
print("Analysis Complete!")
print("="*80)
