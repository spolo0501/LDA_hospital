#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¾åœ‹é†«é™¢è©•è«– - æ¯”è¼ƒ5ã€6ã€7å€‹ä¸»é¡Œçš„LDAæ¨¡å‹
ä½¿ç”¨Gensim LDAï¼Œèˆ‡å°ç£åˆ†æç›¸åŒæ–¹æ³•
"""

import pickle
import pandas as pd
import numpy as np
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel
from gensim import corpora
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# è‹±æ–‡æ–‡æœ¬è™•ç†
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

print("="*80)
print("USA Hospital Reviews - LDA Model Comparison (K=5, 6, 7)")
print("ç¾åœ‹é†«é™¢è©•è«– - LDAæ¨¡å‹æ¯”è¼ƒï¼ˆK=5, 6, 7ï¼‰")
print("="*80)

# æ–‡æœ¬å‰è™•ç†å‡½æ•¸
def preprocess_english_text(text):
    """è‹±æ–‡æ–‡æœ¬å‰è™•ç†"""
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\S+@\S+', '', text)
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)
    text = ' '.join(text.split())
    return text

def tokenize_and_lemmatize(text, stop_words):
    """åˆ†è©èˆ‡è©å½¢é‚„åŸ"""
    lemmatizer = WordNetLemmatizer()
    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(token) for token in tokens
              if token not in stop_words and len(token) > 2]
    return tokens

# è¼‰å…¥è³‡æ–™
print("\nLoading data | è¼‰å…¥è³‡æ–™...")
df = pd.read_csv('cleaned_data_no_dedup/final_cleaned_sample_no_dedup.csv', encoding='utf-8-sig')
df_en = df[df['èªè¨€'] == 'en'].copy()
print(f"âœ“ Loaded {len(df_en):,} English reviews")
print(f"âœ“ å·²è¼‰å…¥ {len(df_en):,} ç­†è‹±æ–‡è©•è«–")

# æ–‡æœ¬å‰è™•ç†
print("\nPreprocessing text | æ–‡æœ¬å‰è™•ç†ä¸­...")
stop_words = set(stopwords.words('english'))
custom_stop_words = {'hospital', 'doctor', 'went', 'like', 'would', 'one', 'get', 'go'}
stop_words.update(custom_stop_words)

df_en['cleaned_text'] = df_en['è©•è«–å…§å®¹'].apply(preprocess_english_text)
df_en['tokens'] = df_en['cleaned_text'].apply(lambda x: tokenize_and_lemmatize(x, stop_words))
df_en = df_en[df_en['tokens'].str.len() > 0].copy()

texts = df_en['tokens'].tolist()
print(f"âœ“ Valid reviews: {len(df_en):,}")
print(f"âœ“ æœ‰æ•ˆè©•è«–: {len(df_en):,}")

# å»ºç«‹å­—å…¸å’Œèªæ–™åº«
print("\nBuilding dictionary and corpus | å»ºç«‹å­—å…¸å’Œèªæ–™åº«...")
dictionary = corpora.Dictionary(texts)
original_size = len(dictionary)

dictionary.filter_extremes(
    no_below=3,
    no_above=0.5,
    keep_n=None
)
dictionary.compactify()

print(f"  Original vocabulary: {original_size} | åŸå§‹è©å½™æ•¸: {original_size}")
print(f"  Filtered vocabulary: {len(dictionary)} | éæ¿¾å¾Œè©å½™æ•¸: {len(dictionary)}")

corpus = [dictionary.doc2bow(text) for text in texts]
print(f"âœ“ Corpus created: {len(corpus)} documents | èªæ–™åº«å»ºç«‹å®Œæˆ: {len(corpus)} ç­†æ–‡æª”")

# è¨“ç·´ä¸¦æ¯”è¼ƒä¸åŒä¸»é¡Œæ•¸çš„æ¨¡å‹
results = []

for num_topics in [5, 6, 7]:
    print("\n" + "="*80)
    print(f"Training K={num_topics} Model | è¨“ç·´ K={num_topics} æ¨¡å‹")
    print("="*80)

    # è¨“ç·´LDAæ¨¡å‹
    lda_model = LdaModel(
        corpus=corpus,
        id2word=dictionary,
        num_topics=num_topics,
        alpha='symmetric',
        eta='auto',
        iterations=100,
        passes=10,
        random_state=42
    )

    # è¨ˆç®—coherence
    coherence_model = CoherenceModel(
        model=lda_model,
        texts=texts,
        dictionary=dictionary,
        coherence='c_v'
    )
    coherence_score = coherence_model.get_coherence()
    perplexity_score = lda_model.log_perplexity(corpus)

    print(f"\n  Coherence Score: {coherence_score:.4f}")
    print(f"  Perplexity: {perplexity_score:.4f}")

    # é¡¯ç¤ºä¸»é¡Œé—œéµè©
    print(f"\n  Top Keywords per Topic:")
    for idx in range(num_topics):
        topic_words = lda_model.show_topic(idx, topn=5)
        keywords = ', '.join([word for word, prob in topic_words])
        print(f"    Topic {idx+1}: {keywords}")

    # ç‚ºæ¯ç¯‡è©•è«–åˆ†é…ä¸»é¡Œ
    topic_assignments = []
    for doc_bow in corpus:
        topic_dist = lda_model.get_document_topics(doc_bow, minimum_probability=0)
        topic_probs = [prob for _, prob in topic_dist]
        dominant_topic = topic_probs.index(max(topic_probs)) + 1
        topic_assignments.append({
            'dominant_topic': dominant_topic,
            'topic_probability': max(topic_probs)
        })

    df_result = pd.concat([df_en.reset_index(drop=True), pd.DataFrame(topic_assignments)], axis=1)

    # è¨ˆç®—ä¸»é¡Œåˆ†å¸ƒ
    topic_dist = df_result['dominant_topic'].value_counts(normalize=True).sort_index() * 100

    print(f"\n  Topic Distribution:")
    for topic_id in range(1, num_topics + 1):
        if topic_id in topic_dist.index:
            count = len(df_result[df_result['dominant_topic'] == topic_id])
            percentage = topic_dist[topic_id]
            avg_rating = df_result[df_result['dominant_topic'] == topic_id]['è©•åˆ†'].mean()
            print(f"    Topic {topic_id}: {count} reviews ({percentage:.1f}%), Avg Rating: {avg_rating:.2f}")

    results.append({
        'num_topics': num_topics,
        'coherence': coherence_score,
        'perplexity': perplexity_score,
        'model': lda_model,
        'topic_distribution': topic_dist
    })

    # å„²å­˜æ¨¡å‹
    model_file = f'usa_gensim_lda_k{num_topics}_model.pkl'
    with open(model_file, 'wb') as f:
        pickle.dump({
            'lda_model': lda_model,
            'dictionary': dictionary,
            'coherence_score': coherence_score,
            'perplexity_score': perplexity_score
        }, f)
    print(f"  âœ“ Model saved: {model_file}")

# æ¯”è¼ƒçµæœ
print("\n" + "="*80)
print("COMPARISON RESULTS | æ¯”è¼ƒçµæœ")
print("="*80)

comparison_df = pd.DataFrame([{
    'Topics': r['num_topics'],
    'Coherence Score': f"{r['coherence']:.4f}",
    'Perplexity': f"{r['perplexity']:.4f}"
} for r in results])

print("\n" + comparison_df.to_string(index=False))

# æ‰¾å‡ºæœ€ä½³æ¨¡å‹
best_idx = np.argmax([r['coherence'] for r in results])
best_model = results[best_idx]

print("\n" + "="*80)
print("RECOMMENDATION | å»ºè­°")
print("="*80)
print(f"\nğŸ† Best Model: K={best_model['num_topics']}")
print(f"ğŸ† æœ€ä½³æ¨¡å‹: K={best_model['num_topics']}")
print(f"   Coherence Score: {best_model['coherence']:.4f}")
print(f"   Perplexity: {best_model['perplexity']:.4f}")

print("\nğŸ“Š Reasoning | ç†ç”±:")
if best_model['num_topics'] == 7:
    print("   âœ“ K=7 matches Taiwan's analysis (å°ç£åˆ†æä½¿ç”¨7å€‹ä¸»é¡Œ)")
    print("   âœ“ Enables direct cross-cultural comparison (å¯ç›´æ¥é€²è¡Œè·¨æ–‡åŒ–æ¯”è¼ƒ)")
    print("   âœ“ Highest coherence score (æœ€é«˜ä¸€è‡´æ€§åˆ†æ•¸)")
elif best_model['num_topics'] == 6:
    print("   âœ“ Good balance between detail and interpretability")
    print("   âœ“ ç´°ç·»åº¦èˆ‡å¯è§£é‡‹æ€§çš„è‰¯å¥½å¹³è¡¡")
elif best_model['num_topics'] == 5:
    print("   âœ“ More focused and stable topics")
    print("   âœ“ ä¸»é¡Œæ›´èšç„¦ä¸”ç©©å®š")

print("\nğŸ’¡ Suggestion | å»ºè­°:")
print("   For cross-cultural comparison with Taiwan (K=7), we recommend using K=7")
print("   ç‚ºèˆ‡å°ç£ï¼ˆK=7ï¼‰é€²è¡Œè·¨æ–‡åŒ–æ¯”è¼ƒï¼Œå»ºè­°ä½¿ç”¨ K=7")
print("   This ensures methodological consistency and enables direct topic mapping.")
print("   é€™ç¢ºä¿æ–¹æ³•è«–ä¸€è‡´æ€§ä¸¦èƒ½ç›´æ¥é€²è¡Œä¸»é¡Œæ˜ å°„ã€‚")

# è¦–è¦ºåŒ–æ¯”è¼ƒ
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Coherence comparison
axes[0].bar(['K=5', 'K=6', 'K=7'], [r['coherence'] for r in results],
            color=['#3498db', '#2ecc71', '#e74c3c'], edgecolor='black', linewidth=1.5)
axes[0].set_ylabel('Coherence Score', fontsize=12, fontweight='bold')
axes[0].set_title('Coherence Score Comparison\nä¸€è‡´æ€§åˆ†æ•¸æ¯”è¼ƒ', fontsize=13, fontweight='bold')
axes[0].grid(axis='y', alpha=0.3, linestyle='--')

# æ·»åŠ æ•¸å€¼æ¨™ç±¤
for i, r in enumerate(results):
    axes[0].text(i, r['coherence'] + 0.01, f"{r['coherence']:.4f}",
                 ha='center', va='bottom', fontsize=11, fontweight='bold')

# Perplexity comparison (æ³¨æ„ï¼šè¶Šä½è¶Šå¥½ï¼Œä½†é€™è£¡é¡¯ç¤ºçµ•å°å€¼)
axes[1].bar(['K=5', 'K=6', 'K=7'], [abs(r['perplexity']) for r in results],
            color=['#3498db', '#2ecc71', '#e74c3c'], edgecolor='black', linewidth=1.5)
axes[1].set_ylabel('Perplexity (absolute value)', fontsize=12, fontweight='bold')
axes[1].set_title('Perplexity Comparison\nå›°æƒ‘åº¦æ¯”è¼ƒ', fontsize=13, fontweight='bold')
axes[1].grid(axis='y', alpha=0.3, linestyle='--')

# æ·»åŠ æ•¸å€¼æ¨™ç±¤
for i, r in enumerate(results):
    axes[1].text(i, abs(r['perplexity']) + 0.05, f"{r['perplexity']:.2f}",
                 ha='center', va='bottom', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.savefig('usa_lda_k5k6k7_comparison.png', dpi=300, bbox_inches='tight')
print(f"\nâœ“ Comparison chart saved: usa_lda_k5k6k7_comparison.png")
print(f"âœ“ æ¯”è¼ƒåœ–è¡¨å·²å„²å­˜: usa_lda_k5k6k7_comparison.png")
plt.close()

print("\n" + "="*80)
print("Analysis Complete! | åˆ†æå®Œæˆï¼")
print("="*80)
print("\nğŸ“ Generated files:")
print("   - usa_gensim_lda_k5_model.pkl")
print("   - usa_gensim_lda_k6_model.pkl")
print("   - usa_gensim_lda_k7_model.pkl")
print("   - usa_lda_k5k6k7_comparison.png")
