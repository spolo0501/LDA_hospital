#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¾åœ‹é†«é™¢è©•è«– Gensim LDA K=7 åˆ†æï¼ˆç°¡åŒ–ç‰ˆï¼Œé¿å…multiprocessingå•é¡Œï¼‰
"""

import pickle
import pandas as pd
import numpy as np
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel
from gensim import corpora
import warnings
warnings.filterwarnings('ignore')

# è‹±æ–‡æ–‡æœ¬è™•ç†
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

print("="*80)
print("USA Hospital Reviews - Gensim LDA K=7 Analysis (Simple Version)")
print("ç¾åœ‹é†«é™¢è©•è«– - Gensim LDA K=7 åˆ†æï¼ˆç°¡åŒ–ç‰ˆï¼‰")
print("="*80)

# æ–‡æœ¬å‰è™•ç†å‡½æ•¸
def preprocess_english_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\S+@\S+', '', text)
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)
    text = ' '.join(text.split())
    return text

def tokenize_and_lemmatize(text, stop_words):
    lemmatizer = WordNetLemmatizer()
    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(token) for token in tokens
              if token not in stop_words and len(token) > 2]
    return tokens

# è¼‰å…¥è³‡æ–™
print("\nLoading data...")
df = pd.read_csv('cleaned_data_no_dedup/final_cleaned_sample_no_dedup.csv', encoding='utf-8-sig')
df_en = df[df['èªè¨€'] == 'en'].copy()
print(f"âœ“ Loaded {len(df_en):,} English reviews")

# æ–‡æœ¬å‰è™•ç†
print("\nPreprocessing text...")
stop_words = set(stopwords.words('english'))
custom_stop_words = {'hospital', 'doctor', 'went', 'like', 'would', 'one', 'get', 'go'}
stop_words.update(custom_stop_words)

df_en['cleaned_text'] = df_en['è©•è«–å…§å®¹'].apply(preprocess_english_text)
df_en['tokens'] = df_en['cleaned_text'].apply(lambda x: tokenize_and_lemmatize(x, stop_words))
df_en = df_en[df_en['tokens'].str.len() > 0].copy()

texts = df_en['tokens'].tolist()
print(f"âœ“ Valid reviews: {len(df_en):,}")

# å»ºç«‹å­—å…¸å’Œèªæ–™åº«
print("\nBuilding dictionary and corpus...")
dictionary = corpora.Dictionary(texts)
original_size = len(dictionary)

dictionary.filter_extremes(no_below=3, no_above=0.5, keep_n=None)
dictionary.compactify()

print(f"  Original vocabulary: {original_size}")
print(f"  Filtered vocabulary: {len(dictionary)}")

corpus = [dictionary.doc2bow(text) for text in texts]
print(f"âœ“ Corpus created: {len(corpus)} documents")

# è¨“ç·´K=7çš„LDAæ¨¡å‹
print("\n" + "="*80)
print("Training K=7 LDA Model...")
print("="*80)

lda_model = LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=7,
    alpha='symmetric',
    eta='auto',
    iterations=100,
    passes=10,
    random_state=42
)

print("\nâœ“ Model training complete!")

# è¨ˆç®—coherenceï¼ˆä½¿ç”¨workers=1é¿å…multiprocessingå•é¡Œï¼‰
print("\nCalculating coherence score...")
coherence_model = CoherenceModel(
    model=lda_model,
    texts=texts,
    dictionary=dictionary,
    coherence='c_v',
    processes=1  # é¿å…multiprocessingå•é¡Œ
)
coherence_score = coherence_model.get_coherence()
perplexity_score = lda_model.log_perplexity(corpus)

print(f"\n{'='*80}")
print("MODEL QUALITY METRICS | æ¨¡å‹å“è³ªæŒ‡æ¨™")
print(f"{'='*80}")
print(f"Coherence Score: {coherence_score:.4f}")
print(f"Perplexity: {perplexity_score:.4f}")

# é¡¯ç¤ºä¸»é¡Œé—œéµè©
print(f"\n{'='*80}")
print("TOPIC KEYWORDS | ä¸»é¡Œé—œéµè©")
print(f"{'='*80}")

topics_keywords = []
for idx in range(7):
    topic_words = lda_model.show_topic(idx, topn=10)
    keywords = ', '.join([word for word, prob in topic_words])
    topics_keywords.append({
        'topic_id': idx + 1,
        'keywords': keywords,
        'top_words': [word for word, prob in topic_words[:5]]
    })
    print(f"\nTopic {idx+1} | ä¸»é¡Œ {idx+1}:")
    print(f"  {keywords}")

# ç‚ºæ¯ç¯‡è©•è«–åˆ†é…ä¸»é¡Œ
print(f"\n{'='*80}")
print("Assigning topics to reviews...")
print("='*80)")

topic_assignments = []
for doc_bow in corpus:
    topic_dist = lda_model.get_document_topics(doc_bow, minimum_probability=0)
    topic_probs = [prob for _, prob in topic_dist]
    dominant_topic = topic_probs.index(max(topic_probs)) + 1
    topic_assignments.append({
        'dominant_topic': dominant_topic,
        'topic_probability': max(topic_probs)
    })

df_result = pd.concat([df_en.reset_index(drop=True), pd.DataFrame(topic_assignments)], axis=1)

# çµ±è¨ˆæ¯å€‹ä¸»é¡Œçš„è©•è«–æ•¸é‡å’Œå¹³å‡è©•åˆ†
print(f"\n{'='*80}")
print("TOPIC STATISTICS | ä¸»é¡Œçµ±è¨ˆ")
print(f"{'='*80}")

topic_stats = df_result.groupby('dominant_topic').agg({
    'è©•åˆ†': ['count', 'mean']
})
topic_stats.columns = ['review_count', 'avg_rating']

topic_summary = []
for topic_id in range(1, 8):
    if topic_id in topic_stats.index:
        count = int(topic_stats.loc[topic_id, 'review_count'])
        rating = topic_stats.loc[topic_id, 'avg_rating']
        percentage = (count / len(df_result)) * 100

        topic_summary.append({
            'Topic_ID': topic_id,
            'Keywords': topics_keywords[topic_id-1]['keywords'],
            'Review_Count': count,
            'Percentage': f"{percentage:.1f}%",
            'Avg_Rating': f"{rating:.2f}"
        })

        print(f"\nTopic {topic_id} | ä¸»é¡Œ {topic_id}:")
        print(f"  Reviews: {count} ({percentage:.1f}%)")
        print(f"  è©•è«–æ•¸: {count} ({percentage:.1f}%)")
        print(f"  Avg Rating: {rating:.2f} stars")
        print(f"  å¹³å‡è©•åˆ†: {rating:.2f} æ˜Ÿ")

# å„²å­˜æ¨¡å‹
model_file = 'usa_gensim_lda_k7_simple_model.pkl'
with open(model_file, 'wb') as f:
    pickle.dump({
        'lda_model': lda_model,
        'dictionary': dictionary,
        'coherence_score': coherence_score,
        'perplexity_score': perplexity_score,
        'topics_keywords': topics_keywords,
        'topic_stats': topic_stats,
        'topic_summary': topic_summary
    }, f)
print(f"\nâœ“ Model saved: {model_file}")

# å„²å­˜ä¸»é¡Œæ‘˜è¦åˆ°CSV
summary_df = pd.DataFrame(topic_summary)
summary_df.to_csv('usa_k7_topic_summary.csv', index=False, encoding='utf-8-sig')
print(f"âœ“ Topic summary saved: usa_k7_topic_summary.csv")

# èˆ‡å°ç£çµæœæ¯”è¼ƒ
print(f"\n{'='*80}")
print("COMPARISON WITH TAIWAN | èˆ‡å°ç£æ¯”è¼ƒ")
print(f"{'='*80}")

print("\nLoading Taiwan K=7 results...")
with open('code/lda_k7_lda_model.pkl', 'rb') as f:
    tw_model = pickle.load(f)

print(f"\n{'Model':<15} {'Coherence':<15} {'Perplexity':<15} {'Topics'}")
print("-" * 60)
print(f"{'Taiwan':<15} {tw_model['coherence_score']:<15.4f} {tw_model['perplexity_score']:<15.4f} {7}")
print(f"{'USA':<15} {coherence_score:<15.4f} {perplexity_score:<15.4f} {7}")
print(f"{'Difference':<15} {coherence_score - tw_model['coherence_score']:<15.4f} {perplexity_score - tw_model['perplexity_score']:<15.4f}")

if coherence_score > tw_model['coherence_score']:
    print(f"\nâœ“ USA model has higher coherence (+{coherence_score - tw_model['coherence_score']:.4f})")
else:
    print(f"\n! Taiwan model has higher coherence (+{tw_model['coherence_score'] - coherence_score:.4f})")

print(f"\n{'='*80}")
print("Analysis Complete! | åˆ†æå®Œæˆï¼")
print(f"{'='*80}")
print("\nâœ… Both Taiwan and USA now use the same methodology (Gensim LDA with K=7)")
print("âœ… å°ç£èˆ‡ç¾åœ‹ç¾åœ¨ä½¿ç”¨ç›¸åŒæ–¹æ³•ï¼ˆGensim LDAï¼ŒK=7ï¼‰")
print("\nğŸ“ Generated files:")
print("   - usa_gensim_lda_k7_simple_model.pkl")
print("   - usa_k7_topic_summary.csv")
