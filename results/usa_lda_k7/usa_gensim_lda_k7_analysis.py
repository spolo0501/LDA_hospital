#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¾åœ‹é†«é™¢è©•è«– Gensim LDA åˆ†æï¼ˆ7å€‹ä¸»é¡Œï¼‰
èˆ‡å°ç£åˆ†æä½¿ç”¨ç›¸åŒæ–¹æ³•èˆ‡åƒæ•¸ï¼Œç¢ºä¿å¯æ¯”è¼ƒæ€§
"""

import pickle
import pandas as pd
import numpy as np
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel
from gensim import corpora
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import warnings
warnings.filterwarnings('ignore')

# è‹±æ–‡æ–‡æœ¬è™•ç†
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# ä¸‹è¼‰å¿…è¦çš„NLTKè³‡æº
print("ä¸‹è¼‰NLTKè³‡æº...")
try:
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('omw-1.4', quiet=True)
    nltk.download('punkt_tab', quiet=True)
    print("âœ“ NLTKè³‡æºæº–å‚™å®Œæˆ")
except Exception as e:
    print(f"âš ï¸  NLTKä¸‹è¼‰è­¦å‘Š: {e}")

# è¨­å®šè‹±æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def preprocess_english_text(text):
    """è‹±æ–‡æ–‡æœ¬å‰è™•ç†"""
    # è½‰å°å¯«
    text = text.lower()

    # ç§»é™¤URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # ç§»é™¤email
    text = re.sub(r'\S+@\S+', '', text)

    # åªä¿ç•™å­—æ¯å’Œç©ºæ ¼
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)

    # ç§»é™¤å¤šé¤˜ç©ºæ ¼
    text = ' '.join(text.split())

    return text

def tokenize_and_lemmatize(text, stop_words):
    """åˆ†è©èˆ‡è©å½¢é‚„åŸ"""
    lemmatizer = WordNetLemmatizer()

    # åˆ†è©
    tokens = word_tokenize(text)

    # è©å½¢é‚„åŸä¸¦ç§»é™¤åœç”¨è©
    tokens = [lemmatizer.lemmatize(token) for token in tokens
              if token not in stop_words and len(token) > 2]

    return tokens

def train_and_analyze_lda(num_topics, output_prefix, df=None, dictionary=None, corpus=None, texts=None):
    """è¨“ç·´æŒ‡å®šä¸»é¡Œæ•¸çš„LDAæ¨¡å‹ä¸¦ç”Ÿæˆåˆ†æå ±å‘Š"""

    print(f"\n{'='*60}")
    print(f"Training LDA Model with {num_topics} Topics")
    print(f"è¨“ç·´ {num_topics} å€‹ä¸»é¡Œçš„ LDA æ¨¡å‹")
    print(f"{'='*60}\n")

    print(f"âœ“ Loaded {len(df)} reviews")
    print(f"âœ“ å·²è¼‰å…¥ {len(df)} ç­†è©•è«–")

    # è¨“ç·´LDAæ¨¡å‹ï¼ˆä½¿ç”¨èˆ‡å°ç£ç›¸åŒçš„åƒæ•¸ï¼‰
    print(f"\nTraining {num_topics}-topic model...")
    print(f"è¨“ç·´ {num_topics} å€‹ä¸»é¡Œçš„æ¨¡å‹...")
    lda_model = LdaModel(
        corpus=corpus,
        id2word=dictionary,
        num_topics=num_topics,
        alpha='symmetric',      # èˆ‡å°ç£ä¸€è‡´
        eta='auto',             # èˆ‡å°ç£ä¸€è‡´
        iterations=100,         # èˆ‡å°ç£ä¸€è‡´
        passes=10,              # èˆ‡å°ç£ä¸€è‡´
        random_state=42         # èˆ‡å°ç£ä¸€è‡´
    )

    # è¨ˆç®—coherenceå’Œperplexity
    coherence_model = CoherenceModel(
        model=lda_model,
        texts=texts,
        dictionary=dictionary,
        coherence='c_v'
    )
    coherence_score = coherence_model.get_coherence()
    perplexity_score = lda_model.log_perplexity(corpus)

    print(f"\nModel Quality Metrics:")
    print(f"  Coherence Score: {coherence_score:.4f}")
    print(f"  Perplexity: {perplexity_score:.4f}")

    # é¡¯ç¤ºä¸»é¡Œé—œéµè©
    print(f"\nTopic Keywords:")
    print(f"ä¸»é¡Œé—œéµè©:")
    print("-" * 60)
    topics_keywords = []
    for idx in range(num_topics):
        topic_words = lda_model.show_topic(idx, topn=10)
        keywords = ', '.join([word for word, prob in topic_words])
        topics_keywords.append({
            'topic_id': idx + 1,
            'keywords': keywords,
            'top_words': [word for word, prob in topic_words[:5]]
        })
        print(f"\nTopic {idx+1} | ä¸»é¡Œ {idx+1}:")
        print(f"  {keywords}")

    # ç‚ºæ¯ç¯‡è©•è«–åˆ†é…ä¸»é¡Œ
    print("\nAssigning topics to reviews...")
    print("ç‚ºè©•è«–åˆ†é…ä¸»é¡Œ...")
    topic_assignments = []
    for doc_bow in corpus:
        topic_dist = lda_model.get_document_topics(doc_bow, minimum_probability=0)
        topic_probs = [prob for _, prob in topic_dist]
        dominant_topic = topic_probs.index(max(topic_probs)) + 1
        topic_assignments.append({
            'dominant_topic': dominant_topic,
            'topic_probability': max(topic_probs)
        })

    df_result = pd.concat([df.reset_index(drop=True), pd.DataFrame(topic_assignments)], axis=1)

    # çµ±è¨ˆæ¯å€‹ä¸»é¡Œçš„è©•è«–æ•¸é‡å’Œå¹³å‡è©•åˆ†
    print("\nTopic Statistics:")
    print("ä¸»é¡Œçµ±è¨ˆ:")
    print("-" * 60)
    topic_stats = df_result.groupby('dominant_topic').agg({
        'è©•åˆ†': ['count', 'mean']
    })
    topic_stats.columns = ['review_count', 'avg_rating']

    topic_summary = []
    for topic_id in range(1, num_topics + 1):
        if topic_id in topic_stats.index:
            count = topic_stats.loc[topic_id, 'review_count']
            rating = topic_stats.loc[topic_id, 'avg_rating']
            percentage = (count / len(df_result)) * 100

            topic_summary.append({
                'Topic_ID': topic_id,
                'Keywords': topics_keywords[topic_id-1]['keywords'],
                'Review_Count': int(count),
                'Percentage': f"{percentage:.1f}%",
                'Avg_Rating': f"{rating:.2f}"
            })

            print(f"\nTopic {topic_id} | ä¸»é¡Œ {topic_id}:")
            print(f"  Reviews: {int(count)} ({percentage:.1f}%)")
            print(f"  è©•è«–æ•¸: {int(count)} ({percentage:.1f}%)")
            print(f"  Avg Rating: {rating:.2f} stars")
            print(f"  å¹³å‡è©•åˆ†: {rating:.2f} æ˜Ÿ")
            print(f"  Keywords: {topics_keywords[topic_id-1]['keywords']}")

    # ç”Ÿæˆè¦–è¦ºåŒ–
    print("\nGenerating visualizations...")
    print("ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨...")

    # 1. æ–‡å­—é›²
    fig, axes = plt.subplots(3, 3, figsize=(20, 20))
    axes = axes.flatten()

    for idx in range(num_topics):
        topic_words = dict(lda_model.show_topic(idx, topn=50))
        wordcloud = WordCloud(
            width=400,
            height=400,
            background_color='white',
            colormap='viridis',
            relative_scaling=0.5,
            min_font_size=10
        ).generate_from_frequencies(topic_words)

        axes[idx].imshow(wordcloud, interpolation='bilinear')
        axes[idx].set_title(f'Topic {idx+1}\nä¸»é¡Œ {idx+1}', fontsize=14, fontweight='bold')
        axes[idx].axis('off')

    # éš±è—å¤šé¤˜çš„å­åœ–
    for idx in range(num_topics, len(axes)):
        axes[idx].axis('off')

    plt.tight_layout()
    plt.savefig(f'{output_prefix}_wordclouds.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ Saved: {output_prefix}_wordclouds.png")
    plt.close()

    # 2. ä¸»é¡Œåˆ†å¸ƒåœ–
    fig, ax = plt.subplots(figsize=(12, 6))
    topic_counts = df_result['dominant_topic'].value_counts().sort_index()
    colors = plt.cm.Set3(np.linspace(0, 1, num_topics))

    bars = ax.bar(topic_counts.index, topic_counts.values, color=colors, edgecolor='black', linewidth=1.5)
    ax.set_xlabel('Topic ID | ä¸»é¡Œç·¨è™Ÿ', fontsize=12, fontweight='bold')
    ax.set_ylabel('Number of Reviews | è©•è«–æ•¸é‡', fontsize=12, fontweight='bold')
    ax.set_title('Topic Distribution (USA Hospitals)\nä¸»é¡Œåˆ†å¸ƒï¼ˆç¾åœ‹é†«é™¢ï¼‰', fontsize=14, fontweight='bold')
    ax.set_xticks(range(1, num_topics + 1))
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}\n({height/len(df_result)*100:.1f}%)',
                ha='center', va='bottom', fontsize=10)

    plt.tight_layout()
    plt.savefig(f'{output_prefix}_distribution.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ Saved: {output_prefix}_distribution.png")
    plt.close()

    # 3. è©•åˆ†ç†±åŠ›åœ–
    fig, ax = plt.subplots(figsize=(10, 6))
    rating_topic_matrix = pd.crosstab(
        df_result['dominant_topic'],
        df_result['è©•åˆ†'],
        normalize='index'
    ) * 100

    sns.heatmap(rating_topic_matrix, annot=True, fmt='.1f', cmap='RdYlGn',
                cbar_kws={'label': 'Percentage | ç™¾åˆ†æ¯” (%)'}, ax=ax)
    ax.set_xlabel('Rating (Stars) | è©•åˆ†ï¼ˆæ˜Ÿï¼‰', fontsize=12, fontweight='bold')
    ax.set_ylabel('Topic ID | ä¸»é¡Œç·¨è™Ÿ', fontsize=12, fontweight='bold')
    ax.set_title('Topic-Rating Heatmap (USA Hospitals)\nä¸»é¡Œ-è©•åˆ†ç†±åŠ›åœ–ï¼ˆç¾åœ‹é†«é™¢ï¼‰',
                 fontsize=14, fontweight='bold')

    plt.tight_layout()
    plt.savefig(f'{output_prefix}_rating_heatmap.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ Saved: {output_prefix}_rating_heatmap.png")
    plt.close()

    # æº–å‚™Excelè¼¸å‡º
    topic_summary_df = pd.DataFrame(topic_summary)

    # é¸æ“‡ä»£è¡¨æ€§è©•è«–
    representative_reviews = []
    for topic_id in range(1, num_topics + 1):
        topic_reviews = df_result[
            (df_result['dominant_topic'] == topic_id) &
            (df_result['topic_probability'] > 0.6)
        ].nlargest(5, 'topic_probability')

        for _, review in topic_reviews.iterrows():
            representative_reviews.append({
                'Topic': topic_id,
                'Hospital': review['é†«é™¢åç¨±'],
                'Rating': review['è©•åˆ†'],
                'Topic_Probability': f"{review['topic_probability']:.3f}",
                'Review_Content': review['è©•è«–å…§å®¹'][:500]  # é™åˆ¶é•·åº¦
            })

    representative_df = pd.DataFrame(representative_reviews)

    # é†«é™¢åˆ†æ
    hospital_topic = df_result.groupby(['é†«é™¢åç¨±', 'dominant_topic']).size().unstack(fill_value=0)
    hospital_topic.columns = [f'Topic_{i}' for i in hospital_topic.columns]

    # å¯«å…¥Excel
    excel_file = f'{output_prefix}_analysis_results.xlsx'
    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
        topic_summary_df.to_excel(writer, sheet_name='Topic_Summary', index=False)
        representative_df.to_excel(writer, sheet_name='Representative_Reviews', index=False)
        hospital_topic.to_excel(writer, sheet_name='Hospital_Topic_Distribution')

        # èª¿æ•´åˆ—å¯¬
        for sheet_name in writer.sheets:
            worksheet = writer.sheets[sheet_name]
            for column in worksheet.columns:
                max_length = 0
                column = [cell for cell in column]
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min((max_length + 2) * 1.2, 80)
                worksheet.column_dimensions[column[0].column_letter].width = adjusted_width

    print(f"âœ“ Excel results saved: {excel_file}")
    print(f"âœ“ Excelçµæœå·²å„²å­˜: {excel_file}")

    # å„²å­˜æ¨¡å‹
    model_file = f'{output_prefix}_lda_model.pkl'
    with open(model_file, 'wb') as f:
        pickle.dump({
            'lda_model': lda_model,
            'dictionary': dictionary,
            'coherence_score': coherence_score,
            'perplexity_score': perplexity_score,
            'topics_keywords': topics_keywords,
            'topic_stats': topic_stats
        }, f)
    print(f"âœ“ Model saved: {model_file}")
    print(f"âœ“ æ¨¡å‹å·²å„²å­˜: {model_file}")

    print(f"\n{'='*60}")
    print(f"K={num_topics} Analysis Complete!")
    print(f"K={num_topics} åˆ†æå®Œæˆ!")
    print(f"{'='*60}\n")

    return {
        'num_topics': num_topics,
        'coherence': coherence_score,
        'perplexity': perplexity_score,
        'topics': topics_keywords,
        'topic_summary': topic_summary
    }

if __name__ == "__main__":
    print("\n" + "="*60)
    print("USA Hospital Reviews - Gensim LDA Analysis (7 Topics)")
    print("ç¾åœ‹é†«é™¢è©•è«– - Gensim LDA åˆ†æï¼ˆ7å€‹ä¸»é¡Œï¼‰")
    print("="*60)

    # è¼‰å…¥è³‡æ–™
    print("\nLoading data...")
    print("è¼‰å…¥è³‡æ–™...")
    df = pd.read_csv('cleaned_data_no_dedup/final_cleaned_sample_no_dedup.csv', encoding='utf-8-sig')
    print(f"âœ“ Loaded {len(df):,} reviews")
    print(f"âœ“ å·²è¼‰å…¥ {len(df):,} ç­†è©•è«–")

    # ç¯©é¸è‹±æ–‡è©•è«–
    df_en = df[df['èªè¨€'] == 'en'].copy()
    print(f"âœ“ English reviews: {len(df_en):,} ({len(df_en)/len(df)*100:.1f}%)")
    print(f"âœ“ è‹±æ–‡è©•è«–: {len(df_en):,} ({len(df_en)/len(df)*100:.1f}%)")

    # æ–‡æœ¬å‰è™•ç†
    print("\nPreprocessing text...")
    print("æ–‡æœ¬å‰è™•ç†ä¸­...")

    # è‹±æ–‡åœç”¨è©
    stop_words = set(stopwords.words('english'))
    # æ·»åŠ é†«é™¢ç›¸é—œå¸¸è¦‹è©ï¼ˆé€™äº›è©å¤ªé€šç”¨ï¼Œä¸å…·å€åˆ†æ€§ï¼‰
    custom_stop_words = {'hospital', 'doctor', 'went', 'like', 'would', 'one', 'get', 'go'}
    stop_words.update(custom_stop_words)

    # è™•ç†æ–‡æœ¬
    print("Processing reviews...")
    df_en['cleaned_text'] = df_en['è©•è«–å…§å®¹'].apply(preprocess_english_text)
    df_en['tokens'] = df_en['cleaned_text'].apply(lambda x: tokenize_and_lemmatize(x, stop_words))

    # ç§»é™¤ç©ºè©•è«–
    df_en = df_en[df_en['tokens'].str.len() > 0].copy()
    print(f"âœ“ Valid reviews after preprocessing: {len(df_en):,}")
    print(f"âœ“ å‰è™•ç†å¾Œæœ‰æ•ˆè©•è«–: {len(df_en):,}")

    texts = df_en['tokens'].tolist()
    print(f"  Average tokens per review: {np.mean([len(t) for t in texts]):.2f}")
    print(f"  å¹³å‡æ¯ç¯‡è©æ•¸: {np.mean([len(t) for t in texts]):.2f}")

    # å»ºç«‹å­—å…¸å’Œèªæ–™åº«
    print("\nBuilding dictionary and corpus...")
    print("å»ºç«‹å­—å…¸å’Œèªæ–™åº«...")
    dictionary = corpora.Dictionary(texts)
    original_size = len(dictionary)

    # éæ¿¾æ¥µç«¯è©å½™ï¼ˆèˆ‡å°ç£ä½¿ç”¨ç›¸åŒåƒæ•¸ï¼‰
    dictionary.filter_extremes(
        no_below=3,      # è‡³å°‘å‡ºç¾åœ¨3å€‹æ–‡æª”ä¸­ï¼ˆèˆ‡å°ç£ä¸€è‡´ï¼‰
        no_above=0.5,    # æœ€å¤šå‡ºç¾åœ¨50%çš„æ–‡æª”ä¸­ï¼ˆèˆ‡å°ç£ä¸€è‡´ï¼‰
        keep_n=None
    )
    dictionary.compactify()

    print(f"  Original vocabulary size: {original_size}")
    print(f"  åŸå§‹è©å½™æ•¸: {original_size}")
    print(f"  Filtered vocabulary size: {len(dictionary)}")
    print(f"  éæ¿¾å¾Œè©å½™æ•¸: {len(dictionary)}")

    # å»ºç«‹èªæ–™åº«ï¼ˆBag of Wordsï¼‰
    corpus = [dictionary.doc2bow(text) for text in texts]
    print(f"âœ“ Corpus created with {len(corpus)} documents")
    print(f"âœ“ èªæ–™åº«å»ºç«‹å®Œæˆï¼Œå…± {len(corpus)} ç­†æ–‡æª”")

    # è¨“ç·´K=7çš„æ¨¡å‹
    result = train_and_analyze_lda(7, 'usa_gensim_lda_k7', df=df_en,
                                   dictionary=dictionary, corpus=corpus, texts=texts)

    # é¡¯ç¤ºæœ€çµ‚çµæœæ‘˜è¦
    print("\n" + "="*60)
    print("Final Results Summary")
    print("æœ€çµ‚çµæœæ‘˜è¦")
    print("="*60)

    print(f"\nNumber of Topics: {result['num_topics']}")
    print(f"ä¸»é¡Œæ•¸: {result['num_topics']}")
    print(f"Coherence Score: {result['coherence']:.4f}")
    print(f"Perplexity: {result['perplexity']:.4f}")

    print("\n7 Topics Keywords:")
    print("7å€‹ä¸»é¡Œçš„é—œéµè©:")
    print("-" * 60)
    for topic in result['topics']:
        print(f"  Topic {topic['topic_id']}: {', '.join(topic['top_words'])}")

    print("\nTopic Distribution:")
    print("ä¸»é¡Œåˆ†å¸ƒ:")
    print("-" * 60)
    for item in result['topic_summary']:
        print(f"  Topic {item['Topic_ID']}: {item['Review_Count']} reviews ({item['Percentage']}), "
              f"Avg Rating: {item['Avg_Rating']} stars")

    print("\n" + "="*60)
    print("Generated Files:")
    print("å·²ç”Ÿæˆä»¥ä¸‹ç ”ç©¶æˆæœæª”æ¡ˆ:")
    print("="*60)
    print("âœ“ usa_gensim_lda_k7_wordclouds.png - Word clouds for 7 topics")
    print("âœ“ usa_gensim_lda_k7_distribution.png - Topic distribution chart")
    print("âœ“ usa_gensim_lda_k7_rating_heatmap.png - Topic-rating heatmap")
    print("âœ“ usa_gensim_lda_k7_analysis_results.xlsx - Complete analysis results Excel file")
    print("    â””â”€ Sheet 1: Topic Summary")
    print("    â””â”€ Sheet 2: Representative Reviews")
    print("    â””â”€ Sheet 3: Hospital Topic Distribution")
    print("âœ“ usa_gensim_lda_k7_lda_model.pkl - Trained LDA model file")
    print("="*60)
    print("\nAnalysis complete! All research outputs are ready.")
    print("åˆ†æå®Œæˆï¼æ‰€æœ‰ç ”ç©¶æˆæœæª”æ¡ˆå·²æº–å‚™å°±ç·’ã€‚")
    print("\nğŸŠ Now Taiwan and USA analyses use the same methodology (Gensim LDA)!")
    print("ğŸŠ ç¾åœ¨å°ç£èˆ‡ç¾åœ‹åˆ†æä½¿ç”¨ç›¸åŒæ–¹æ³•ï¼ˆGensim LDAï¼‰ï¼")
