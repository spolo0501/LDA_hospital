#!/usr/bin/env python3
"""
Taiwan-USA Healthcare Direct Comparison Literature Search
å°ˆé–€æœå°‹å°ç£èˆ‡ç¾åœ‹é†«ç™‚ç›´æ¥æ¯”è¼ƒçš„æ–‡ç»
"""

import sys
import os
import time

# åŠ å…¥ LiteratureReview ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, '/Users/simon/Downloads/Claude_code/LiteratureReview')

from wos_scraper_api import WOSScraperAPI

def main():
    print("\n" + "="*80)
    print("Taiwan-USA Healthcare Direct Comparison Literature Search")
    print("å°ç¾é†«ç™‚æ¯”è¼ƒå°ˆé–€æ–‡ç»æœå°‹ - 4 çµ„æœå°‹ï¼Œç›®æ¨™ 50-80 ç¯‡")
    print("="*80 + "\n")

    # è¼¸å‡ºç›®éŒ„
    output_dir = "/Users/simon/Library/CloudStorage/Dropbox/paper/Working paper/Hospitals/LDA_hospital/Literature_Review/Chapter_2.3_Healthcare_Systems"

    # å®šç¾© 4 çµ„æœå°‹
    searches = [
        {
            "id": "2.3-TW-USA-1",
            "query": "Taiwan AND (USA OR 'United States' OR America) AND healthcare AND (quality OR service)",
            "max_results": 30,
            "description": "å°ç¾é†«ç™‚å“è³ªç›´æ¥æ¯”è¼ƒ",
            "year_filter": "(2015-2024)"
        },
        {
            "id": "2.3-TW-USA-2",
            "query": "Taiwan AND 'United States' AND ('health system' OR 'healthcare system') AND comparison",
            "max_results": 20,
            "description": "å°ç¾é†«ç™‚é«”ç³»æ¯”è¼ƒ",
            "year_filter": "(2015-2024)"
        },
        {
            "id": "2.3-TW-USA-3",
            "query": "(Asian OR Asia) AND (American OR 'United States') AND healthcare AND comparison AND quality",
            "max_results": 20,
            "description": "äºç¾é†«ç™‚å“è³ªæ¯”è¼ƒ",
            "year_filter": "(2015-2024)"
        },
        {
            "id": "2.3-TW-USA-4",
            "query": "('national health insurance' OR 'universal healthcare') AND ('market based' OR 'private insurance') AND comparison",
            "max_results": 20,
            "description": "å…¨æ°‘å¥ä¿vså¸‚å ´åˆ¶åº¦æ¯”è¼ƒ",
            "year_filter": "(2015-2024)"
        }
    ]

    # å‰µå»ºæŠ“å–å™¨
    scraper = WOSScraperAPI(headless=False)

    try:
        # ç²å– Sessionï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
        print("ğŸ” æ­£åœ¨ç²å– WOS Session ID...")
        print("â³ è«‹åœ¨æ¥ä¸‹ä¾†çš„ 30 ç§’å…§ç¢ºèªå·²ç™»å…¥ Web of Science")
        print("   (å¦‚æœç€è¦½å™¨è‡ªå‹•æ‰“é–‹ WoS é é¢ï¼Œè«‹ç¢ºèªå·²ç™»å…¥)\n")

        if not scraper.get_session(wait_time=30):
            print("âŒ ç„¡æ³•ç²å– Session ID")
            print("   è«‹ç¢ºèªï¼š")
            print("   1. æ‚¨çš„æ©Ÿæ§‹æœ‰ Web of Science è¨‚é–±")
            print("   2. å·²é€éæ©Ÿæ§‹ç¶²è·¯æˆ– VPN é€£ç·š")
            print("   3. ç€è¦½å™¨ä¸­å·²æˆåŠŸç™»å…¥ WoS")
            return

        print("\n" + "="*80)
        print("âœ… Session ç²å–æˆåŠŸï¼é–‹å§‹åŸ·è¡Œ 4 çµ„å°ç¾æ¯”è¼ƒæœå°‹...")
        print("="*80 + "\n")

        all_results = []
        total_papers = 0

        # åŸ·è¡Œæ¯çµ„æœå°‹
        for i, search in enumerate(searches, 1):
            print(f"\n{'='*80}")
            print(f"æœå°‹ {i}/4: {search['description']}")
            print(f"æŸ¥è©¢: {search['query']}")
            print(f"å¹´ä»½: {search['year_filter']}")
            print(f"ç›®æ¨™: {search['max_results']} ç¯‡")
            print('='*80)

            # åŸ·è¡Œæœå°‹
            papers = scraper.search_api(
                query=search['query'],
                max_results=search['max_results'],
                exclude_conference=True
            )

            # æ‰‹å‹•éæ¿¾å¹´ä»½
            if papers and search.get('year_filter'):
                year_range = search['year_filter'].strip('()')
                start_year, end_year = map(int, year_range.split('-'))
                papers = [p for p in papers if p.get('year') != 'N/A' and start_year <= int(p['year']) <= end_year]
                print(f"   (å¹´ä»½éæ¿¾å¾Œå‰©é¤˜ {len(papers)} ç¯‡)")

            if papers:
                print(f"âœ… æˆåŠŸæŠ“å– {len(papers)} ç¯‡æ–‡ç»")

                # ä¿å­˜çµæœ
                filename = f"{output_dir}/{search['id']}_{search['description'].replace(' ', '_')}"
                scraper.save_results(papers, filename)

                all_results.extend(papers)
                total_papers += len(papers)

                # é¡¯ç¤ºå‰ 5 ç¯‡
                print(f"\nå‰ 5 ç¯‡æ–‡ç»:")
                for j, paper in enumerate(papers[:5], 1):
                    print(f"  {j}. [{paper['citations']} å¼•ç”¨] {paper['title'][:80]}...")

                # æª¢æŸ¥æ˜¯å¦åŒæ™‚åŒ…å« Taiwan å’Œ USA
                taiwan_usa_count = 0
                for paper in papers:
                    text = (paper.get('title', '') + ' ' + paper.get('abstract', '')).lower()
                    if 'taiwan' in text and ('usa' in text or 'united states' in text or 'america' in text):
                        taiwan_usa_count += 1

                print(f"\n   â­ å…¶ä¸­åŒæ™‚æåˆ°å°ç£å’Œç¾åœ‹: {taiwan_usa_count} ç¯‡")

            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°æ–‡ç»")

            # é¿å…è«‹æ±‚éå¿«
            if i < len(searches):
                print(f"\nâ³ ç­‰å¾… 3 ç§’å¾Œç¹¼çºŒä¸‹ä¸€çµ„æœå°‹...")
                time.sleep(3)

        # æœ€çµ‚çµ±è¨ˆ
        print("\n" + "="*80)
        print("ğŸ‰ æ‰€æœ‰æœå°‹å®Œæˆï¼")
        print("="*80)
        print(f"\nğŸ“Š ç¸½è¨ˆ:")
        print(f"  - åŸ·è¡Œæœå°‹çµ„æ•¸: {len(searches)} çµ„")
        print(f"  - æŠ“å–æ–‡ç»ç¸½æ•¸: {total_papers} ç¯‡")

        # æª¢æŸ¥åŒæ™‚åŒ…å«å°ç£å’Œç¾åœ‹çš„æ–‡ç»
        taiwan_usa_papers = []
        for paper in all_results:
            text = (paper.get('title', '') + ' ' + paper.get('abstract', '')).lower()
            if 'taiwan' in text and ('usa' in text or 'united states' in text or 'america' in text):
                taiwan_usa_papers.append(paper)

        print(f"\nâ­ åŒæ™‚æåˆ°å°ç£å’Œç¾åœ‹çš„æ–‡ç»: {len(taiwan_usa_papers)} ç¯‡")

        # å¹´ä»½åˆ†å¸ƒ
        from collections import Counter
        years = [p['year'] for p in all_results if p['year'] != 'N/A']
        year_counts = Counter(years)

        print(f"\nğŸ“… å¹´ä»½åˆ†å¸ƒ:")
        for year in sorted(year_counts.keys(), reverse=True)[:10]:
            print(f"  {year}: {year_counts[year]} ç¯‡")

        # å¼•ç”¨æ•¸çµ±è¨ˆ
        citations = [p['citations'] for p in all_results]
        if citations:
            print(f"\nğŸ“ˆ å¼•ç”¨æ•¸çµ±è¨ˆ:")
            print(f"  - ç¸½å¼•ç”¨æ•¸: {sum(citations)}")
            print(f"  - å¹³å‡å¼•ç”¨: {sum(citations) / len(citations):.1f}")
            print(f"  - æœ€é«˜å¼•ç”¨: {max(citations)}")
            print(f"  - ä¸­ä½æ•¸: {sorted(citations)[len(citations)//2]}")

        # é«˜å¼•ç”¨æ–‡ç»ï¼ˆTop 10ï¼‰
        high_cited = sorted(all_results, key=lambda x: x['citations'], reverse=True)[:10]
        print(f"\nâ­ é«˜å¼•ç”¨æ–‡ç» (Top 10):")
        for i, paper in enumerate(high_cited, 1):
            print(f"  {i}. [{paper['citations']} å¼•ç”¨] {paper['title'][:70]}...")
            print(f"     {paper['journal']}, {paper['year']}")

        # å°ç¾ç›´æ¥æ¯”è¼ƒçš„é«˜å¼•ç”¨æ–‡ç»
        if taiwan_usa_papers:
            taiwan_usa_high_cited = sorted(taiwan_usa_papers, key=lambda x: x['citations'], reverse=True)[:5]
            print(f"\nğŸ¯ å°ç¾ç›´æ¥æ¯”è¼ƒçš„é«˜å¼•ç”¨æ–‡ç» (Top 5):")
            for i, paper in enumerate(taiwan_usa_high_cited, 1):
                print(f"  {i}. [{paper['citations']} å¼•ç”¨] {paper['title'][:70]}...")
                print(f"     {paper['journal']}, {paper['year']}")

        print("\n" + "="*80)
        print("ğŸ’¾ æ‰€æœ‰çµæœå·²ä¿å­˜åˆ°:")
        print(f"   {output_dir}")
        print("="*80)

        # å»ºè­°ä¸‹ä¸€æ­¥
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè­°:")
        print(f"  1. å„ªå…ˆé–±è®€ {len(taiwan_usa_papers)} ç¯‡å°ç¾ç›´æ¥æ¯”è¼ƒæ–‡ç»")
        print(f"  2. é–±è®€é«˜å¼•ç”¨æ–‡ç»ï¼ˆTop 10ï¼‰")
        print(f"  3. æ•´åˆåˆ° Chapter 2.3ï¼ˆé†«ç™‚é«”ç³»æ¯”è¼ƒï¼‰")
        print(f"  4. å¦‚æœæ–‡ç»ä¸è¶³ï¼Œè€ƒæ…®èª¿æ•´é—œéµå­—å†æœå°‹")

    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

    finally:
        scraper.close()
        print("\nğŸ”’ ç€è¦½å™¨å·²é—œé–‰")


if __name__ == "__main__":
    main()
