#!/usr/bin/env python3
"""
Taiwan-USA Healthcare Search (åŒ…å«æœƒè­°è«–æ–‡)
ä½¿ç”¨ /wos-search å‘½ä»¤åŸ·è¡Œ
"""

import sys
import os

# åŠ å…¥ LiteratureReview ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, '/Users/simon/Downloads/Claude_code/LiteratureReview')

from wos_scraper_api import WOSScraperAPI

def main():
    print("\n" + "="*80)
    print("Taiwan-USA Healthcare Literature Search (åŒ…å«æœƒè­°è«–æ–‡)")
    print("="*80 + "\n")

    # è¼¸å‡ºç›®éŒ„
    output_dir = "/Users/simon/Library/CloudStorage/Dropbox/paper/Working paper/Hospitals/LDA_hospital/Literature_Review/Chapter_2.3_Healthcare_Systems"

    # æœå°‹è¨­å®š
    search = {
        "query": 'Taiwan AND ("United States" OR USA OR America) AND (healthcare OR "health care" OR hospital)',
        "max_results": 50,
        "year_start": 2015,
        "year_end": 2024,
        "exclude_conference": False,  # åŒ…å«æœƒè­°è«–æ–‡
        "output_prefix": "2.3-TW-USA-WITH-CONF"
    }

    # å‰µå»ºæŠ“å–å™¨
    scraper = WOSScraperAPI(headless=False)

    try:
        # ç²å– Session
        print("ğŸ” æ­£åœ¨ç²å– WOS Session ID...")
        print("â³ è«‹åœ¨æ¥ä¸‹ä¾†çš„ 30 ç§’å…§ç¢ºèªå·²ç™»å…¥ Web of Science\n")

        if not scraper.get_session(wait_time=30):
            print("âŒ ç„¡æ³•ç²å– Session ID")
            return

        print("\n" + "="*80)
        print("âœ… Session ç²å–æˆåŠŸï¼é–‹å§‹æœå°‹...")
        print("="*80 + "\n")

        print(f"æŸ¥è©¢: {search['query']}")
        print(f"å¹´ä»½: {search['year_start']}-{search['year_end']}")
        print(f"æœ€å¤š: {search['max_results']} ç¯‡")
        print(f"æœƒè­°è«–æ–‡: åŒ…å« âœ…\n")

        # åŸ·è¡Œæœå°‹
        papers = scraper.search_api(
            query=search['query'],
            max_results=search['max_results'],
            exclude_conference=search['exclude_conference']
        )

        # å¹´ä»½éæ¿¾
        if papers:
            papers = [p for p in papers if p.get('year') != 'N/A' and
                     search['year_start'] <= int(p['year']) <= search['year_end']]
            print(f"å¹´ä»½éæ¿¾å¾Œ: {len(papers)} ç¯‡\n")

        if papers:
            print(f"âœ… æˆåŠŸæŠ“å– {len(papers)} ç¯‡æ–‡ç»\n")

            # ä¿å­˜çµæœ
            filename = f"{output_dir}/{search['output_prefix']}_å°ç¾é†«ç™‚å«æœƒè­°è«–æ–‡"
            scraper.save_results(papers, filename)

            # çµ±è¨ˆ
            from collections import Counter

            # æ–‡ç»é¡å‹çµ±è¨ˆ
            doc_types = [p.get('document_type', 'Unknown') for p in papers]
            type_counts = Counter(doc_types)

            print("ğŸ“Š æ–‡ç»é¡å‹åˆ†å¸ƒ:")
            for doc_type, count in type_counts.most_common():
                print(f"  {doc_type}: {count} ç¯‡")

            # å¹´ä»½åˆ†å¸ƒ
            years = [p['year'] for p in papers if p['year'] != 'N/A']
            year_counts = Counter(years)

            print(f"\nğŸ“… å¹´ä»½åˆ†å¸ƒ:")
            for year in sorted(year_counts.keys(), reverse=True)[:10]:
                print(f"  {year}: {year_counts[year]} ç¯‡")

            # å¼•ç”¨æ•¸çµ±è¨ˆ
            citations = [p['citations'] for p in papers]
            if citations:
                print(f"\nğŸ“ˆ å¼•ç”¨æ•¸çµ±è¨ˆ:")
                print(f"  - ç¸½å¼•ç”¨æ•¸: {sum(citations)}")
                print(f"  - å¹³å‡å¼•ç”¨: {sum(citations) / len(citations):.1f}")
                print(f"  - æœ€é«˜å¼•ç”¨: {max(citations)}")

            # å°ç¾ç›´æ¥æ¯”è¼ƒ
            taiwan_usa_count = 0
            taiwan_usa_papers = []
            for paper in papers:
                text = (paper.get('title', '') + ' ' + paper.get('abstract', '')).lower()
                if 'taiwan' in text and ('usa' in text or 'united states' in text or 'america' in text):
                    taiwan_usa_count += 1
                    taiwan_usa_papers.append(paper)

            print(f"\nâ­ åŒæ™‚æåˆ°å°ç£å’Œç¾åœ‹: {taiwan_usa_count} ç¯‡")

            # Top 10 é«˜å¼•ç”¨
            high_cited = sorted(papers, key=lambda x: x['citations'], reverse=True)[:10]
            print(f"\nâ­ é«˜å¼•ç”¨æ–‡ç» (Top 10):")
            for i, paper in enumerate(high_cited, 1):
                print(f"  {i}. [{paper['citations']} å¼•ç”¨] {paper['title'][:70]}...")
                print(f"     {paper['journal']}, {paper['year']} ({paper.get('document_type', 'N/A')})")

            print("\n" + "="*80)
            print("ğŸ’¾ çµæœå·²ä¿å­˜:")
            print(f"   {filename}.csv")
            print(f"   {filename}.json")
            print("="*80)

        else:
            print("âš ï¸ æœªæ‰¾åˆ°æ–‡ç»")

    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

    finally:
        scraper.close()
        print("\nğŸ”’ ç€è¦½å™¨å·²é—œé–‰")


if __name__ == "__main__":
    main()
