#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
2.1-1 è£œå……æœå°‹ï¼šç¶“å…¸æœå‹™å“è³ªç†è«–æ–‡ç»
ç›®æ¨™ï¼šæŠ“å– SERVQUALã€Donabedian ç­‰é«˜å¼•ç”¨ç¶“å…¸ç†è«–æ–‡ç»
å¹´ä»½ç¯„åœï¼š1980-2024ï¼ˆ45å¹´ï¼Œæ¶µè“‹æ‰€æœ‰ç¶“å…¸ï¼‰
ç›®æ¨™æ•¸é‡ï¼š100ç¯‡
"""

import sys
import os
from datetime import datetime

# æ·»åŠ  WOS scraper è·¯å¾‘
sys.path.insert(0, '/Users/simon/Downloads/Claude_code/LiteratureReview')
from wos_scraper_api import WOSScraperAPI

def main():
    # åˆå§‹åŒ– scraper
    scraper = WOSScraperAPI(headless=False)

    # è¼¸å‡ºç›®éŒ„
    output_dir = "Literature_Review/Chapter_2.1_Healthcare_Service_Quality"
    os.makedirs(output_dir, exist_ok=True)

    # æœå°‹å®šç¾©
    search = {
        "id": "2.1-1_classic_theories",
        "query": "SERVQUAL OR Donabedian OR \"service quality dimensions\" OR SERVPERF OR \"healthcare quality framework\" OR \"patient satisfaction theory\"",
        "max_results": 100,
        "description": "ç¶“å…¸æœå‹™å“è³ªç†è«–æ–‡ç»ï¼ˆ1980-2024ï¼‰",
        "year_filter": "(1980-2024)"
    }

    print("=" * 80)
    print(f"ğŸ” é–‹å§‹åŸ·è¡Œè£œå……æœå°‹ï¼š{search['id']}")
    print(f"ğŸ“‹ æè¿°ï¼š{search['description']}")
    print(f"ğŸ”‘ é—œéµå­—ï¼š{search['query']}")
    print(f"ğŸ“Š ç›®æ¨™æ•¸é‡ï¼š{search['max_results']}")
    print(f"ğŸ“… å¹´ä»½ç¯„åœï¼š{search['year_filter']}")
    print(f"ğŸ¯ ç›®æ¨™ï¼šæŠ“å– SERVQUALã€Donabedian ç­‰é«˜å¼•ç”¨ç¶“å…¸ç†è«–æ–‡ç»")
    print("=" * 80)

    try:
        # ç™»å…¥ WOS
        if scraper.get_session(wait_time=30):
            print("âœ… æˆåŠŸç™»å…¥ Web of Science")

            # åŸ·è¡Œæœå°‹
            print(f"\nğŸ” æ­£åœ¨æœå°‹ç¶“å…¸ç†è«–æ–‡ç»...")
            papers = scraper.search_api(
                query=search['query'],
                max_results=search['max_results']
            )

            if papers:
                print(f"ğŸ“¥ åˆæ­¥æª¢ç´¢åˆ° {len(papers)} ç¯‡æ–‡ç»")

                # å®¢æˆ¶ç«¯å¹´ä»½ç¯©é¸
                if search.get('year_filter'):
                    year_range = search['year_filter'].strip('()')
                    start_year, end_year = map(int, year_range.split('-'))

                    original_count = len(papers)
                    papers = [p for p in papers if p.get('year') != 'N/A' and start_year <= int(p['year']) <= end_year]

                    print(f"ğŸ“… å¹´ä»½ç¯©é¸ï¼š{start_year}-{end_year}")
                    print(f"   ç¯©é¸å‰ï¼š{original_count} ç¯‡")
                    print(f"   ç¯©é¸å¾Œï¼š{len(papers)} ç¯‡")

                # æŒ‰å¼•ç”¨æ¬¡æ•¸æ’åºï¼ˆå®¢æˆ¶ç«¯æ’åºï¼‰
                papers_sorted = sorted(papers, key=lambda x: int(x.get('citations', 0)), reverse=True)

                # å„²å­˜çµæœ
                output_base = f"{output_dir}/{search['id']}_ç¶“å…¸æœå‹™å“è³ªç†è«–"
                scraper.save_results(papers_sorted, output_base)

                print(f"\nâœ… æœå°‹å®Œæˆï¼")
                print(f"ğŸ“ çµæœå·²å„²å­˜è‡³ï¼š")
                print(f"   - {output_base}.csv")
                print(f"   - {output_base}.json")

                # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
                print(f"\nğŸ“Š æ–‡ç»çµ±è¨ˆï¼š")
                print(f"   ç¸½æ•¸ï¼š{len(papers_sorted)} ç¯‡")

                # å¹´ä»½åˆ†å¸ƒ
                year_dist = {}
                for p in papers_sorted:
                    year = p.get('year', 'N/A')
                    year_dist[year] = year_dist.get(year, 0) + 1

                print(f"\nğŸ“… å¹´ä»½åˆ†å¸ƒï¼ˆå‰10å¹´ï¼‰ï¼š")
                sorted_years = sorted(year_dist.items(), key=lambda x: x[1], reverse=True)[:10]
                for year, count in sorted_years:
                    print(f"   {year}: {count} ç¯‡")

                # é«˜å¼•ç”¨æ–‡ç»ï¼ˆTop 20ï¼‰
                print(f"\nğŸŒŸ ç¶“å…¸é«˜å¼•ç”¨æ–‡ç»ï¼ˆTop 20ï¼‰ï¼š")
                print(f"{'='*80}")
                for i, p in enumerate(papers_sorted[:20], 1):
                    citations = int(p.get('citations', 0))
                    title = p.get('title', 'N/A')
                    year = p.get('year', 'N/A')
                    journal = p.get('journal', 'N/A')
                    authors = p.get('authors', 'N/A')

                    # å–ç¬¬ä¸€ä½œè€…
                    first_author = authors.split(';')[0] if authors != 'N/A' else 'N/A'

                    print(f"\n{i}. [{year}] è¢«å¼• {citations} æ¬¡")
                    print(f"   {title[:100]}...")
                    print(f"   ä½œè€…ï¼š{first_author}")
                    print(f"   æœŸåˆŠï¼š{journal}")

                # æª¢æŸ¥æ˜¯å¦æœ‰ç¶“å…¸æ–‡ç»é—œéµå­—
                print(f"\nğŸ” ç¶“å…¸ç†è«–é—œéµå­—å‡ºç¾æƒ…æ³ï¼š")
                keywords_count = {
                    'SERVQUAL': 0,
                    'Donabedian': 0,
                    'SERVPERF': 0,
                    'Parasuraman': 0,
                    'Zeithaml': 0
                }

                for p in papers_sorted:
                    title = p.get('title', '').upper()
                    authors = p.get('authors', '').upper()
                    keywords = p.get('keywords', '').upper()

                    for key in keywords_count.keys():
                        if key.upper() in title or key.upper() in authors or key.upper() in keywords:
                            keywords_count[key] += 1

                for key, count in keywords_count.items():
                    print(f"   {key}: {count} ç¯‡")

            else:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç»")

        else:
            print("âŒ ç„¡æ³•ç™»å…¥ Web of Science")

    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š")
        print(f"   {str(e)}")
        import traceback
        traceback.print_exc()

    finally:
        # é—œé–‰ç€è¦½å™¨
        scraper.close()
        print("\nğŸ”š ç¨‹å¼åŸ·è¡Œå®Œç•¢")

if __name__ == "__main__":
    main()
