#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
2.1-1 ä¿®æ­£ç‰ˆï¼šé†«ç™‚æœå‹™å“è³ªåŸºç¤ç†è«–æ–‡ç»æœå°‹
å¹´ä»½ç¯„åœï¼š1990-2024ï¼ˆ35å¹´ï¼Œæ¶µè“‹ç¶“å…¸ç†è«–ï¼‰
ç›®æ¨™æ•¸é‡ï¼š150ç¯‡
é—œéµå­—ï¼šé†«ç™‚å°ˆå±¬æœå‹™å“è³ªç†è«–
"""

import sys
import os
from datetime import datetime

# æ·»åŠ  WOS scraper è·¯å¾‘
sys.path.insert(0, '/Users/simon/Downloads/Claude_code/LiteratureReview')
from wos_scraper_api import WOSScraperAPI

def main():
    # åˆå§‹åŒ– scraper
    scraper = WOSScraperAPI(headless=False)

    # è¼¸å‡ºç›®éŒ„
    output_dir = "Literature_Review/Chapter_2.1_Healthcare_Service_Quality"
    os.makedirs(output_dir, exist_ok=True)

    # æœå°‹å®šç¾©
    search = {
        "id": "2.1-1_revised",
        "query": "(healthcare OR hospital OR medical) service quality AND (theory OR framework OR model OR dimension)",
        "max_results": 150,
        "description": "é†«ç™‚æœå‹™å“è³ªåŸºç¤ç†è«–ï¼ˆ1990-2024ï¼‰- ä¿®æ­£ç‰ˆ",
        "year_filter": "(1990-2024)"
    }

    print("=" * 80)
    print(f"ğŸ” é–‹å§‹åŸ·è¡Œæœå°‹ï¼š{search['id']}")
    print(f"ğŸ“‹ æè¿°ï¼š{search['description']}")
    print(f"ğŸ”‘ é—œéµå­—ï¼š{search['query']}")
    print(f"ğŸ“Š ç›®æ¨™æ•¸é‡ï¼š{search['max_results']}")
    print(f"ğŸ“… å¹´ä»½ç¯„åœï¼š{search['year_filter']}")
    print("=" * 80)

    try:
        # ç™»å…¥ WOS
        if scraper.get_session(wait_time=30):
            print("âœ… æˆåŠŸç™»å…¥ Web of Science")

            # åŸ·è¡Œæœå°‹
            print(f"\nğŸ” æ­£åœ¨æœå°‹...")
            papers = scraper.search_api(
                query=search['query'],
                max_results=search['max_results']
            )

            if papers:
                print(f"ğŸ“¥ åˆæ­¥æª¢ç´¢åˆ° {len(papers)} ç¯‡æ–‡ç»")

                # å®¢æˆ¶ç«¯å¹´ä»½ç¯©é¸
                if search.get('year_filter'):
                    year_range = search['year_filter'].strip('()')
                    start_year, end_year = map(int, year_range.split('-'))

                    original_count = len(papers)
                    papers = [p for p in papers if p.get('year') != 'N/A' and start_year <= int(p['year']) <= end_year]

                    print(f"ğŸ“… å¹´ä»½ç¯©é¸ï¼š{start_year}-{end_year}")
                    print(f"   ç¯©é¸å‰ï¼š{original_count} ç¯‡")
                    print(f"   ç¯©é¸å¾Œï¼š{len(papers)} ç¯‡")

                # å„²å­˜çµæœ
                output_base = f"{output_dir}/{search['id']}_é†«ç™‚æœå‹™å“è³ªåŸºç¤ç†è«–"
                scraper.save_results(papers, output_base)

                print(f"\nâœ… æœå°‹å®Œæˆï¼")
                print(f"ğŸ“ çµæœå·²å„²å­˜è‡³ï¼š")
                print(f"   - {output_base}.csv")
                print(f"   - {output_base}.json")

                # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
                print(f"\nğŸ“Š æ–‡ç»çµ±è¨ˆï¼š")
                print(f"   ç¸½æ•¸ï¼š{len(papers)} ç¯‡")

                # å¹´ä»½åˆ†å¸ƒ
                year_dist = {}
                for p in papers:
                    year = p.get('year', 'N/A')
                    year_dist[year] = year_dist.get(year, 0) + 1

                print(f"\nğŸ“… å¹´ä»½åˆ†å¸ƒï¼ˆå‰10å¹´ï¼‰ï¼š")
                sorted_years = sorted(year_dist.items(), key=lambda x: x[1], reverse=True)[:10]
                for year, count in sorted_years:
                    print(f"   {year}: {count} ç¯‡")

                # é«˜å¼•ç”¨æ–‡ç»
                print(f"\nğŸŒŸ é«˜å¼•ç”¨æ–‡ç»ï¼ˆTop 10ï¼‰ï¼š")
                sorted_papers = sorted(papers, key=lambda x: int(x.get('citations', 0)), reverse=True)[:10]
                for i, p in enumerate(sorted_papers, 1):
                    print(f"   {i}. [{p.get('year')}] {p.get('title')[:80]}...")
                    print(f"      è¢«å¼•æ¬¡æ•¸ï¼š{p.get('citations', 0)}")
                    print(f"      æœŸåˆŠï¼š{p.get('journal', 'N/A')}")
                    print()

            else:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç»")

        else:
            print("âŒ ç„¡æ³•ç™»å…¥ Web of Science")

    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š")
        print(f"   {str(e)}")
        import traceback
        traceback.print_exc()

    finally:
        # é—œé–‰ç€è¦½å™¨
        scraper.close()
        print("\nğŸ”š ç¨‹å¼åŸ·è¡Œå®Œç•¢")

if __name__ == "__main__":
    main()
