#!/usr/bin/env python3
"""
Chapter 2.5 WOS Literature Search
Text Mining and Topic Modeling - 4 çµ„æœå°‹
"""

import sys
import os
import time

# åŠ å…¥ LiteratureReview ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, '/Users/simon/Downloads/Claude_code/LiteratureReview')

from wos_scraper_api import WOSScraperAPI

def main():
    print("\n" + "="*80)
    print("Chapter 2.5: Text Mining and Topic Modeling")
    print("WOS æ–‡ç»æœå°‹ - 4 çµ„æœå°‹ï¼Œç›®æ¨™ 170 ç¯‡")
    print("="*80 + "\n")

    # è¼¸å‡ºç›®éŒ„
    output_dir = "/Users/simon/Library/CloudStorage/Dropbox/paper/Working paper/Hospitals/LDA_hospital/Literature_Review/Chapter_2.5_Text_Mining"

    # å®šç¾© 4 çµ„æœå°‹
    searches = [
        {
            "id": "2.5-1",
            "query": "topic modeling AND (healthcare OR patient OR medical)",
            "max_results": 50,
            "description": "Topic_modeling_æ‡‰ç”¨",
            "year_filter": "(2018-2024)"
        },
        {
            "id": "2.5-2",
            "query": "LDA OR \"latent dirichlet allocation\" AND healthcare",
            "max_results": 40,
            "description": "LDA_åœ¨é†«ç™‚",
            "year_filter": "(2015-2024)"
        },
        {
            "id": "2.5-3",
            "query": "text mining AND (patient feedback OR patient reviews)",
            "max_results": 40,
            "description": "æ–‡æœ¬æŒ–æ˜æ–¹æ³•",
            "year_filter": "(2018-2024)"
        },
        {
            "id": "2.5-4",
            "query": "natural language processing AND healthcare quality",
            "max_results": 40,
            "description": "NLP_æ‡‰ç”¨",
            "year_filter": "(2018-2024)"
        }
    ]

    # å‰µå»ºæŠ“å–å™¨
    scraper = WOSScraperAPI(headless=False)

    try:
        # ç²å– Sessionï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
        print("ğŸ” æ­£åœ¨ç²å– WOS Session ID...")
        print("â³ è«‹åœ¨æ¥ä¸‹ä¾†çš„ 30 ç§’å…§ç¢ºèªå·²ç™»å…¥ Web of Science\n")

        if not scraper.get_session(wait_time=30):
            print("âŒ ç„¡æ³•ç²å– Session ID")
            return

        print("\n" + "="*80)
        print("âœ… Session ç²å–æˆåŠŸï¼é–‹å§‹åŸ·è¡Œ 4 çµ„æœå°‹...")
        print("="*80 + "\n")

        all_results = []
        total_papers = 0

        # åŸ·è¡Œæ¯çµ„æœå°‹
        for i, search in enumerate(searches, 1):
            print(f"\n{'='*80}")
            print(f"æœå°‹ {i}/4: {search['description']}")
            print(f"æŸ¥è©¢: {search['query']} AND PY={search['year_filter']}")
            print(f"ç›®æ¨™: {search['max_results']} ç¯‡")
            print('='*80)

            # åŸ·è¡Œæœå°‹ï¼ˆå…ˆä¸åŠ å¹´ä»½éæ¿¾ï¼‰
            papers = scraper.search_api(
                query=search['query'],
                max_results=search['max_results'],
                exclude_conference=True
            )

            # æ‰‹å‹•éæ¿¾å¹´ä»½
            if papers and search.get('year_filter'):
                year_range = search['year_filter'].strip('()')
                start_year, end_year = map(int, year_range.split('-'))
                papers = [p for p in papers if p.get('year') != 'N/A' and start_year <= int(p['year']) <= end_year]
                print(f"   (å¹´ä»½éæ¿¾å¾Œå‰©é¤˜ {len(papers)} ç¯‡)")

            if papers:
                print(f"âœ… æˆåŠŸæŠ“å– {len(papers)} ç¯‡æ–‡ç»")

                # ä¿å­˜çµæœ
                filename = f"{output_dir}/{search['id']}_{search['description']}"
                scraper.save_results(papers, filename)

                all_results.extend(papers)
                total_papers += len(papers)

                # é¡¯ç¤ºå‰ 3 ç¯‡
                print(f"\nå‰ 3 ç¯‡æ–‡ç»:")
                for j, paper in enumerate(papers[:3], 1):
                    print(f"  {j}. [{paper['citations']} å¼•ç”¨] {paper['title'][:60]}...")

            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°æ–‡ç»")

            # é¿å…è«‹æ±‚éå¿«
            if i < len(searches):
                print(f"\nâ³ ç­‰å¾… 3 ç§’å¾Œç¹¼çºŒä¸‹ä¸€çµ„æœå°‹...")
                time.sleep(3)

        # æœ€çµ‚çµ±è¨ˆ
        print("\n" + "="*80)
        print("ğŸ‰ æ‰€æœ‰æœå°‹å®Œæˆï¼")
        print("="*80)
        print(f"\nğŸ“Š ç¸½è¨ˆ:")
        print(f"  - åŸ·è¡Œæœå°‹çµ„æ•¸: {len(searches)} çµ„")
        print(f"  - æŠ“å–æ–‡ç»ç¸½æ•¸: {total_papers} ç¯‡")

        # å¹´ä»½åˆ†å¸ƒ
        from collections import Counter
        years = [p['year'] for p in all_results if p['year'] != 'N/A']
        year_counts = Counter(years)

        print(f"\nğŸ“… å¹´ä»½åˆ†å¸ƒ:")
        for year in sorted(year_counts.keys(), reverse=True)[:10]:
            print(f"  {year}: {year_counts[year]} ç¯‡")

        # å¼•ç”¨æ•¸çµ±è¨ˆ
        citations = [p['citations'] for p in all_results]
        if citations:
            print(f"\nğŸ“ˆ å¼•ç”¨æ•¸çµ±è¨ˆ:")
            print(f"  - ç¸½å¼•ç”¨æ•¸: {sum(citations)}")
            print(f"  - å¹³å‡å¼•ç”¨: {sum(citations) / len(citations):.1f}")
            print(f"  - æœ€é«˜å¼•ç”¨: {max(citations)}")
            print(f"  - ä¸­ä½æ•¸: {sorted(citations)[len(citations)//2]}")

        # é«˜å¼•ç”¨æ–‡ç»ï¼ˆTop 10ï¼‰
        high_cited = sorted(all_results, key=lambda x: x['citations'], reverse=True)[:10]
        print(f"\nâ­ é«˜å¼•ç”¨æ–‡ç» (Top 10):")
        for i, paper in enumerate(high_cited, 1):
            print(f"  {i}. [{paper['citations']} å¼•ç”¨] {paper['title'][:70]}...")
            print(f"     {paper['journal']}, {paper['year']}")

        print("\n" + "="*80)
        print("ğŸ’¾ æ‰€æœ‰çµæœå·²ä¿å­˜åˆ°:")
        print(f"   {output_dir}")
        print("="*80)

    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

    finally:
        scraper.close()
        print("\nğŸ”’ ç€è¦½å™¨å·²é—œé–‰")


if __name__ == "__main__":
    main()
