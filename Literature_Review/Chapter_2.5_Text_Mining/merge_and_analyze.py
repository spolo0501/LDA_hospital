#!/usr/bin/env python3
"""
åˆä½µä¸¦åˆ†æ Chapter 2.5 çš„æ‰€æœ‰æœå°‹çµæœ
"""

import pandas as pd
import os
from collections import Counter

# è¨­å®šç›®éŒ„
base_dir = "/Users/simon/Library/CloudStorage/Dropbox/paper/Working paper/Hospitals/LDA_hospital/Literature_Review/Chapter_2.5_Text_Mining"

# æ‰€æœ‰ CSV æª”æ¡ˆï¼ˆåŸºæœ¬æœå°‹ + è£œå……æœå°‹ï¼‰
csv_files = [
    # åŸºæœ¬æœå°‹
    "2.5-1_Topic_modeling_æ‡‰ç”¨.csv",
    "2.5-2_LDA_åœ¨é†«ç™‚.csv",
    "2.5-3_æ–‡æœ¬æŒ–æ˜æ–¹æ³•.csv",
    "2.5-4_NLP_æ‡‰ç”¨.csv",
    # è£œå……æœå°‹
    "2.5-S1_LDAæ‚£è€…è©•è«–åˆ†æ.csv",
    "2.5-S2_ä¸»é¡Œæ¨¡å‹ç·šä¸Šè©•è«–.csv",
    "2.5-S3_æ–‡æœ¬æŒ–æ˜æœå‹™å“è³ª.csv",
    "2.5-S4_æƒ…æ„Ÿåˆ†ææ‚£è€…æ»¿æ„åº¦.csv",
    "2.5-S5_NLPæ‚£è€…é«”é©—å“è³ª.csv"
]

print("="*80)
print("Chapter 2.5 æ–‡ç»æœå°‹çµæœåˆä½µèˆ‡åˆ†æ")
print("="*80 + "\n")

# è®€å–æ‰€æœ‰ CSV
all_papers = []
for csv_file in csv_files:
    file_path = os.path.join(base_dir, csv_file)
    if os.path.exists(file_path):
        df = pd.read_csv(file_path)
        print(f"âœ… {csv_file}: {len(df)} ç¯‡")
        all_papers.append(df)
    else:
        print(f"âš ï¸  {csv_file}: æª”æ¡ˆä¸å­˜åœ¨")

# åˆä½µ
combined = pd.concat(all_papers, ignore_index=True)
print(f"\nğŸ“Š åˆä½µå‰ç¸½æ•¸: {len(combined)} ç¯‡")

# å»é‡ï¼ˆä¾æ“š DOIï¼‰
combined_dedup = combined.drop_duplicates(subset=['doi'], keep='first')
print(f"ğŸ“Š å»é‡å¾Œç¸½æ•¸: {len(combined_dedup)} ç¯‡")
print(f"   (ç§»é™¤ {len(combined) - len(combined_dedup)} ç¯‡é‡è¤‡æ–‡ç»)\n")

# è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸
print("ğŸ” è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸...")

relevant_keywords = {
    "topic_modeling": ["topic modeling", "topic model", "latent dirichlet", "lda", "probabilistic topic"],
    "text_mining": ["text mining", "text analysis", "data mining", "content analysis"],
    "nlp": ["natural language processing", "nlp", "language model", "word embedding", "bert", "gpt"],
    "healthcare_quality": ["healthcare quality", "service quality", "patient satisfaction", "quality of care"],
    "patient_feedback": ["patient feedback", "patient review", "patient comment", "patient experience", "patient report"],
    "machine_learning": ["machine learning", "deep learning", "neural network", "supervised learning", "unsupervised learning"]
}

def calculate_relevance_score(row):
    score = 0
    text = f"{str(row['title']).lower()} {str(row['abstract']).lower()} {str(row['keywords']).lower()}"

    # é—œéµå­—åŒ¹é…
    for category, keywords in relevant_keywords.items():
        for keyword in keywords:
            if keyword in text:
                score += 1

    # å¼•ç”¨æ•¸åŠ åˆ†
    citations = row['citations'] if pd.notna(row['citations']) else 0
    if citations >= 20:
        score += 3
    elif citations >= 10:
        score += 2
    elif citations >= 5:
        score += 1

    return score

combined_dedup['relevance_score'] = combined_dedup.apply(calculate_relevance_score, axis=1)

# æŒ‰ç›¸é—œæ€§æ’åº
combined_sorted = combined_dedup.sort_values(by='relevance_score', ascending=False)

# ä¿å­˜çµæœ
output_all = os.path.join(base_dir, "Chapter_2.5_COMBINED_ALL.csv")
output_sorted = os.path.join(base_dir, "Chapter_2.5_COMBINED_SORTED_BY_RELEVANCE.csv")

combined_dedup.to_csv(output_all, index=False, encoding='utf-8-sig')
combined_sorted.to_csv(output_sorted, index=False, encoding='utf-8-sig')

print(f"âœ… å·²ä¿å­˜: Chapter_2.5_COMBINED_ALL.csv")
print(f"âœ… å·²ä¿å­˜: Chapter_2.5_COMBINED_SORTED_BY_RELEVANCE.csv\n")

# çµ±è¨ˆåˆ†æ
print("="*80)
print("ğŸ“Š æ–‡ç»çµ±è¨ˆåˆ†æ")
print("="*80 + "\n")

# å¹´ä»½åˆ†å¸ƒ
print("ğŸ“… å¹´ä»½åˆ†å¸ƒ:")
years = combined_sorted['year'].value_counts().sort_index(ascending=False)
for year, count in years.head(10).items():
    print(f"  {year}: {count} ç¯‡")

# å¼•ç”¨æ•¸çµ±è¨ˆ
print(f"\nğŸ“ˆ å¼•ç”¨æ•¸çµ±è¨ˆ:")
citations = combined_sorted['citations'].dropna()
if len(citations) > 0:
    print(f"  - ç¸½å¼•ç”¨æ•¸: {int(citations.sum())}")
    print(f"  - å¹³å‡å¼•ç”¨: {citations.mean():.1f}")
    print(f"  - æœ€é«˜å¼•ç”¨: {int(citations.max())}")
    print(f"  - ä¸­ä½æ•¸: {int(citations.median())}")

# ç›¸é—œæ€§åˆ†æ•¸åˆ†å¸ƒ
print(f"\nğŸ¯ ç›¸é—œæ€§åˆ†æ•¸åˆ†å¸ƒ:")
score_dist = combined_sorted['relevance_score'].value_counts().sort_index(ascending=False)
for score, count in score_dist.head(10).items():
    print(f"  åˆ†æ•¸ {score}: {count} ç¯‡")

# é«˜ç›¸é—œæ€§æ–‡ç»ï¼ˆåˆ†æ•¸ >= 3ï¼‰
high_relevance = combined_sorted[combined_sorted['relevance_score'] >= 3]
print(f"\nâ­ é«˜åº¦ç›¸é—œæ–‡ç» (åˆ†æ•¸ >= 3): {len(high_relevance)} ç¯‡")

# Top 20 é«˜ç›¸é—œæ–‡ç»
print(f"\nğŸ“š Top 20 é«˜ç›¸é—œæ€§æ–‡ç»:")
print("="*80)
for i, row in combined_sorted.head(20).iterrows():
    print(f"{row.name+1}. [{int(row['citations'])} å¼•ç”¨ | åˆ†æ•¸ {row['relevance_score']}]")
    print(f"   {row['title'][:90]}...")
    print(f"   {row['journal']}, {row['year']}")
    print()

# æœŸåˆŠåˆ†å¸ƒï¼ˆTop 10ï¼‰
print("ğŸ“– æœŸåˆŠåˆ†å¸ƒ (Top 10):")
journals = combined_sorted['journal'].value_counts()
for journal, count in journals.head(10).items():
    print(f"  {journal}: {count} ç¯‡")

print("\n" + "="*80)
print("âœ… åˆ†æå®Œæˆï¼")
print("="*80)
