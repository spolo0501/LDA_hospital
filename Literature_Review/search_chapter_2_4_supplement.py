#!/usr/bin/env python3
"""
Chapter 2.4 è£œå……æœå°‹
Online Reviews as a Data Source - 5 çµ„è£œå……æœå°‹
é‡å°æ›´ç›¸é—œçš„æ–‡ç»ï¼Œç‰¹åˆ¥æ˜¯ç·šä¸Šé†«ç™‚è©•è«–çš„æ•ˆåº¦ã€ä»£è¡¨æ€§ã€æ–‡æœ¬åˆ†æ
"""

import sys
import os
import time

# åŠ å…¥ LiteratureReview ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, '/Users/simon/Downloads/Claude_code/LiteratureReview')

from wos_scraper_api import WOSScraperAPI

def main():
    print("\n" + "="*80)
    print("Chapter 2.4: Online Reviews as a Data Source - è£œå……æœå°‹")
    print("WOS æ–‡ç»æœå°‹ - 5 çµ„è£œå……æœå°‹ï¼Œç›®æ¨™ 60-80 ç¯‡")
    print("="*80 + "\n")

    # è¼¸å‡ºç›®éŒ„
    output_dir = "/Users/simon/Library/CloudStorage/Dropbox/paper/Working paper/Hospitals/LDA_hospital/Literature_Review/Chapter_2.4_Online_Reviews"

    # å®šç¾© 5 çµ„è£œå……æœå°‹
    searches = [
        {
            "id": "2.4-S1",
            "query": "patient reviews AND (validity OR reliability OR representativeness)",
            "max_results": 20,
            "description": "æ‚£è€…è©•è«–æ•ˆåº¦ç ”ç©¶",
            "year_filter": "(2015-2024)"
        },
        {
            "id": "2.4-S2",
            "query": "text mining AND (patient feedback OR patient experience OR patient satisfaction)",
            "max_results": 20,
            "description": "æ–‡æœ¬æŒ–æ˜æ‚£è€…å›é¥‹",
            "year_filter": "(2018-2024)"
        },
        {
            "id": "2.4-S3",
            "query": "sentiment analysis AND (hospital OR healthcare OR medical)",
            "max_results": 15,
            "description": "æƒ…æ„Ÿåˆ†æé†«ç™‚é ˜åŸŸ",
            "year_filter": "(2018-2024)"
        },
        {
            "id": "2.4-S4",
            "query": "(Yelp OR Google reviews OR online reviews) AND (physician OR doctor OR hospital)",
            "max_results": 15,
            "description": "ç·šä¸Šè©•è«–å¹³å°é†«ç™‚ç ”ç©¶",
            "year_filter": "(2016-2024)"
        },
        {
            "id": "2.4-S5",
            "query": "online reputation AND (hospital OR healthcare) AND (patient choice OR patient behavior)",
            "max_results": 15,
            "description": "ç·šä¸Šè²è­½èˆ‡æ‚£è€…é¸æ“‡",
            "year_filter": "(2016-2024)"
        }
    ]

    # å‰µå»ºæŠ“å–å™¨
    scraper = WOSScraperAPI(headless=False)

    try:
        # ç²å– Sessionï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
        print("ğŸ” æ­£åœ¨ç²å– WOS Session ID...")
        print("â³ è«‹åœ¨æ¥ä¸‹ä¾†çš„ 30 ç§’å…§ç¢ºèªå·²ç™»å…¥ Web of Science\n")

        if not scraper.get_session(wait_time=30):
            print("âŒ ç„¡æ³•ç²å– Session ID")
            return

        print("\n" + "="*80)
        print("âœ… Session ç²å–æˆåŠŸï¼é–‹å§‹åŸ·è¡Œ 5 çµ„è£œå……æœå°‹...")
        print("="*80 + "\n")

        all_results = []
        total_papers = 0

        # åŸ·è¡Œæ¯çµ„æœå°‹
        for i, search in enumerate(searches, 1):
            print(f"\n{'='*80}")
            print(f"è£œå……æœå°‹ {i}/5: {search['description']}")
            print(f"æŸ¥è©¢: {search['query']} AND PY={search['year_filter']}")
            print(f"ç›®æ¨™: {search['max_results']} ç¯‡")
            print('='*80)

            # åŸ·è¡Œæœå°‹ï¼ˆå…ˆä¸åŠ å¹´ä»½éæ¿¾ï¼‰
            papers = scraper.search_api(
                query=search['query'],
                max_results=search['max_results'],
                exclude_conference=True
            )

            # æ‰‹å‹•éæ¿¾å¹´ä»½
            if papers and search.get('year_filter'):
                year_range = search['year_filter'].strip('()')
                start_year, end_year = map(int, year_range.split('-'))
                papers = [p for p in papers if p.get('year') != 'N/A' and start_year <= int(p['year']) <= end_year]
                print(f"   (å¹´ä»½éæ¿¾å¾Œå‰©é¤˜ {len(papers)} ç¯‡)")

            if papers:
                print(f"âœ… æˆåŠŸæŠ“å– {len(papers)} ç¯‡æ–‡ç»")

                # ä¿å­˜çµæœ
                filename = f"{output_dir}/{search['id']}_{search['description'].replace(' ', '_')}"
                scraper.save_results(papers, filename)

                all_results.extend(papers)
                total_papers += len(papers)

                # é¡¯ç¤ºå‰ 3 ç¯‡
                print(f"\nå‰ 3 ç¯‡æ–‡ç»:")
                for j, paper in enumerate(papers[:3], 1):
                    print(f"  {j}. [{paper['citations']} å¼•ç”¨] {paper['title'][:60]}...")

            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°æ–‡ç»")

            # é¿å…è«‹æ±‚éå¿«
            if i < len(searches):
                print(f"\nâ³ ç­‰å¾… 3 ç§’å¾Œç¹¼çºŒä¸‹ä¸€çµ„æœå°‹...")
                time.sleep(3)

        # æœ€çµ‚çµ±è¨ˆ
        print("\n" + "="*80)
        print("ğŸ‰ è£œå……æœå°‹å®Œæˆï¼")
        print("="*80)
        print(f"\nğŸ“Š è£œå……æœå°‹ç¸½è¨ˆ:")
        print(f"  - åŸ·è¡Œæœå°‹çµ„æ•¸: {len(searches)} çµ„")
        print(f"  - æŠ“å–æ–‡ç»ç¸½æ•¸: {total_papers} ç¯‡")

        # å¹´ä»½åˆ†å¸ƒ
        from collections import Counter
        years = [p['year'] for p in all_results if p['year'] != 'N/A']
        year_counts = Counter(years)

        print(f"\nğŸ“… å¹´ä»½åˆ†å¸ƒ:")
        for year in sorted(year_counts.keys(), reverse=True)[:10]:
            print(f"  {year}: {year_counts[year]} ç¯‡")

        # å¼•ç”¨æ•¸çµ±è¨ˆ
        citations = [p['citations'] for p in all_results]
        if citations:
            print(f"\nğŸ“ˆ å¼•ç”¨æ•¸çµ±è¨ˆ:")
            print(f"  - ç¸½å¼•ç”¨æ•¸: {sum(citations)}")
            print(f"  - å¹³å‡å¼•ç”¨: {sum(citations) / len(citations):.1f}")
            print(f"  - æœ€é«˜å¼•ç”¨: {max(citations)}")
            print(f"  - ä¸­ä½æ•¸: {sorted(citations)[len(citations)//2]}")

        # é«˜å¼•ç”¨æ–‡ç»ï¼ˆTop 10ï¼‰
        high_cited = sorted(all_results, key=lambda x: x['citations'], reverse=True)[:10]
        print(f"\nâ­ é«˜å¼•ç”¨æ–‡ç» (Top 10):")
        for i, paper in enumerate(high_cited, 1):
            print(f"  {i}. [{paper['citations']} å¼•ç”¨] {paper['title'][:70]}...")
            print(f"     {paper['journal']}, {paper['year']}")

        print("\n" + "="*80)
        print("ğŸ’¾ è£œå……æœå°‹çµæœå·²ä¿å­˜åˆ°:")
        print(f"   {output_dir}")
        print("="*80)

        print("\nğŸ“Œ ä¸‹ä¸€æ­¥ï¼š")
        print("  1. åˆä½µåŸå§‹ 95 ç¯‡ + è£œå……æ–‡ç»")
        print(f"  2. é è¨ˆç¸½æ–‡ç»æ•¸: 95 + {total_papers} = {95 + total_papers} ç¯‡")
        print("  3. å»é‡å¾Œé–‹å§‹åˆ†æ Chapter 2.4 æ–‡ç»")
        print("  4. è­˜åˆ¥é«˜å¼•ç”¨æ–‡ç»ä¸¦æ•´åˆåˆ°ç¾æœ‰ç« ç¯€å…§å®¹")

    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

    finally:
        scraper.close()
        print("\nğŸ”’ ç€è¦½å™¨å·²é—œé–‰")


if __name__ == "__main__":
    main()
