#!/usr/bin/env python3
"""
Chapter 2.3 è£œå……æœå°‹
Healthcare Systems Taiwan vs. USA - 6 çµ„è£œå……æœå°‹
"""

import sys
import os
import time

# åŠ å…¥ LiteratureReview ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, '/Users/simon/Downloads/Claude_code/LiteratureReview')

from wos_scraper_api import WOSScraperAPI

def main():
    print("\n" + "="*80)
    print("Chapter 2.3: Healthcare Systems - è£œå……æœå°‹")
    print("WOS æ–‡ç»æœå°‹ - 6 çµ„è£œå……æœå°‹ï¼Œç›®æ¨™ 110 ç¯‡")
    print("="*80 + "\n")

    # è¼¸å‡ºç›®éŒ„
    output_dir = "/Users/simon/Library/CloudStorage/Dropbox/paper/Working paper/Hospitals/LDA_hospital/Literature_Review/Chapter_2.3_Healthcare_Systems"

    # å®šç¾© 6 çµ„è£œå……æœå°‹
    searches = [
        {
            "id": "2.3-S1",
            "query": "Taiwan AND (healthcare OR hospital) AND (quality OR performance)",
            "max_results": 25,
            "description": "å°ç£é†«ç™‚å“è³ªç¸¾æ•ˆï¼ˆ2015-2024ï¼‰",
            "year_filter": "(2015-2024)"
        },
        {
            "id": "2.3-S2",
            "query": "Taiwan AND health system AND (reform OR policy)",
            "max_results": 20,
            "description": "å°ç£å¥åº·é«”ç³»æ”¹é©ï¼ˆ2015-2024ï¼‰",
            "year_filter": "(2015-2024)"
        },
        {
            "id": "2.3-S3",
            "query": "USA AND healthcare system AND quality",
            "max_results": 20,
            "description": "ç¾åœ‹é†«ç™‚é«”ç³»å“è³ªï¼ˆ2018-2024ï¼‰",
            "year_filter": "(2018-2024)"
        },
        {
            "id": "2.3-S4",
            "query": "Medicare AND Medicaid AND quality",
            "max_results": 15,
            "description": "Medicareèˆ‡Medicaidå“è³ªï¼ˆ2018-2024ï¼‰",
            "year_filter": "(2018-2024)"
        },
        {
            "id": "2.3-S5",
            "query": "healthcare system AND international comparison",
            "max_results": 15,
            "description": "é†«ç™‚é«”ç³»åœ‹éš›æ¯”è¼ƒï¼ˆ2015-2024ï¼‰",
            "year_filter": "(2015-2024)"
        },
        {
            "id": "2.3-S6",
            "query": "universal coverage AND healthcare quality",
            "max_results": 15,
            "description": "å…¨æ°‘è¦†è“‹èˆ‡é†«ç™‚å“è³ªï¼ˆ2018-2024ï¼‰",
            "year_filter": "(2018-2024)"
        }
    ]

    # å‰µå»ºæŠ“å–å™¨
    scraper = WOSScraperAPI(headless=False)

    try:
        # ç²å– Session
        print("ğŸ” æ­£åœ¨ç²å– WOS Session ID...")
        print("â³ è«‹åœ¨æ¥ä¸‹ä¾†çš„ 30 ç§’å…§ç¢ºèªå·²ç™»å…¥ Web of Science\n")

        if not scraper.get_session(wait_time=30):
            print("âŒ ç„¡æ³•ç²å– Session ID")
            return

        print("\n" + "="*80)
        print("âœ… Session ç²å–æˆåŠŸï¼é–‹å§‹åŸ·è¡Œ 6 çµ„è£œå……æœå°‹...")
        print("="*80 + "\n")

        all_results = []
        total_papers = 0

        # åŸ·è¡Œæ¯çµ„æœå°‹
        for i, search in enumerate(searches, 1):
            print(f"\n{'='*80}")
            print(f"è£œå……æœå°‹ {i}/6: {search['description']}")
            print(f"æŸ¥è©¢: {search['query']} AND PY={search['year_filter']}")
            print(f"ç›®æ¨™: {search['max_results']} ç¯‡")
            print('='*80)

            # åŸ·è¡Œæœå°‹
            papers = scraper.search_api(
                query=search['query'],
                max_results=search['max_results'],
                exclude_conference=True
            )

            # æ‰‹å‹•éæ¿¾å¹´ä»½
            if papers and search.get('year_filter'):
                year_range = search['year_filter'].strip('()')
                start_year, end_year = map(int, year_range.split('-'))
                papers = [p for p in papers if p.get('year') != 'N/A' and start_year <= int(p['year']) <= end_year]
                print(f"   (å¹´ä»½éæ¿¾å¾Œå‰©é¤˜ {len(papers)} ç¯‡)")

            if papers:
                print(f"âœ… æˆåŠŸæŠ“å– {len(papers)} ç¯‡æ–‡ç»")

                # ä¿å­˜çµæœ
                filename = f"{output_dir}/{search['id']}_{search['description'].split('ï¼ˆ')[0].replace(' ', '_')}"
                scraper.save_results(papers, filename)

                all_results.extend(papers)
                total_papers += len(papers)

                # é¡¯ç¤ºå‰ 3 ç¯‡
                print(f"\nå‰ 3 ç¯‡æ–‡ç»:")
                for j, paper in enumerate(papers[:3], 1):
                    print(f"  {j}. [{paper['citations']} å¼•ç”¨] {paper['title'][:60]}...")

            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°æ–‡ç»")

            # é¿å…è«‹æ±‚éå¿«
            if i < len(searches):
                print(f"\nâ³ ç­‰å¾… 3 ç§’å¾Œç¹¼çºŒä¸‹ä¸€çµ„æœå°‹...")
                time.sleep(3)

        # æœ€çµ‚çµ±è¨ˆ
        print("\n" + "="*80)
        print("ğŸ‰ è£œå……æœå°‹å®Œæˆï¼")
        print("="*80)
        print(f"\nğŸ“Š è£œå……æœå°‹ç¸½è¨ˆ:")
        print(f"  - åŸ·è¡Œæœå°‹çµ„æ•¸: {len(searches)} çµ„")
        print(f"  - æŠ“å–æ–‡ç»ç¸½æ•¸: {total_papers} ç¯‡")

        # å¹´ä»½åˆ†å¸ƒ
        from collections import Counter
        years = [p['year'] for p in all_results if p['year'] != 'N/A']
        year_counts = Counter(years)

        print(f"\nğŸ“… å¹´ä»½åˆ†å¸ƒ:")
        for year in sorted(year_counts.keys(), reverse=True)[:10]:
            print(f"  {year}: {year_counts[year]} ç¯‡")

        # å¼•ç”¨æ•¸çµ±è¨ˆ
        citations = [p['citations'] for p in all_results]
        if citations:
            print(f"\nğŸ“ˆ å¼•ç”¨æ•¸çµ±è¨ˆ:")
            print(f"  - ç¸½å¼•ç”¨æ•¸: {sum(citations)}")
            print(f"  - å¹³å‡å¼•ç”¨: {sum(citations) / len(citations):.1f}")
            print(f"  - æœ€é«˜å¼•ç”¨: {max(citations)}")

        # é«˜å¼•ç”¨æ–‡ç»
        high_cited = sorted(all_results, key=lambda x: x['citations'], reverse=True)[:5]
        print(f"\nâ­ é«˜å¼•ç”¨æ–‡ç» (Top 5):")
        for i, paper in enumerate(high_cited, 1):
            print(f"  {i}. [{paper['citations']} å¼•ç”¨] {paper['title'][:70]}...")
            print(f"     {paper['journal']}, {paper['year']}")

        print("\n" + "="*80)
        print("ğŸ’¾ è£œå……æœå°‹çµæœå·²ä¿å­˜åˆ°:")
        print(f"   {output_dir}")
        print("="*80)

        print("\nğŸ“Œ ä¸‹ä¸€æ­¥ï¼š")
        print("  1. åˆä½µåŸå§‹ 54 ç¯‡ + è£œå……æ–‡ç»")
        print(f"  2. é è¨ˆç¸½æ–‡ç»æ•¸: 54 + {total_papers} = {54 + total_papers} ç¯‡")
        print("  3. å»é‡å¾Œè©•ä¼°æ˜¯å¦é”æ¨™ï¼ˆç›®æ¨™ 160 ç¯‡ï¼‰")

    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

    finally:
        scraper.close()
        print("\nğŸ”’ ç€è¦½å™¨å·²é—œé–‰")


if __name__ == "__main__":
    main()
