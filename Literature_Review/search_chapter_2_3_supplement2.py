#!/usr/bin/env python3
"""
Chapter 2.3 ç¬¬äºŒè¼ªè£œå……æœå°‹
Healthcare Systems Taiwan vs. USA - 4 çµ„è£œå……æœå°‹
"""

import sys
import os
import time

# åŠ å…¥ LiteratureReview ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, '/Users/simon/Downloads/Claude_code/LiteratureReview')

from wos_scraper_api import WOSScraperAPI

def main():
    print("\n" + "="*80)
    print("Chapter 2.3: Healthcare Systems - ç¬¬äºŒè¼ªè£œå……æœå°‹")
    print("WOS æ–‡ç»æœå°‹ - 4 çµ„è£œå……æœå°‹ï¼Œç›®æ¨™ 50 ç¯‡")
    print("="*80 + "\n")

    # è¼¸å‡ºç›®éŒ„
    output_dir = "/Users/simon/Library/CloudStorage/Dropbox/paper/Working paper/Hospitals/LDA_hospital/Literature_Review/Chapter_2.3_Healthcare_Systems"

    # å®šç¾© 4 çµ„ç¬¬äºŒè¼ªè£œå……æœå°‹
    searches = [
        {
            "id": "2.3-S7",
            "query": "health insurance AND Taiwan AND satisfaction",
            "max_results": 15,
            "description": "å°ç£å¥ä¿æ»¿æ„åº¦ï¼ˆ2015-2024ï¼‰",
            "year_filter": "(2015-2024)"
        },
        {
            "id": "2.3-S8",
            "query": "primary care AND USA AND quality",
            "max_results": 15,
            "description": "ç¾åœ‹åˆç´šç…§è­·å“è³ªï¼ˆ2018-2024ï¼‰",
            "year_filter": "(2018-2024)"
        },
        {
            "id": "2.3-S9",
            "query": "OECD AND healthcare AND (comparison OR performance)",
            "max_results": 10,
            "description": "OECDé†«ç™‚é«”ç³»æ¯”è¼ƒï¼ˆ2015-2024ï¼‰",
            "year_filter": "(2015-2024)"
        },
        {
            "id": "2.3-S10",
            "query": "health insurance AND access AND quality",
            "max_results": 10,
            "description": "å¥ä¿å¯è¿‘æ€§èˆ‡å“è³ªï¼ˆ2018-2024ï¼‰",
            "year_filter": "(2018-2024)"
        }
    ]

    # å‰µå»ºæŠ“å–å™¨
    scraper = WOSScraperAPI(headless=False)

    try:
        # ç²å– Session
        print("ğŸ” æ­£åœ¨ç²å– WOS Session ID...")
        print("â³ è«‹åœ¨æ¥ä¸‹ä¾†çš„ 30 ç§’å…§ç¢ºèªå·²ç™»å…¥ Web of Science\n")

        if not scraper.get_session(wait_time=30):
            print("âŒ ç„¡æ³•ç²å– Session ID")
            return

        print("\n" + "="*80)
        print("âœ… Session ç²å–æˆåŠŸï¼é–‹å§‹åŸ·è¡Œ 4 çµ„ç¬¬äºŒè¼ªè£œå……æœå°‹...")
        print("="*80 + "\n")

        all_results = []
        total_papers = 0

        # åŸ·è¡Œæ¯çµ„æœå°‹
        for i, search in enumerate(searches, 1):
            print(f"\n{'='*80}")
            print(f"ç¬¬äºŒè¼ªè£œå…… {i}/4: {search['description']}")
            print(f"æŸ¥è©¢: {search['query']} AND PY={search['year_filter']}")
            print(f"ç›®æ¨™: {search['max_results']} ç¯‡")
            print('='*80)

            # åŸ·è¡Œæœå°‹
            papers = scraper.search_api(
                query=search['query'],
                max_results=search['max_results'],
                exclude_conference=True
            )

            # æ‰‹å‹•éæ¿¾å¹´ä»½
            if papers and search.get('year_filter'):
                year_range = search['year_filter'].strip('()')
                start_year, end_year = map(int, year_range.split('-'))
                papers = [p for p in papers if p.get('year') != 'N/A' and start_year <= int(p['year']) <= end_year]
                print(f"   (å¹´ä»½éæ¿¾å¾Œå‰©é¤˜ {len(papers)} ç¯‡)")

            if papers:
                print(f"âœ… æˆåŠŸæŠ“å– {len(papers)} ç¯‡æ–‡ç»")

                # ä¿å­˜çµæœ
                filename = f"{output_dir}/{search['id']}_{search['description'].split('ï¼ˆ')[0].replace(' ', '_')}"
                scraper.save_results(papers, filename)

                all_results.extend(papers)
                total_papers += len(papers)

                # é¡¯ç¤ºå‰ 3 ç¯‡
                print(f"\nå‰ 3 ç¯‡æ–‡ç»:")
                for j, paper in enumerate(papers[:3], 1):
                    print(f"  {j}. [{paper['citations']} å¼•ç”¨] {paper['title'][:60]}...")

            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°æ–‡ç»")

            # é¿å…è«‹æ±‚éå¿«
            if i < len(searches):
                print(f"\nâ³ ç­‰å¾… 3 ç§’å¾Œç¹¼çºŒä¸‹ä¸€çµ„æœå°‹...")
                time.sleep(3)

        # æœ€çµ‚çµ±è¨ˆ
        print("\n" + "="*80)
        print("ğŸ‰ ç¬¬äºŒè¼ªè£œå……æœå°‹å®Œæˆï¼")
        print("="*80)
        print(f"\nğŸ“Š ç¬¬äºŒè¼ªç¸½è¨ˆ:")
        print(f"  - åŸ·è¡Œæœå°‹çµ„æ•¸: {len(searches)} çµ„")
        print(f"  - æŠ“å–æ–‡ç»ç¸½æ•¸: {total_papers} ç¯‡")

        # å¹´ä»½åˆ†å¸ƒ
        from collections import Counter
        years = [p['year'] for p in all_results if p['year'] != 'N/A']
        year_counts = Counter(years)

        print(f"\nğŸ“… å¹´ä»½åˆ†å¸ƒ:")
        for year in sorted(year_counts.keys(), reverse=True)[:10]:
            print(f"  {year}: {year_counts[year]} ç¯‡")

        # å¼•ç”¨æ•¸çµ±è¨ˆ
        citations = [p['citations'] for p in all_results]
        if citations:
            print(f"\nğŸ“ˆ å¼•ç”¨æ•¸çµ±è¨ˆ:")
            print(f"  - ç¸½å¼•ç”¨æ•¸: {sum(citations)}")
            print(f"  - å¹³å‡å¼•ç”¨: {sum(citations) / len(citations):.1f}")
            print(f"  - æœ€é«˜å¼•ç”¨: {max(citations)}")

        print("\n" + "="*80)
        print("ğŸ’¾ ç¬¬äºŒè¼ªè£œå……æœå°‹çµæœå·²ä¿å­˜")
        print("="*80)

        print("\nğŸ“Œ ä¸‹ä¸€æ­¥ï¼š")
        print("  1. åˆä½µæ‰€æœ‰æ–‡ç»ï¼ˆåŸå§‹ + ç¬¬ä¸€è¼ªè£œå…… + ç¬¬äºŒè¼ªè£œå……ï¼‰")
        print(f"  2. é è¨ˆç¸½æ–‡ç»æ•¸: 118 + {total_papers} = {118 + total_papers} ç¯‡")
        print("  3. å»é‡å¾Œæœ€çµ‚è©•ä¼°")

    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

    finally:
        scraper.close()
        print("\nğŸ”’ ç€è¦½å™¨å·²é—œé–‰")


if __name__ == "__main__":
    main()
