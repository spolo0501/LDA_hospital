#!/usr/bin/env python3
"""
ä¸»é¡Œæ•¸é‡å„ªåŒ–åˆ†æž
æ¯”è¼ƒ 5ã€6ã€7 å€‹ä¸»é¡Œçš„ LDA æ¨¡åž‹ï¼Œæ‰¾å‡ºæœ€é©åˆæ§‹é¢è§£é‡‹çš„ä¸»é¡Œæ•¸é‡
ç‚ºå°ç¾Žæ¯”è¼ƒç ”ç©¶åšæº–å‚™
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ–‡æœ¬è™•ç†
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# LDA ç›¸é—œ
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# ä¸»é¡Œä¸€è‡´æ€§è©•ä¼°
try:
    import gensim
    from gensim.models import CoherenceModel
    from gensim.corpora import Dictionary
    GENSIM_AVAILABLE = True
except ImportError:
    GENSIM_AVAILABLE = False
    print("âš ï¸  Gensim æœªå®‰è£ï¼Œå°‡è·³éŽ Coherence Score è¨ˆç®—")

# è¨­å®š
import os
os.makedirs('topic_optimization_results', exist_ok=True)

# è¨­å®šç¹ªåœ–æ¨£å¼
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

print("=" * 80)
print("ðŸ” ä¸»é¡Œæ•¸é‡å„ªåŒ–åˆ†æž - ç‚ºå°ç¾Žæ¯”è¼ƒåšæº–å‚™")
print("=" * 80)
print()

# ============================================================================
# 1. è³‡æ–™è¼‰å…¥èˆ‡å‰è™•ç†
# ============================================================================
print("=" * 80)
print("ðŸ“Š 1. è³‡æ–™è¼‰å…¥èˆ‡å‰è™•ç†")
print("=" * 80)
print()

print("ðŸ“‚ è®€å–è³‡æ–™...")
df = pd.read_csv('cleaned_data_no_dedup/final_cleaned_sample_no_dedup.csv', encoding='utf-8-sig')
print(f"âœ… è³‡æ–™è¼‰å…¥å®Œæˆ: {len(df):,} æ¢è©•è«–")
print()

# åªä¿ç•™è‹±æ–‡è©•è«–
df_en = df[df['èªžè¨€'] == 'en'].copy()
print(f"ðŸ“ ç¯©é¸è‹±æ–‡è©•è«–: {len(df_en):,} æ¢ ({len(df_en)/len(df)*100:.1f}%)")
print()

# ============================================================================
# 2. æ–‡æœ¬å‰è™•ç†
# ============================================================================
print("=" * 80)
print("ðŸ“Š 2. æ–‡æœ¬å‰è™•ç†")
print("=" * 80)
print()

def preprocess_text(text):
    """æ–‡æœ¬å‰è™•ç†"""
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\S+@\S+', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

print("ðŸ”„ åŸ·è¡Œæ–‡æœ¬å‰è™•ç†...")
df_en['cleaned_text'] = df_en['è©•è«–å…§å®¹'].apply(preprocess_text)

# åœç”¨è©ž
stop_words = set(stopwords.words('english'))
custom_stopwords = {'hospital', 'dr', 'doctor', 'patient', 'visited', 'visit',
                    'one', 'would', 'get', 'like', 'go', 'went', 'take', 'make'}
stop_words.update(custom_stopwords)

lemmatizer = WordNetLemmatizer()

def tokenize_and_lemmatize(text):
    """åˆ†è©žå’Œè©žå½¢é‚„åŽŸ"""
    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(token) for token in tokens
              if token not in stop_words and len(token) > 2]
    return ' '.join(tokens)

print("ðŸ”„ åŸ·è¡Œåˆ†è©žå’Œè©žå½¢é‚„åŽŸ...")
df_en['processed_text'] = df_en['cleaned_text'].apply(tokenize_and_lemmatize)
df_en = df_en[df_en['processed_text'].str.len() > 20].copy()
print(f"âœ… å‰è™•ç†å®Œæˆ: {len(df_en):,} æ¢æœ‰æ•ˆè©•è«–")
print()

# åˆ†åˆ¥è™•ç†æ­£é¢å’Œè² é¢è©•è«–
df_positive = df_en[df_en['è©•åˆ†'] >= 4].copy()
df_negative = df_en[df_en['è©•åˆ†'] <= 2].copy()

print(f"ðŸ“Š æ­£é¢è©•è«–: {len(df_positive):,} æ¢")
print(f"ðŸ“Š è² é¢è©•è«–: {len(df_negative):,} æ¢")
print()

# ============================================================================
# 3. æ¸¬è©¦ä¸åŒä¸»é¡Œæ•¸é‡ (5, 6, 7)
# ============================================================================
print("=" * 80)
print("ðŸ“Š 3. æ¸¬è©¦ä¸åŒä¸»é¡Œæ•¸é‡ (5, 6, 7)")
print("=" * 80)
print()

def evaluate_lda_model(texts, n_topics, sentiment_label):
    """è©•ä¼° LDA æ¨¡åž‹"""
    print(f"\n{'=' * 80}")
    print(f"ðŸ” æ¸¬è©¦ {n_topics} å€‹ä¸»é¡Œ ({sentiment_label})")
    print(f"{'=' * 80}\n")

    # å‰µå»ºæ–‡æª”-è©žé »çŸ©é™£
    vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)
    doc_term_matrix = vectorizer.fit_transform(texts)

    # LDA æ¨¡åž‹
    lda_model = LatentDirichletAllocation(
        n_components=n_topics,
        random_state=42,
        max_iter=20,
        learning_method='online',
        n_jobs=-1
    )

    lda_output = lda_model.fit_transform(doc_term_matrix)

    # è¨ˆç®— Perplexityï¼ˆè¶Šä½Žè¶Šå¥½ï¼‰
    perplexity = lda_model.perplexity(doc_term_matrix)

    # è¨ˆç®— Log Likelihoodï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
    log_likelihood = lda_model.score(doc_term_matrix)

    # æå–ä¸»é¡Œè©ž
    feature_names = vectorizer.get_feature_names_out()
    topics = []

    print(f"ðŸ“‹ ä¸»é¡Œåˆ†æžçµæžœ:")
    print("-" * 80)

    for topic_idx, topic in enumerate(lda_model.components_):
        top_indices = topic.argsort()[-10:][::-1]
        top_words = [feature_names[i] for i in top_indices]
        top_weights = [topic[i] for i in top_indices]
        topics.append({
            'words': top_words,
            'weights': top_weights
        })

        print(f"\nä¸»é¡Œ {topic_idx + 1}:")
        print(f"  é—œéµè©ž: {', '.join(top_words[:10])}")

        # è¨ˆç®—ä¸»é¡Œå…§ä¸€è‡´æ€§ï¼ˆè©žæ¬Šé‡çš„æ¨™æº–å·®ï¼Œè¶Šä½Žè¡¨ç¤ºè¶Šé›†ä¸­ï¼‰
        topic_concentration = np.std(top_weights)
        print(f"  ä¸»é¡Œé›†ä¸­åº¦: {topic_concentration:.4f}")

    print()

    # è¨ˆç®—ä¸»é¡Œåˆ†å¸ƒçš„å‡å‹»æ€§ï¼ˆè¶Šå‡å‹»è¡¨ç¤ºä¸»é¡Œå€åˆ†åº¦è¶Šå¥½ï¼‰
    topic_distribution = lda_output.mean(axis=0)
    topic_uniformity = np.std(topic_distribution)

    print(f"ðŸ“Š æ¨¡åž‹è©•ä¼°æŒ‡æ¨™:")
    print(f"   Perplexity: {perplexity:.2f} (è¶Šä½Žè¶Šå¥½)")
    print(f"   Log Likelihood: {log_likelihood:.2f} (è¶Šé«˜è¶Šå¥½)")
    print(f"   ä¸»é¡Œåˆ†å¸ƒæ¨™æº–å·®: {topic_uniformity:.4f} (é©ä¸­ç‚ºä½³)")
    print()

    # è¨ˆç®— Coherence Scoreï¼ˆå¦‚æžœ Gensim å¯ç”¨ï¼‰
    coherence_score = None
    if GENSIM_AVAILABLE:
        try:
            # æº–å‚™æ–‡æœ¬ç”¨æ–¼ Coherence è¨ˆç®—
            texts_tokenized = [text.split() for text in texts]
            dictionary = Dictionary(texts_tokenized)

            # è½‰æ›ä¸»é¡Œç‚º Gensim æ ¼å¼
            topics_for_coherence = [[word for word in topic['words'][:10]] for topic in topics]

            # è¨ˆç®— Coherence Score (C_v)
            cm = CoherenceModel(
                topics=topics_for_coherence,
                texts=texts_tokenized,
                dictionary=dictionary,
                coherence='c_v'
            )
            coherence_score = cm.get_coherence()
            print(f"   Coherence Score (C_v): {coherence_score:.4f} (è¶Šé«˜è¶Šå¥½)")
            print()
        except Exception as e:
            print(f"   âš ï¸  Coherence Score è¨ˆç®—å¤±æ•—: {e}")
            print()

    return {
        'n_topics': n_topics,
        'sentiment': sentiment_label,
        'lda_model': lda_model,
        'vectorizer': vectorizer,
        'topics': topics,
        'perplexity': perplexity,
        'log_likelihood': log_likelihood,
        'topic_uniformity': topic_uniformity,
        'coherence_score': coherence_score,
        'lda_output': lda_output
    }

# æ¸¬è©¦æ­£é¢è©•è«–çš„ä¸åŒä¸»é¡Œæ•¸é‡
results_positive = []
for n_topics in [5, 6, 7]:
    result = evaluate_lda_model(
        df_positive['processed_text'].values,
        n_topics,
        'æ­£é¢è©•è«–'
    )
    results_positive.append(result)

# æ¸¬è©¦è² é¢è©•è«–çš„ä¸åŒä¸»é¡Œæ•¸é‡
results_negative = []
for n_topics in [5, 6, 7]:
    result = evaluate_lda_model(
        df_negative['processed_text'].values,
        n_topics,
        'è² é¢è©•è«–'
    )
    results_negative.append(result)

# ============================================================================
# 4. æ¯”è¼ƒåˆ†æžèˆ‡è¦–è¦ºåŒ–
# ============================================================================
print("=" * 80)
print("ðŸ“Š 4. æ¯”è¼ƒåˆ†æžèˆ‡è¦–è¦ºåŒ–")
print("=" * 80)
print()

# å‰µå»ºæ¯”è¼ƒè¡¨æ ¼
comparison_data = []
for result in results_positive + results_negative:
    comparison_data.append({
        'æƒ…æ„Ÿ': result['sentiment'],
        'ä¸»é¡Œæ•¸': result['n_topics'],
        'Perplexity': result['perplexity'],
        'Log Likelihood': result['log_likelihood'],
        'ä¸»é¡Œåˆ†å¸ƒæ¨™æº–å·®': result['topic_uniformity'],
        'Coherence Score': result['coherence_score'] if result['coherence_score'] else np.nan
    })

comparison_df = pd.DataFrame(comparison_data)

print("ðŸ“Š æ¨¡åž‹æ¯”è¼ƒè¡¨:")
print("-" * 80)
print(comparison_df.to_string(index=False))
print()

# è¦–è¦ºåŒ–æ¯”è¼ƒ
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# æ­£é¢è©•è«–æŒ‡æ¨™
pos_results = [r for r in results_positive]
n_topics_list = [5, 6, 7]

# 1. Perplexity æ¯”è¼ƒ - æ­£é¢
ax1 = axes[0, 0]
perplexity_pos = [r['perplexity'] for r in pos_results]
bars = ax1.bar(n_topics_list, perplexity_pos, color='lightgreen', edgecolor='black', linewidth=1.5)
ax1.set_xlabel('Number of Topics', fontsize=12, fontweight='bold')
ax1.set_ylabel('Perplexity (Lower is Better)', fontsize=12, fontweight='bold')
ax1.set_title('Positive Reviews - Perplexity', fontsize=14, fontweight='bold', pad=15)
ax1.set_xticks(n_topics_list)
ax1.grid(axis='y', alpha=0.3)
for bar, value in zip(bars, perplexity_pos):
    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
             f'{value:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

# 2. Log Likelihood æ¯”è¼ƒ - æ­£é¢
ax2 = axes[0, 1]
log_lik_pos = [r['log_likelihood'] for r in pos_results]
bars = ax2.bar(n_topics_list, log_lik_pos, color='lightblue', edgecolor='black', linewidth=1.5)
ax2.set_xlabel('Number of Topics', fontsize=12, fontweight='bold')
ax2.set_ylabel('Log Likelihood (Higher is Better)', fontsize=12, fontweight='bold')
ax2.set_title('Positive Reviews - Log Likelihood', fontsize=14, fontweight='bold', pad=15)
ax2.set_xticks(n_topics_list)
ax2.grid(axis='y', alpha=0.3)
for bar, value in zip(bars, log_lik_pos):
    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
             f'{value:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

# 3. Coherence Score æ¯”è¼ƒ - æ­£é¢
ax3 = axes[0, 2]
if GENSIM_AVAILABLE and all(r['coherence_score'] for r in pos_results):
    coherence_pos = [r['coherence_score'] for r in pos_results]
    bars = ax3.bar(n_topics_list, coherence_pos, color='lightyellow', edgecolor='black', linewidth=1.5)
    ax3.set_xlabel('Number of Topics', fontsize=12, fontweight='bold')
    ax3.set_ylabel('Coherence Score (Higher is Better)', fontsize=12, fontweight='bold')
    ax3.set_title('Positive Reviews - Coherence', fontsize=14, fontweight='bold', pad=15)
    ax3.set_xticks(n_topics_list)
    ax3.grid(axis='y', alpha=0.3)
    for bar, value in zip(bars, coherence_pos):
        ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
                 f'{value:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
else:
    ax3.text(0.5, 0.5, 'Coherence Score\nNot Available',
             ha='center', va='center', fontsize=12, transform=ax3.transAxes)
    ax3.axis('off')

# è² é¢è©•è«–æŒ‡æ¨™
neg_results = [r for r in results_negative]

# 4. Perplexity æ¯”è¼ƒ - è² é¢
ax4 = axes[1, 0]
perplexity_neg = [r['perplexity'] for r in neg_results]
bars = ax4.bar(n_topics_list, perplexity_neg, color='lightcoral', edgecolor='black', linewidth=1.5)
ax4.set_xlabel('Number of Topics', fontsize=12, fontweight='bold')
ax4.set_ylabel('Perplexity (Lower is Better)', fontsize=12, fontweight='bold')
ax4.set_title('Negative Reviews - Perplexity', fontsize=14, fontweight='bold', pad=15)
ax4.set_xticks(n_topics_list)
ax4.grid(axis='y', alpha=0.3)
for bar, value in zip(bars, perplexity_neg):
    ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
             f'{value:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

# 5. Log Likelihood æ¯”è¼ƒ - è² é¢
ax5 = axes[1, 1]
log_lik_neg = [r['log_likelihood'] for r in neg_results]
bars = ax5.bar(n_topics_list, log_lik_neg, color='lightblue', edgecolor='black', linewidth=1.5)
ax5.set_xlabel('Number of Topics', fontsize=12, fontweight='bold')
ax5.set_ylabel('Log Likelihood (Higher is Better)', fontsize=12, fontweight='bold')
ax5.set_title('Negative Reviews - Log Likelihood', fontsize=14, fontweight='bold', pad=15)
ax5.set_xticks(n_topics_list)
ax5.grid(axis='y', alpha=0.3)
for bar, value in zip(bars, log_lik_neg):
    ax5.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
             f'{value:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

# 6. Coherence Score æ¯”è¼ƒ - è² é¢
ax6 = axes[1, 2]
if GENSIM_AVAILABLE and all(r['coherence_score'] for r in neg_results):
    coherence_neg = [r['coherence_score'] for r in neg_results]
    bars = ax6.bar(n_topics_list, coherence_neg, color='lightyellow', edgecolor='black', linewidth=1.5)
    ax6.set_xlabel('Number of Topics', fontsize=12, fontweight='bold')
    ax6.set_ylabel('Coherence Score (Higher is Better)', fontsize=12, fontweight='bold')
    ax6.set_title('Negative Reviews - Coherence', fontsize=14, fontweight='bold', pad=15)
    ax6.set_xticks(n_topics_list)
    ax6.grid(axis='y', alpha=0.3)
    for bar, value in zip(bars, coherence_neg):
        ax6.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
                 f'{value:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
else:
    ax6.text(0.5, 0.5, 'Coherence Score\nNot Available',
             ha='center', va='center', fontsize=12, transform=ax6.transAxes)
    ax6.axis('off')

plt.tight_layout()
plt.savefig('topic_optimization_results/topic_number_comparison.png', dpi=300, bbox_inches='tight')
print("âœ… æ¯”è¼ƒåœ–å·²å„²å­˜: topic_optimization_results/topic_number_comparison.png")
print()

# ============================================================================
# 5. æ§‹é¢è§£é‡‹æ€§åˆ†æž
# ============================================================================
print("=" * 80)
print("ðŸ“Š 5. æ§‹é¢è§£é‡‹æ€§åˆ†æž")
print("=" * 80)
print()

def analyze_topic_interpretability(results, sentiment_label):
    """åˆ†æžä¸»é¡Œçš„å¯è§£é‡‹æ€§"""
    print(f"\n{'=' * 80}")
    print(f"ðŸ“‹ {sentiment_label} - ä¸»é¡Œæ§‹é¢è§£é‡‹æ€§åˆ†æž")
    print(f"{'=' * 80}\n")

    for result in results:
        n_topics = result['n_topics']
        print(f"\n## {n_topics} å€‹ä¸»é¡Œçš„æ§‹é¢è§£é‡‹:\n")

        for topic_idx, topic_info in enumerate(result['topics']):
            words = topic_info['words'][:10]
            weights = topic_info['weights'][:10]

            print(f"### ä¸»é¡Œ {topic_idx + 1}")
            print(f"é—œéµè©ž: {', '.join(words)}")

            # å˜—è©¦çµ¦å‡ºæ§‹é¢è§£é‡‹
            # é€™è£¡å¯ä»¥æ ¹æ“šé—œéµè©žè‡ªå‹•åˆ¤æ–·æ§‹é¢é¡žåž‹
            # æˆ–è€…ç•™ç©ºè®“ç ”ç©¶è€…è‡ªè¡Œå‘½å
            print(f"å»ºè­°æ§‹é¢åç¨±: [å¾…å‘½å]")
            print(f"è©žæ¬Šé‡ç¯„åœ: {weights[0]:.3f} - {weights[-1]:.3f}")
            print()

        print(f"**å¯è§£é‡‹æ€§è©•ä¼°**: ")
        print(f"- Perplexity: {result['perplexity']:.2f}")
        if result['coherence_score']:
            print(f"- Coherence Score: {result['coherence_score']:.4f}")
        print(f"- ä¸»é¡Œåˆ†å¸ƒå‡å‹»åº¦: {result['topic_uniformity']:.4f}")
        print()

# åˆ†æžæ­£é¢å’Œè² é¢è©•è«–çš„å¯è§£é‡‹æ€§
analyze_topic_interpretability(results_positive, "æ­£é¢è©•è«–")
analyze_topic_interpretability(results_negative, "è² é¢è©•è«–")

# ============================================================================
# 6. ç”Ÿæˆè©³ç´°å ±å‘Š
# ============================================================================
print("=" * 80)
print("ðŸ“Š 6. ç”Ÿæˆè©³ç´°å ±å‘Š")
print("=" * 80)
print()

# ç¢ºå®šæŽ¨è–¦çš„ä¸»é¡Œæ•¸é‡
def recommend_n_topics(results):
    """åŸºæ–¼æŒ‡æ¨™æŽ¨è–¦ä¸»é¡Œæ•¸é‡"""
    scores = []
    for result in results:
        # æ¨™æº–åŒ–å„æŒ‡æ¨™ï¼ˆè¶Šä½Ž/è¶Šé«˜è¶Šå¥½ï¼‰
        score = 0

        # Perplexityï¼ˆè¶Šä½Žè¶Šå¥½ï¼Œå–è² å€¼ï¼‰
        perplexities = [r['perplexity'] for r in results]
        norm_perplexity = (max(perplexities) - result['perplexity']) / (max(perplexities) - min(perplexities)) if max(perplexities) != min(perplexities) else 0
        score += norm_perplexity * 0.3

        # Log Likelihoodï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        log_liks = [r['log_likelihood'] for r in results]
        norm_log_lik = (result['log_likelihood'] - min(log_liks)) / (max(log_liks) - min(log_liks)) if max(log_liks) != min(log_liks) else 0
        score += norm_log_lik * 0.3

        # Coherence Scoreï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        if result['coherence_score']:
            coherences = [r['coherence_score'] for r in results if r['coherence_score']]
            norm_coherence = (result['coherence_score'] - min(coherences)) / (max(coherences) - min(coherences)) if max(coherences) != min(coherences) else 0
            score += norm_coherence * 0.4

        scores.append(score)

    best_idx = np.argmax(scores)
    return results[best_idx]['n_topics'], scores

recommended_pos, scores_pos = recommend_n_topics(results_positive)
recommended_neg, scores_neg = recommend_n_topics(results_negative)

print(f"ðŸ“Š æŽ¨è–¦ä¸»é¡Œæ•¸é‡:")
print(f"   æ­£é¢è©•è«–: {recommended_pos} å€‹ä¸»é¡Œ (ç¶œåˆè©•åˆ†: {max(scores_pos):.3f})")
print(f"   è² é¢è©•è«–: {recommended_neg} å€‹ä¸»é¡Œ (ç¶œåˆè©•åˆ†: {max(scores_neg):.3f})")
print()

# ç”Ÿæˆå ±å‘Š
report = f"""
# ðŸ” ä¸»é¡Œæ•¸é‡å„ªåŒ–åˆ†æžå ±å‘Š

**ç”Ÿæˆæ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**åˆ†æžç›®çš„**: ç‚ºå°ç¾Žæ¯”è¼ƒç ”ç©¶ç¢ºå®šæœ€ä½³ä¸»é¡Œæ•¸é‡
**è³‡æ–™ä¾†æº**: ç¾Žåœ‹é†«é™¢è©•è«– (3,363 æ¢)

---

## 1. åˆ†æžæ¦‚è¦½

### æ¸¬è©¦ä¸»é¡Œæ•¸é‡
- 5 å€‹ä¸»é¡Œ
- 6 å€‹ä¸»é¡Œ
- 7 å€‹ä¸»é¡Œ

### è©•ä¼°æŒ‡æ¨™
- **Perplexity**: æ¨¡åž‹è¤‡é›œåº¦ï¼ˆè¶Šä½Žè¶Šå¥½ï¼‰
- **Log Likelihood**: æ¨¡åž‹æ“¬åˆåº¦ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
- **Coherence Score**: ä¸»é¡Œä¸€è‡´æ€§ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
- **ä¸»é¡Œåˆ†å¸ƒå‡å‹»åº¦**: ä¸»é¡Œå€åˆ†åº¦ï¼ˆé©ä¸­ç‚ºä½³ï¼‰

---

## 2. æ­£é¢è©•è«–åˆ†æžçµæžœ

### æ¨¡åž‹æ¯”è¼ƒ

| ä¸»é¡Œæ•¸ | Perplexity | Log Likelihood | Coherence Score | æŽ¨è–¦åº¦ |
|--------|-----------|----------------|-----------------|--------|
"""

for i, result in enumerate(results_positive):
    coherence = f"{result['coherence_score']:.4f}" if result['coherence_score'] else "N/A"
    recommended_mark = "â­ **æŽ¨è–¦**" if result['n_topics'] == recommended_pos else ""
    report += f"| {result['n_topics']} | {result['perplexity']:.2f} | {result['log_likelihood']:.2f} | {coherence} | {recommended_mark} |\n"

report += f"""

### æŽ¨è–¦ä¸»é¡Œæ•¸é‡: {recommended_pos} å€‹ â­

**ç†ç”±**:
"""

# æ‰¾å‡ºæŽ¨è–¦çš„çµæžœ
recommended_result_pos = [r for r in results_positive if r['n_topics'] == recommended_pos][0]
report += f"""
- Perplexity: {recommended_result_pos['perplexity']:.2f}
- Log Likelihood: {recommended_result_pos['log_likelihood']:.2f}
"""
if recommended_result_pos['coherence_score']:
    report += f"- Coherence Score: {recommended_result_pos['coherence_score']:.4f}\n"
report += f"- ä¸»é¡Œåˆ†å¸ƒå‡å‹»åº¦: {recommended_result_pos['topic_uniformity']:.4f}\n"

report += f"""

### ä¸»é¡Œæ§‹é¢ï¼ˆ{recommended_pos} å€‹ä¸»é¡Œï¼‰

"""

for topic_idx, topic_info in enumerate(recommended_result_pos['topics']):
    report += f"""
#### ä¸»é¡Œ {topic_idx + 1}
- **é—œéµè©ž**: {', '.join(topic_info['words'][:10])}
- **å»ºè­°æ§‹é¢åç¨±**: [å¾…ç ”ç©¶è€…å‘½å]
- **é©ç”¨æ–¼å°ç¾Žæ¯”è¼ƒ**: âœ…

"""

report += """

---

## 3. è² é¢è©•è«–åˆ†æžçµæžœ

### æ¨¡åž‹æ¯”è¼ƒ

| ä¸»é¡Œæ•¸ | Perplexity | Log Likelihood | Coherence Score | æŽ¨è–¦åº¦ |
|--------|-----------|----------------|-----------------|--------|
"""

for i, result in enumerate(results_negative):
    coherence = f"{result['coherence_score']:.4f}" if result['coherence_score'] else "N/A"
    recommended_mark = "â­ **æŽ¨è–¦**" if result['n_topics'] == recommended_neg else ""
    report += f"| {result['n_topics']} | {result['perplexity']:.2f} | {result['log_likelihood']:.2f} | {coherence} | {recommended_mark} |\n"

report += f"""

### æŽ¨è–¦ä¸»é¡Œæ•¸é‡: {recommended_neg} å€‹ â­

**ç†ç”±**:
"""

recommended_result_neg = [r for r in results_negative if r['n_topics'] == recommended_neg][0]
report += f"""
- Perplexity: {recommended_result_neg['perplexity']:.2f}
- Log Likelihood: {recommended_result_neg['log_likelihood']:.2f}
"""
if recommended_result_neg['coherence_score']:
    report += f"- Coherence Score: {recommended_result_neg['coherence_score']:.4f}\n"
report += f"- ä¸»é¡Œåˆ†å¸ƒå‡å‹»åº¦: {recommended_result_neg['topic_uniformity']:.4f}\n"

report += f"""

### ä¸»é¡Œæ§‹é¢ï¼ˆ{recommended_neg} å€‹ä¸»é¡Œï¼‰

"""

for topic_idx, topic_info in enumerate(recommended_result_neg['topics']):
    report += f"""
#### ä¸»é¡Œ {topic_idx + 1}
- **é—œéµè©ž**: {', '.join(topic_info['words'][:10])}
- **å»ºè­°æ§‹é¢åç¨±**: [å¾…ç ”ç©¶è€…å‘½å]
- **é©ç”¨æ–¼å°ç¾Žæ¯”è¼ƒ**: âœ…

"""

report += f"""

---

## 4. å°ç¾Žæ¯”è¼ƒå»ºè­°

### æŽ¨è–¦é…ç½®

**æ­£é¢è©•è«–**: {recommended_pos} å€‹ä¸»é¡Œ
**è² é¢è©•è«–**: {recommended_neg} å€‹ä¸»é¡Œ

### è·¨åœ‹æ¯”è¼ƒæ³¨æ„äº‹é …

1. **èªžè¨€å·®ç•°è™•ç†**
   - å°ç£è³‡æ–™éœ€è¦é€²è¡Œç¹é«”ä¸­æ–‡åˆ†è©ž
   - å»ºè­°ä½¿ç”¨ jieba æˆ– ckiptagger
   - åœç”¨è©žéœ€è¦å¦å¤–è¨­å®š

2. **ä¸»é¡Œæ•¸é‡ä¸€è‡´æ€§**
   - å»ºè­°å°ç£è³‡æ–™ä¹Ÿä½¿ç”¨ç›¸åŒçš„ä¸»é¡Œæ•¸é‡
   - ä¾¿æ–¼æ§‹é¢å°ç…§å’Œæ¯”è¼ƒ

3. **æ§‹é¢å‘½ååŽŸå‰‡**
   - ä½¿ç”¨ä¸­æ€§ã€è·¨æ–‡åŒ–çš„æ§‹é¢åç¨±
   - ä¾‹å¦‚ï¼šæœå‹™å“è³ªã€æºé€šæ•ˆçŽ‡ã€ç­‰å¾…æ™‚é–“ç­‰

4. **å¯æ¯”è¼ƒçš„æ§‹é¢ç¯„ä¾‹**
   - âœ… é†«ç™‚å°ˆæ¥­æ€§
   - âœ… æœå‹™æ…‹åº¦
   - âœ… ç­‰å¾…æ™‚é–“
   - âœ… æºé€šå“è³ª
   - âœ… è¨­æ–½ç’°å¢ƒ
   - âœ… è²»ç”¨é€æ˜Žåº¦

---

## 5. ä¸‹ä¸€æ­¥è¡Œå‹•

### ç«‹å³åŸ·è¡Œ

1. âœ… ä½¿ç”¨æŽ¨è–¦çš„ä¸»é¡Œæ•¸é‡åˆ†æžå°ç£è³‡æ–™
2. âœ… ç‚ºæ¯å€‹ä¸»é¡Œå‘½åæ§‹é¢
3. âœ… å»ºç«‹å°ç¾Žå°ç…§è¡¨

### åˆ†æžæµç¨‹

```python
# å°ç£è³‡æ–™åˆ†æžç¯„ä¾‹
# 1. è¼‰å…¥å°ç£é†«é™¢è©•è«–è³‡æ–™
# 2. ä½¿ç”¨ç›¸åŒçš„ä¸»é¡Œæ•¸é‡ ({recommended_pos} å€‹æ­£é¢, {recommended_neg} å€‹è² é¢)
# 3. é€²è¡Œ LDA åˆ†æž
# 4. æ¯”è¼ƒå°ç¾Žä¸»é¡Œå·®ç•°
```

### ç ”ç©¶å•é¡Œç¯„ä¾‹

1. å°ç¾Žå…©åœ‹åœ¨æ­£é¢è©•è«–ä¸­æœ€é—œæ³¨çš„æ§‹é¢æ˜¯å¦ç›¸åŒï¼Ÿ
2. è² é¢è©•è«–çš„ç—›é»žæ˜¯å¦æœ‰æ–‡åŒ–å·®ç•°ï¼Ÿ
3. å“ªäº›æœå‹™æ§‹é¢æ˜¯è·¨æ–‡åŒ–å…±é€šçš„ï¼Ÿ
4. å“ªäº›æ§‹é¢é¡¯ç¤ºå‡ºé¡¯è‘—çš„åœ‹å®¶å·®ç•°ï¼Ÿ

---

## 6. è¼¸å‡ºæª”æ¡ˆ

- `topic_optimization_results/topic_number_comparison.png` - ä¸»é¡Œæ•¸é‡æ¯”è¼ƒåœ–
- `topic_optimization_results/TOPIC_OPTIMIZATION_REPORT.md` - æœ¬å ±å‘Š
- `topic_optimization_results/recommended_models.pkl` - æŽ¨è–¦æ¨¡åž‹ï¼ˆå¾…å„²å­˜ï¼‰

---

**åˆ†æžå®Œæˆ** âœ…

**å»ºè­°**: ä½¿ç”¨ {recommended_pos} å€‹æ­£é¢ä¸»é¡Œå’Œ {recommended_neg} å€‹è² é¢ä¸»é¡Œé€²è¡Œå°ç¾Žæ¯”è¼ƒç ”ç©¶
"""

# å„²å­˜å ±å‘Š
report_file = 'topic_optimization_results/TOPIC_OPTIMIZATION_REPORT.md'
with open(report_file, 'w', encoding='utf-8') as f:
    f.write(report)

print(f"âœ… ä¸»é¡Œå„ªåŒ–å ±å‘Šå·²å„²å­˜: {report_file}")
print()

# å„²å­˜æŽ¨è–¦çš„æ¨¡åž‹
import pickle

models_to_save = {
    'positive': {
        'n_topics': recommended_pos,
        'model': recommended_result_pos['lda_model'],
        'vectorizer': recommended_result_pos['vectorizer'],
        'topics': recommended_result_pos['topics']
    },
    'negative': {
        'n_topics': recommended_neg,
        'model': recommended_result_neg['lda_model'],
        'vectorizer': recommended_result_neg['vectorizer'],
        'topics': recommended_result_neg['topics']
    }
}

with open('topic_optimization_results/recommended_models.pkl', 'wb') as f:
    pickle.dump(models_to_save, f)

print("âœ… æŽ¨è–¦æ¨¡åž‹å·²å„²å­˜: topic_optimization_results/recommended_models.pkl")
print()

# ============================================================================
# å®Œæˆ
# ============================================================================
print("=" * 80)
print("âœ… ä¸»é¡Œæ•¸é‡å„ªåŒ–åˆ†æžå®Œæˆï¼")
print("=" * 80)
print()
print(f"ðŸ“Š æŽ¨è–¦é…ç½®:")
print(f"   æ­£é¢è©•è«–: {recommended_pos} å€‹ä¸»é¡Œ")
print(f"   è² é¢è©•è«–: {recommended_neg} å€‹ä¸»é¡Œ")
print()
print("ðŸ“ è¼¸å‡ºæª”æ¡ˆ:")
print("   â€¢ topic_optimization_results/topic_number_comparison.png")
print("   â€¢ topic_optimization_results/TOPIC_OPTIMIZATION_REPORT.md")
print("   â€¢ topic_optimization_results/recommended_models.pkl")
print()
print("ðŸš€ æº–å‚™å¥½é€²è¡Œå°ç¾Žæ¯”è¼ƒåˆ†æžï¼")
print("=" * 80)
