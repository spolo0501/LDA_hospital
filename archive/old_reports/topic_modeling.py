#!/usr/bin/env python3
"""
ä¸»é¡Œå»ºæ¨¡åˆ†æž - LDA å’Œ BERTopic
åˆ†æžé†«é™¢è©•è«–ä¸­çš„ä¸»è¦è©±é¡Œå’Œä¸»é¡Œ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ–‡æœ¬è™•ç†
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# LDA ç›¸é—œ
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import pyLDAvis
import pyLDAvis.lda_model

# BERTopic ç›¸é—œï¼ˆå¦‚æžœå¯ç”¨ï¼‰
try:
    from bertopic import BERTopic
    from sentence_transformers import SentenceTransformer
    BERTOPIC_AVAILABLE = True
except ImportError:
    BERTOPIC_AVAILABLE = False
    print("âš ï¸  BERTopic æœªå®‰è£ï¼Œå°‡åªåŸ·è¡Œ LDA åˆ†æž")

# è¨­å®š
import os
os.makedirs('topic_modeling_results', exist_ok=True)

print("=" * 80)
print("ðŸ” ä¸»é¡Œå»ºæ¨¡åˆ†æž - LDA & BERTopic")
print("=" * 80)
print()

# ä¸‹è¼‰ NLTK è³‡æºï¼ˆå¦‚æžœå°šæœªä¸‹è¼‰ï¼‰
print("ðŸ“¥ æº–å‚™ NLTK è³‡æº...")
try:
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('omw-1.4', quiet=True)
    nltk.download('punkt_tab', quiet=True)
    print("âœ… NLTK è³‡æºæº–å‚™å®Œæˆ")
except Exception as e:
    print(f"âš ï¸  NLTK ä¸‹è¼‰è­¦å‘Š: {e}")
print()

# ============================================================================
# 1. è³‡æ–™è¼‰å…¥èˆ‡å‰è™•ç†
# ============================================================================
print("=" * 80)
print("ðŸ“Š 1. è³‡æ–™è¼‰å…¥èˆ‡å‰è™•ç†")
print("=" * 80)
print()

print("ðŸ“‚ è®€å–è³‡æ–™...")
df = pd.read_csv('cleaned_data_no_dedup/final_cleaned_sample_no_dedup.csv', encoding='utf-8-sig')
print(f"âœ… è³‡æ–™è¼‰å…¥å®Œæˆ: {len(df):,} æ¢è©•è«–")
print()

# åªä¿ç•™è‹±æ–‡è©•è«–ï¼ˆå› ç‚ºæ–‡æœ¬åˆ†æžå·¥å…·ä¸»è¦é‡å°è‹±æ–‡ï¼‰
df_en = df[df['èªžè¨€'] == 'en'].copy()
print(f"ðŸ“ ç¯©é¸è‹±æ–‡è©•è«–: {len(df_en):,} æ¢ ({len(df_en)/len(df)*100:.1f}%)")
print()

# ============================================================================
# 2. æ–‡æœ¬å‰è™•ç†
# ============================================================================
print("=" * 80)
print("ðŸ“Š 2. æ–‡æœ¬å‰è™•ç†")
print("=" * 80)
print()

def preprocess_text(text):
    """æ–‡æœ¬å‰è™•ç†"""
    # è½‰å°å¯«
    text = text.lower()

    # ç§»é™¤ URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # ç§»é™¤ email
    text = re.sub(r'\S+@\S+', '', text)

    # ç§»é™¤æ•¸å­—
    text = re.sub(r'\d+', '', text)

    # ç§»é™¤æ¨™é»žç¬¦è™Ÿä½†ä¿ç•™å¥å­çµæ§‹
    text = re.sub(r'[^\w\s]', ' ', text)

    # ç§»é™¤å¤šé¤˜ç©ºç™½
    text = re.sub(r'\s+', ' ', text).strip()

    return text

print("ðŸ”„ åŸ·è¡Œæ–‡æœ¬å‰è™•ç†...")
df_en['cleaned_text'] = df_en['è©•è«–å…§å®¹'].apply(preprocess_text)

# ç§»é™¤åœç”¨è©žå’Œè©žå½¢é‚„åŽŸ
stop_words = set(stopwords.words('english'))
# æ·»åŠ é†«é™¢ç›¸é—œçš„å¸¸è¦‹è©žï¼ˆå¯èƒ½ä¸å…·å€åˆ†æ€§ï¼‰
custom_stopwords = {'hospital', 'dr', 'doctor', 'patient', 'visited', 'visit', 'one', 'would', 'get', 'like', 'go', 'went'}
stop_words.update(custom_stopwords)

lemmatizer = WordNetLemmatizer()

def tokenize_and_lemmatize(text):
    """åˆ†è©žå’Œè©žå½¢é‚„åŽŸ"""
    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(token) for token in tokens
              if token not in stop_words and len(token) > 2]
    return ' '.join(tokens)

print("ðŸ”„ åŸ·è¡Œåˆ†è©žå’Œè©žå½¢é‚„åŽŸ...")
df_en['processed_text'] = df_en['cleaned_text'].apply(tokenize_and_lemmatize)

# ç§»é™¤éŽçŸ­çš„æ–‡æœ¬
df_en = df_en[df_en['processed_text'].str.len() > 20].copy()
print(f"âœ… å‰è™•ç†å®Œæˆ: {len(df_en):,} æ¢æœ‰æ•ˆè©•è«–")
print()

# ============================================================================
# 3. LDA ä¸»é¡Œå»ºæ¨¡
# ============================================================================
print("=" * 80)
print("ðŸ“Š 3. LDA ä¸»é¡Œå»ºæ¨¡")
print("=" * 80)
print()

# åˆ†åˆ¥å°æ­£é¢å’Œè² é¢è©•è«–é€²è¡Œä¸»é¡Œå»ºæ¨¡
df_positive = df_en[df_en['è©•åˆ†'] >= 4].copy()
df_negative = df_en[df_en['è©•åˆ†'] <= 2].copy()

print(f"ðŸ“Š æ­£é¢è©•è«–: {len(df_positive):,} æ¢")
print(f"ðŸ“Š è² é¢è©•è«–: {len(df_negative):,} æ¢")
print()

def perform_lda(texts, n_topics=5, n_top_words=10, label=''):
    """åŸ·è¡Œ LDA ä¸»é¡Œå»ºæ¨¡"""
    print(f"ðŸ” åŸ·è¡Œ LDA åˆ†æž ({label})...")

    # å‰µå»ºæ–‡æª”-è©žé »çŸ©é™£
    vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)
    doc_term_matrix = vectorizer.fit_transform(texts)

    # LDA æ¨¡åž‹
    lda_model = LatentDirichletAllocation(
        n_components=n_topics,
        random_state=42,
        max_iter=20,
        learning_method='online',
        n_jobs=-1
    )

    lda_output = lda_model.fit_transform(doc_term_matrix)

    # æå–ä¸»é¡Œè©ž
    feature_names = vectorizer.get_feature_names_out()
    topics = []

    print(f"\nðŸ“‹ {label} - ä¸»é¡Œåˆ†æžçµæžœ:")
    print("-" * 80)

    for topic_idx, topic in enumerate(lda_model.components_):
        top_indices = topic.argsort()[-n_top_words:][::-1]
        top_words = [feature_names[i] for i in top_indices]
        topics.append(top_words)

        print(f"\nä¸»é¡Œ {topic_idx + 1}:")
        print(f"  é—œéµè©ž: {', '.join(top_words[:10])}")

    print()

    return lda_model, vectorizer, doc_term_matrix, lda_output, topics

# å°æ­£é¢è©•è«–é€²è¡Œ LDA
print("\n" + "=" * 80)
print("ðŸ“Š æ­£é¢è©•è«–ä¸»é¡Œåˆ†æž (LDA)")
print("=" * 80)
lda_pos, vec_pos, dtm_pos, output_pos, topics_pos = perform_lda(
    df_positive['processed_text'].values,
    n_topics=5,
    label='æ­£é¢è©•è«–'
)

# å°è² é¢è©•è«–é€²è¡Œ LDA
print("\n" + "=" * 80)
print("ðŸ“Š è² é¢è©•è«–ä¸»é¡Œåˆ†æž (LDA)")
print("=" * 80)
lda_neg, vec_neg, dtm_neg, output_neg, topics_neg = perform_lda(
    df_negative['processed_text'].values,
    n_topics=5,
    label='è² é¢è©•è«–'
)

# ============================================================================
# 4. LDA è¦–è¦ºåŒ–
# ============================================================================
print("=" * 80)
print("ðŸ“Š 4. LDA è¦–è¦ºåŒ–")
print("=" * 80)
print()

# 4.1 ä¸»é¡Œåˆ†å¸ƒç†±åœ– - æ­£é¢è©•è«–
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# æ­£é¢è©•è«– - ä¸»é¡Œåˆ†å¸ƒ
ax1 = axes[0, 0]
topic_dist_pos = output_pos.mean(axis=0)
colors_pos = plt.cm.Greens(np.linspace(0.4, 0.8, len(topic_dist_pos)))
bars = ax1.bar(range(1, len(topic_dist_pos) + 1), topic_dist_pos, color=colors_pos, edgecolor='black', linewidth=1.5)
ax1.set_xlabel('Topic Number', fontsize=12, fontweight='bold')
ax1.set_ylabel('Average Probability', fontsize=12, fontweight='bold')
ax1.set_title('Positive Reviews - Topic Distribution (LDA)', fontsize=14, fontweight='bold', pad=15)
ax1.set_xticks(range(1, len(topic_dist_pos) + 1))
ax1.grid(axis='y', alpha=0.3)

for bar, value in zip(bars, topic_dist_pos):
    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
             f'{value:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

# è² é¢è©•è«– - ä¸»é¡Œåˆ†å¸ƒ
ax2 = axes[0, 1]
topic_dist_neg = output_neg.mean(axis=0)
colors_neg = plt.cm.Reds(np.linspace(0.4, 0.8, len(topic_dist_neg)))
bars = ax2.bar(range(1, len(topic_dist_neg) + 1), topic_dist_neg, color=colors_neg, edgecolor='black', linewidth=1.5)
ax2.set_xlabel('Topic Number', fontsize=12, fontweight='bold')
ax2.set_ylabel('Average Probability', fontsize=12, fontweight='bold')
ax2.set_title('Negative Reviews - Topic Distribution (LDA)', fontsize=14, fontweight='bold', pad=15)
ax2.set_xticks(range(1, len(topic_dist_neg) + 1))
ax2.grid(axis='y', alpha=0.3)

for bar, value in zip(bars, topic_dist_neg):
    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
             f'{value:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

# 4.2 ä¸»é¡Œé—œéµè©ž - æ­£é¢è©•è«–å‰3å€‹ä¸»é¡Œ
ax3 = axes[1, 0]
ax3.axis('off')
ax3.set_title('Positive Reviews - Top 3 Topics Keywords', fontsize=14, fontweight='bold', pad=15)

# å‰µå»ºä¸»é¡Œè©žæ¬Šé‡æ–‡æœ¬
topic_text = ""
for topic_idx in range(min(3, len(topics_pos))):
    topic_text += f"\nTopic {topic_idx + 1}:\n"
    topic = lda_pos.components_[topic_idx]
    feature_names = vec_pos.get_feature_names_out()
    top_indices = topic.argsort()[-15:][::-1]
    for idx in top_indices:
        weight = topic[idx]
        topic_text += f"  â€¢ {feature_names[idx]}: {weight:.3f}\n"

ax3.text(0.05, 0.95, topic_text, transform=ax3.transAxes, fontsize=9,
         verticalalignment='top', fontfamily='monospace',
         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))

# 4.3 ä¸»é¡Œé—œéµè©ž - è² é¢è©•è«–å‰3å€‹ä¸»é¡Œ
ax4 = axes[1, 1]
ax4.axis('off')
ax4.set_title('Negative Reviews - Top 3 Topics Keywords', fontsize=14, fontweight='bold', pad=15)

topic_text_neg = ""
for topic_idx in range(min(3, len(topics_neg))):
    topic_text_neg += f"\nTopic {topic_idx + 1}:\n"
    topic = lda_neg.components_[topic_idx]
    feature_names = vec_neg.get_feature_names_out()
    top_indices = topic.argsort()[-15:][::-1]
    for idx in top_indices:
        weight = topic[idx]
        topic_text_neg += f"  â€¢ {feature_names[idx]}: {weight:.3f}\n"

ax4.text(0.05, 0.95, topic_text_neg, transform=ax4.transAxes, fontsize=9,
         verticalalignment='top', fontfamily='monospace',
         bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.3))

plt.tight_layout()
plt.savefig('topic_modeling_results/lda_topics_analysis.png', dpi=300, bbox_inches='tight')
print("âœ… LDA ä¸»é¡Œåˆ†æžåœ–å·²å„²å­˜: topic_modeling_results/lda_topics_analysis.png")
print()

# ============================================================================
# 5. BERTopic åˆ†æžï¼ˆå¦‚æžœå¯ç”¨ï¼‰
# ============================================================================
if BERTOPIC_AVAILABLE:
    print("=" * 80)
    print("ðŸ“Š 5. BERTopic ä¸»é¡Œå»ºæ¨¡")
    print("=" * 80)
    print()

    try:
        # æ­£é¢è©•è«– BERTopic
        print("ðŸ” åŸ·è¡Œ BERTopic åˆ†æžï¼ˆæ­£é¢è©•è«–ï¼‰...")
        topic_model_pos = BERTopic(
            language="english",
            calculate_probabilities=True,
            verbose=False,
            nr_topics=5
        )

        topics_pos_bert, probs_pos = topic_model_pos.fit_transform(df_positive['è©•è«–å…§å®¹'].values)

        print("\nðŸ“‹ æ­£é¢è©•è«– - BERTopic çµæžœ:")
        print("-" * 80)
        topic_info_pos = topic_model_pos.get_topic_info()
        print(topic_info_pos.head(10))
        print()

        # è² é¢è©•è«– BERTopic
        print("ðŸ” åŸ·è¡Œ BERTopic åˆ†æžï¼ˆè² é¢è©•è«–ï¼‰...")
        topic_model_neg = BERTopic(
            language="english",
            calculate_probabilities=True,
            verbose=False,
            nr_topics=5
        )

        topics_neg_bert, probs_neg = topic_model_neg.fit_transform(df_negative['è©•è«–å…§å®¹'].values)

        print("\nðŸ“‹ è² é¢è©•è«– - BERTopic çµæžœ:")
        print("-" * 80)
        topic_info_neg = topic_model_neg.get_topic_info()
        print(topic_info_neg.head(10))
        print()

        # BERTopic è¦–è¦ºåŒ–
        try:
            print("ðŸ“Š ç”Ÿæˆ BERTopic è¦–è¦ºåŒ–...")

            # æ­£é¢è©•è«–è¦–è¦ºåŒ–
            fig_pos = topic_model_pos.visualize_topics()
            fig_pos.write_html('topic_modeling_results/bertopic_positive_topics.html')
            print("âœ… æ­£é¢è©•è«– BERTopic è¦–è¦ºåŒ–å·²å„²å­˜: topic_modeling_results/bertopic_positive_topics.html")

            # è² é¢è©•è«–è¦–è¦ºåŒ–
            fig_neg = topic_model_neg.visualize_topics()
            fig_neg.write_html('topic_modeling_results/bertopic_negative_topics.html')
            print("âœ… è² é¢è©•è«– BERTopic è¦–è¦ºåŒ–å·²å„²å­˜: topic_modeling_results/bertopic_negative_topics.html")
            print()
        except Exception as e:
            print(f"âš ï¸  BERTopic è¦–è¦ºåŒ–è­¦å‘Š: {e}")

    except Exception as e:
        print(f"âš ï¸  BERTopic åˆ†æžéŒ¯èª¤: {e}")
        print()

# ============================================================================
# 6. ç”Ÿæˆä¸»é¡Œå»ºæ¨¡å ±å‘Š
# ============================================================================
print("=" * 80)
print("ðŸ“Š 6. ç”Ÿæˆä¸»é¡Œå»ºæ¨¡å ±å‘Š")
print("=" * 80)
print()

report = f"""
# ðŸ” ä¸»é¡Œå»ºæ¨¡åˆ†æžå ±å‘Š (LDA & BERTopic)

**ç”Ÿæˆæ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**è³‡æ–™ä¾†æº**: cleaned_data_no_dedup/final_cleaned_sample_no_dedup.csv
**åˆ†æžæ¨£æœ¬**: {len(df_en):,} æ¢è‹±æ–‡è©•è«–

---

## 1. åˆ†æžæ¦‚è¦½

### è³‡æ–™åˆ†å¸ƒ
- **ç¸½è©•è«–æ•¸**: {len(df):,} æ¢
- **è‹±æ–‡è©•è«–**: {len(df_en):,} æ¢ ({len(df_en)/len(df)*100:.1f}%)
- **æ­£é¢è©•è«–** (4-5æ˜Ÿ): {len(df_positive):,} æ¢
- **è² é¢è©•è«–** (1-2æ˜Ÿ): {len(df_negative):,} æ¢

### åˆ†æžæ–¹æ³•
- **LDA** (Latent Dirichlet Allocation): å‚³çµ±ä¸»é¡Œå»ºæ¨¡æ–¹æ³•
- **BERTopic**: åŸºæ–¼ Transformer çš„ç¾ä»£ä¸»é¡Œå»ºæ¨¡æ–¹æ³•{'ï¼ˆå·²åŸ·è¡Œï¼‰' if BERTOPIC_AVAILABLE else 'ï¼ˆæœªå®‰è£ï¼‰'}

---

## 2. LDA ä¸»é¡Œåˆ†æžçµæžœ

### æ­£é¢è©•è«–ä¸»é¡Œ (4-5æ˜Ÿ)

"""

# æ·»åŠ æ­£é¢è©•è«–ä¸»é¡Œ
for topic_idx, topic_words in enumerate(topics_pos):
    report += f"\n**ä¸»é¡Œ {topic_idx + 1}**\n"
    report += f"- é—œéµè©ž: {', '.join(topic_words[:10])}\n"

report += "\n### è² é¢è©•è«–ä¸»é¡Œ (1-2æ˜Ÿ)\n"

# æ·»åŠ è² é¢è©•è«–ä¸»é¡Œ
for topic_idx, topic_words in enumerate(topics_neg):
    report += f"\n**ä¸»é¡Œ {topic_idx + 1}**\n"
    report += f"- é—œéµè©ž: {', '.join(topic_words[:10])}\n"

report += f"""

---

## 3. ä¸»è¦ç™¼ç¾

### æ­£é¢è©•è«–ä¸»é¡Œç‰¹å¾µ
- ä¸»è¦é—œæ³¨é†«ç™‚å“è³ªã€é†«è­·äººå“¡æ…‹åº¦ã€æ²»ç™‚æ•ˆæžœ
- é—œéµè©žåæ˜ å‡ºå°å°ˆæ¥­æœå‹™çš„æ»¿æ„

### è² é¢è©•è«–ä¸»é¡Œç‰¹å¾µ
- ä¸»è¦é—œæ³¨ç­‰å¾…æ™‚é–“ã€æºé€šå•é¡Œã€æœå‹™æ…‹åº¦
- é—œéµè©žåæ˜ å‡ºå°æœå‹™æµç¨‹çš„ä¸æ»¿

---

## 4. è¼¸å‡ºæª”æ¡ˆ

### LDA åˆ†æž
- `topic_modeling_results/lda_topics_analysis.png` - LDA ä¸»é¡Œåˆ†å¸ƒå’Œé—œéµè©ž

### BERTopic åˆ†æž
"""

if BERTOPIC_AVAILABLE:
    report += """- `topic_modeling_results/bertopic_positive_topics.html` - æ­£é¢è©•è«–ä¸»é¡Œè¦–è¦ºåŒ–
- `topic_modeling_results/bertopic_negative_topics.html` - è² é¢è©•è«–ä¸»é¡Œè¦–è¦ºåŒ–
"""
else:
    report += "- BERTopic æœªå®‰è£ï¼Œè«‹å®‰è£ä»¥ç²å¾—æ›´æ·±å…¥çš„ä¸»é¡Œåˆ†æž\n"

report += """
---

## 5. å»ºè­°èˆ‡çµè«–

### é†«é™¢ç®¡ç†å»ºè­°

**åŸºæ–¼æ­£é¢è©•è«–ä¸»é¡Œ**:
- ç¹¼çºŒä¿æŒé«˜å“è³ªçš„é†«ç™‚æœå‹™
- å¼·åŒ–é†«è­·äººå“¡çš„å°ˆæ¥­åŸ¹è¨“
- ç¶­è­·è‰¯å¥½çš„æºé€šæ©Ÿåˆ¶

**åŸºæ–¼è² é¢è©•è«–ä¸»é¡Œ**:
- å„ªåŒ–é ç´„å’Œç­‰å¾…æµç¨‹
- åŠ å¼·é†«è­·äººå“¡æºé€šæŠ€å·§åŸ¹è¨“
- æ”¹å–„æ•´é«”æœå‹™é«”é©—

---

**å ±å‘Šç”Ÿæˆå®Œæˆ** âœ…
"""

# å„²å­˜å ±å‘Š
report_file = 'topic_modeling_results/TOPIC_MODELING_REPORT.md'
with open(report_file, 'w', encoding='utf-8') as f:
    f.write(report)

print(f"âœ… ä¸»é¡Œå»ºæ¨¡å ±å‘Šå·²å„²å­˜: {report_file}")
print()

# ============================================================================
# å®Œæˆ
# ============================================================================
print("=" * 80)
print("âœ… ä¸»é¡Œå»ºæ¨¡åˆ†æžå®Œæˆï¼")
print("=" * 80)
print()
print("ðŸ“ è¼¸å‡ºæª”æ¡ˆ:")
print("   â€¢ topic_modeling_results/lda_topics_analysis.png - LDA åˆ†æžåœ–")
if BERTOPIC_AVAILABLE:
    print("   â€¢ topic_modeling_results/bertopic_positive_topics.html - æ­£é¢è©•è«– BERTopic")
    print("   â€¢ topic_modeling_results/bertopic_negative_topics.html - è² é¢è©•è«– BERTopic")
print("   â€¢ topic_modeling_results/TOPIC_MODELING_REPORT.md - å®Œæ•´å ±å‘Š")
print()
print("ðŸš€ ä¸‹ä¸€æ­¥: æƒ…æ„Ÿåˆ†æž")
print("=" * 80)
