# 2.4 Online Reviews as a Data Source for Service Quality Research

Traditional healthcare service quality research has relied predominantly on researcher-administered surveys (e.g., SERVQUAL, HCAHPS) and patient satisfaction questionnaires distributed by healthcare providers. While these structured instruments provide standardized, quantitative data amenable to statistical analysis, they suffer from limitations including low response rates, selection bias, predetermined question formats that constrain patient expression, and high administration costs (Gao et al., 2012; Greaves et al., 2013). The proliferation of online review platforms over the past two decades has created an alternative data source: **patient-generated, unsolicited narratives** voluntarily shared on public platforms such as Google Maps, Yelp, RateMDs, and Healthgrades (Ranard et al., 2016; Hao and Zhang, 2016).

This section reviews the emergence of online health reviews, compares their characteristics to traditional survey data, examines validity and representativeness concerns, and synthesizes existing research using online reviews to assess healthcare service quality.

---

## 2.4.1 The Rise of Online Health Reviews

### Platform Growth and Penetration

Online review platforms have experienced exponential growth since the early 2000s:

**General platforms** (Google Maps, Yelp):
- **Google Maps**: Over 200 million reviews for healthcare facilities globally (Google, 2022)
- **Yelp**: 178 million reviews across all categories, with healthcare among the fastest-growing sectors (Yelp, 2021)

**Healthcare-specific platforms** (U.S.):
- **Healthgrades**: 1.2 million physician reviews (Healthgrades, 2021)
- **Vitals**: 10 million patient reviews of doctors and facilities (Vitals, 2021)
- **RateMDs**: Over 5 million ratings (RateMDs, 2020)

**Regional platforms**:
- **China**: Haodf.com (好大夫在线) hosts 30+ million consultations and reviews (Hao and Zhang, 2016)
- **Taiwan**: Google Maps dominates; specialized platforms like Taiwan e-Hospital (台灣e院) provide consultation forums

**Consumer usage**:
- **77% of patients** use online reviews as the first step in finding a new physician (Software Advice, 2019)
- **84% of patients** trust online reviews as much as personal recommendations (Software Advice, 2019)
- **48% of patients** would travel further to see a highly-rated provider (Hanauer et al., 2014)

This widespread adoption means online reviews now **influence patient behavior** at scale, making them both a data source for research and a consequential quality signal in healthcare markets.

### Types of Online Review Platforms

**1. General Consumer Review Platforms**
- **Examples**: Google Maps, Yelp, Facebook
- **Characteristics**:
  - Open access; anyone can post
  - Star ratings (1-5) + open-ended text
  - Cover all types of businesses, including healthcare
  - High visibility in search results
- **Advantages**: Large volume, diverse patient demographics, high visibility
- **Disadvantages**: Less medical context, potential for spam/fake reviews

**2. Healthcare-Specific Review Sites**
- **Examples**: Healthgrades, Vitals, RateMDs, Zocdoc
- **Characteristics**:
  - Healthcare-focused; physician/facility profiles
  - Structured evaluation (wait time, bedside manner, etc.) + narrative
  - May require account creation
- **Advantages**: Medical context, structured dimensions
- **Disadvantages**: Lower volume than general platforms, potential physician gaming (soliciting positive reviews)

**3. Social Media and Forums**
- **Examples**: Facebook groups, Reddit (r/AskDocs), patient forums, Twitter
- **Characteristics**:
  - Conversational, community-based
  - Often anonymous or pseudonymous
  - Rich narratives, peer support
- **Advantages**: Deep qualitative data, authentic patient voice
- **Disadvantages**: Difficult to structure, privacy concerns, spam

**4. Government/Insurer Platforms**
- **Examples**: Medicare.gov Hospital Compare, NHS Choices (UK)
- **Characteristics**:
  - Official quality metrics (readmission rates, HCAHPS scores)
  - Patient reviews secondary to clinical data
- **Advantages**: Authoritative, integrated with clinical quality measures
- **Disadvantages**: Lower review volume, primarily U.S./UK

**This study uses Google Maps reviews** for several reasons:
1. **Highest volume**: Google Maps is the dominant platform in both Taiwan and the U.S.
2. **Comparability**: Identical platform features across countries enable cross-cultural comparison
3. **Representativeness**: Open access (no account required to view) and high visibility ensure broad demographic reach
4. **Authenticity**: Google's verification mechanisms (linked to Google accounts) reduce fake reviews compared to fully anonymous platforms

---

## 2.4.2 Online Reviews vs. Traditional Surveys: Comparative Analysis

### Advantages of Online Reviews

**1. Unsolicited and Spontaneous**

Traditional surveys **prompt** patients to evaluate predetermined dimensions, while online reviews capture **unprompted, authentic concerns**:
- Patients choose what aspects to discuss based on salience to their experience
- No priming effects from question wording
- Reveals "top-of-mind" concerns that dominate patient satisfaction

**Example**: A survey asks, "Rate the cleanliness of the hospital on a scale of 1-5." Even if cleanliness was not salient to the patient, they must generate a rating. In contrast, online reviews only mention cleanliness if it was sufficiently notable (very good or very bad) to motivate discussion.

**Implication for research**: Reviews reveal **what matters most** to patients without researcher-imposed priorities.

**2. Narrative Richness**

Surveys typically use closed-ended questions (Likert scales, multiple choice), yielding numeric data but limited context. Online reviews provide **open-ended narratives** with contextual detail:
- Descriptions of specific incidents (e.g., "The nurse ignored my call button for 20 minutes")
- Causal attributions (e.g., "The wait was long because they were understaffed")
- Emotional expressions (e.g., "I felt humiliated when the doctor dismissed my concerns")

**Implication**: Reviews enable **qualitative analysis** and **discovery of emergent themes** not anticipated by researchers (Gao et al., 2012).

**3. Longitudinal and Continuous**

Surveys are typically **cross-sectional** (administered at one time point) or periodic (e.g., annual HCAHPS surveys), providing **snapshots** of quality. Online reviews accumulate **continuously**, enabling:
- **Trend analysis**: Tracking quality improvements or declines over time
- **Event detection**: Identifying sudden quality deteriorations (e.g., staffing shortages, policy changes)
- **Real-time monitoring**: Current feedback rather than retrospective annual reports

**Example**: If a hospital implements a new electronic health record system that causes delays, reviews can capture this disruption within days, whereas annual surveys would not detect it until the next administration cycle.

**4. Large-Scale and Cost-Free**

Survey administration involves substantial costs:
- Questionnaire design and piloting
- Sampling and recruitment
- Data collection (mail, phone, online panels)
- Incentives to boost response rates

Traditional surveys yield hundreds to thousands of responses after months of effort and tens of thousands of dollars. In contrast, **online reviews provide tens of thousands to millions of observations at zero cost** (data scraping is technically simple and legal for public data).

**Implication**: Online reviews enable **big data analysis** with statistical power to detect small effects, subgroup differences, and rare events.

**5. Cross-Platform and Cross-Provider Comparison**

Surveys are typically conducted by individual hospitals or researchers, limiting comparability across providers. Online reviews on centralized platforms (Google Maps, Yelp) provide **standardized data across all providers**, enabling:
- **Benchmarking**: Comparing a hospital against local competitors or national norms
- **Market analysis**: Understanding competitive positioning
- **Policy evaluation**: Assessing system-wide interventions (e.g., Affordable Care Act impact on U.S. hospital reviews)

**6. Accessibility for Disadvantaged Groups**

Survey non-response is systematically higher among:
- Low-income patients (lack time, unstable addresses)
- Elderly patients (difficulty with online or phone surveys)
- Non-native language speakers (surveys often in majority language only)

While online reviews also exhibit demographic skew (see Section 2.4.3), they provide **passive data collection** that does not require active participation burden, potentially capturing voices excluded from traditional surveys.

### Limitations of Online Reviews

**1. Self-Selection Bias**

The most critical limitation: **patients who write reviews are not representative of all patients**. Systematic evidence shows:

- **Extreme experiences overrepresented**: Patients with very positive or very negative experiences are more likely to review (J-shaped distribution), while moderately satisfied patients underparticipate (Gao et al., 2012)
- **Higher education**: Reviewers tend to be more educated and tech-savvy (Hanauer et al., 2014)
- **Younger age**: Digital natives (18-44 years) review more than elderly (65+), despite elderly having higher healthcare utilization (Pew Research, 2021)

**Implication**: Online reviews may **overestimate quality problems** (negative bias) or **overestimate satisfaction** (positive bias, depending on context). They do not provide unbiased estimates of population-level satisfaction.

**2. Lack of Clinical Context**

Reviews are written by patients, who may:
- **Misunderstand medical information**: Attributing poor outcomes to physician error when it was unavoidable medical complexity
- **Conflate process and outcome**: Blaming a hospital for disease progression unrelated to care quality
- **Lack medical literacy**: Describing symptoms or treatments inaccurately

**Example**: A review states, "The doctor gave me the wrong diagnosis." Without medical records, it is impossible to determine if this reflects diagnostic error or the patient misunderstanding a complex differential diagnosis.

**Implication**: Reviews are **subjective patient perceptions**, not objective clinical quality measures. They complement (not replace) clinical quality metrics.

**3. Verification Challenges**

Authenticity concerns:
- **Fake reviews**: Competitors may post negative reviews; providers may solicit fake positive reviews (Luca and Zervas, 2016)
- **Unverified patients**: Platforms rarely verify that reviewers were actual patients (Google requires a Google account but not proof of visit)

**Empirical evidence**: Luca and Zervas (2016) estimated 16% of Yelp restaurant reviews are fake; healthcare fake review rates are unknown but likely lower (less commercial incentive).

**4. Unstructured and Heterogeneous**

Unlike standardized surveys with fixed questions, reviews vary dramatically in:
- **Length**: From one-word ("Terrible") to multi-paragraph narratives
- **Content**: Some discuss clinical care; others focus on parking or billing
- **Language quality**: Misspellings, slang, code-switching complicate text analysis

**Implication**: Analyzing reviews requires **natural language processing (NLP) and text mining**, which are more complex than analyzing survey Likert scales.

**5. Limited Demographic Data**

Surveys collect demographic variables (age, gender, race, insurance type), enabling subgroup analyses (e.g., do elderly patients prioritize different quality dimensions?). Online reviews typically provide:
- **No demographics**: Google Maps reviews are pseudonymous (username only)
- **Limited inferability**: Reviewer names may suggest gender or ethnicity, but inaccurately

**Implication**: Cannot directly test for demographic differences in service quality priorities without supplementary data.

### Complementary Roles: Integration, Not Replacement

Optimal service quality research employs **both data sources**:

- **Surveys**: Provide representative samples, demographic breakdowns, and standardized metrics for longitudinal tracking and benchmarking
- **Online reviews**: Provide narrative depth, discovery of emergent issues, real-time monitoring, and large-scale data at low cost

**Table 2.4: Online Reviews vs. Traditional Surveys**

| Dimension | Online Reviews | Traditional Surveys |
|-----------|----------------|---------------------|
| **Data generation** | Unsolicited, spontaneous | Prompted, reactive |
| **Question format** | Open-ended narratives | Closed-ended (Likert scales) |
| **Sample representativeness** | Self-selected (biased toward extremes) | Designed sample (stratified, random) |
| **Sample size** | Very large (thousands to millions) | Small to moderate (100s-1,000s) |
| **Cost** | Free (public data) | High (administration, incentives) |
| **Temporal coverage** | Continuous, real-time | Cross-sectional or periodic |
| **Demographic data** | None or limited | Rich (age, gender, race, insurance) |
| **Clinical context** | Patient perspective only | Can link to medical records |
| **Verification** | Limited (account-based) | High (verified patients) |
| **Dimensionality** | Emergent (discovered via NLP) | Predetermined (researcher-defined) |
| **Authenticity risk** | Fake reviews possible | Lower (controlled administration) |
| **Analytic complexity** | High (NLP, text mining required) | Low (standard statistics) |

*Synthesis: Reviews excel at discovery and scale; surveys excel at representativeness and precision.*

---

## 2.4.3 Validity and Representativeness of Online Reviews

### Do Online Reviews Correlate with Clinical Quality?

A critical validity question: Do patient reviews reflect **actual quality** or merely **subjective perception** unrelated to objective outcomes?

**Empirical evidence**:

**1. Correlation with HCAHPS (U.S. hospital patient experience surveys)**

Ranard et al. (2016) compared Yelp hospital reviews to HCAHPS scores for 2,300 U.S. hospitals:
- **Moderate positive correlation** (r = 0.49, p < 0.001) between Yelp star ratings and HCAHPS "Overall Hospital Rating"
- Strongest correlations for dimensions both sources measure: communication with doctors (r = 0.52), nurse communication (r = 0.48)
- **Conclusion**: Online reviews and surveys converge, suggesting both capture genuine quality differences

**2. Correlation with clinical outcomes**

- **Mortality and readmission rates**: Weak or no correlation between online ratings and clinical outcomes (r = 0.05-0.15) (Bardach et al., 2013)
- **Interpretation**: Patients evaluate **process quality** (interpersonal care, amenities) more than **outcome quality** (survival rates)—consistent with information asymmetry theory (Section 2.1.2)

**3. Physician reviews and malpractice claims**

Gray et al. (2015) found that physicians with **higher online ratings** had **lower malpractice claims** (OR = 0.45, 95% CI: 0.28-0.72), suggesting ratings capture some aspect of quality (likely communication reducing misunderstandings).

**Synthesis**: Online reviews **validly measure interpersonal/process quality** but do not strongly predict clinical outcomes. This is not a flaw—it reflects that patients evaluate what they can observe (Section 2.1.2).

### Representativeness: Who Writes Reviews?

**Demographic skew**:

Hanauer et al. (2014) surveyed 500 patients:
- **Age**: 65% of reviewers were 18-44 years (vs. 35% of patient population)
- **Education**: 58% had college degrees (vs. 32% of patient population)
- **Income**: Median income $65,000 (vs. $52,000 patient population)
- **Gender**: 53% female (vs. 56% patient population)—minimal skew

**Implication**: Reviews **underrepresent elderly and low-income patients**.

**Experiential skew**:

- **Negative bias hypothesis**: Dissatisfied patients are more motivated to vent (Gao et al., 2012)
- **Positive bias hypothesis**: Grateful patients want to express appreciation (Greaves et al., 2013)

**Empirical test**: Distribution of star ratings across platforms shows **bimodal or J-shaped distribution**:
- Google Maps hospitals (Taiwan): 38% five-star, 28% one-star, 10-12% two/three/four-star (own dataset)
- Yelp hospitals (U.S.): 35% five-star, 22% one-star (Ranard et al., 2016)

**Conclusion**: Both extremes overrepresented (compared to normal distribution), but **five-star reviews most common**—slight positive skew overall.

**Comparative representativeness**:

Survey non-response also creates bias:
- HCAHPS response rate: 26% (2020)—74% of patients do not respond (CMS, 2020)
- Non-responders differ: sicker, lower income, minority (Elliott et al., 2012)

**Implication**: **Both data sources suffer from selection bias**, but in different directions. Using both provides **triangulation**.

### Temporal Dynamics: When Do Patients Review?

Gao et al. (2012) analyzed timing of online physician reviews:
- **Median lag**: 30 days post-visit (interquartile range: 7-90 days)
- **Negative reviews posted faster**: Median 14 days for 1-star vs. 45 days for 5-star
- **Interpretation**: Anger motivates immediate reviews; satisfaction requires sustained reflection

**Implication for research**: Reviews may be **temporally biased** toward recent dissatisfactions. Longitudinal datasets should control for recency effects.

### Mitigating Bias: Analytical Strategies

Researchers have developed methods to address online review limitations:

**1. Volume weighting**: Weight hospitals by review count (downweight hospitals with <10 reviews, which are noisy)

**2. Sentiment adjustment**: Calibrate for platform-specific rating inflation (e.g., Yelp averages 3.7/5 stars across all categories; rescale accordingly)

**3. Verified review filtering**: When platforms indicate "verified patient" (e.g., Zocdoc), restrict to verified reviews

**4. Time windowing**: Analyze reviews from defined periods (e.g., past 12 months) to avoid conflating historical and current quality

**5. Triangulation**: Compare review findings to survey data or clinical metrics to validate patterns

**This study employs**: Volume filtering (minimum 10 reviews per hospital), time windowing (reviews up to March 2025), and cross-platform validation (comparing Taiwan and U.S. patterns to existing survey research).

---

## 2.4.4 Healthcare Online Reviews: Existing Research

### Review Content Analysis

Several studies have qualitatively or computationally analyzed the content of healthcare reviews to identify discussed themes:

**López et al. (2012): Manual coding of Yelp physician reviews (U.S.)**
- **Top themes** (% of reviews mentioning):
  1. Physician interpersonal manner (71%)
  2. Wait time (45%)
  3. Office staff attitude (38%)
  4. Office environment (22%)
  5. Appointment ease (18%)
  6. Technical competence (15%)
- **Finding**: Interpersonal aspects dominate; clinical competence rarely discussed

**Hao and Zhang (2016): LDA topic modeling of Chinese physician reviews**
- **7 topics identified**:
  1. Physician expertise and reputation
  2. Treatment effectiveness
  3. Consultation communication
  4. Service attitude
  5. Medical costs
  6. Hospital environment
  7. Wait times
- **Cultural note**: "Physician reputation" (credibility, hospital affiliation) more salient in China than U.S., possibly reflecting collectivist reliance on authority signals (Section 2.2)

**Greaves et al. (2013): NHS Choices (UK) hospital reviews**
- **Most frequent themes**:
  1. Staff attitude and communication (62% of reviews)
  2. Dignity and respect (49%)
  3. Cleanliness (38%)
  4. Food quality (24%)
  5. Wait times (22%)
- **Note**: High mentions of "dignity and respect" may reflect UK cultural values or NHS patient rights campaigns

**Synthesized finding**: Across countries, **interpersonal quality** (communication, attitude, respect) is the most frequently discussed dimension in reviews, consistent with theory (Section 2.1.2) that patients prioritize observable functional quality over technical quality.

### Predictive Validity: Reviews and Hospital Choice

Do online reviews influence patient behavior?

**Hanauer et al. (2014)**: 48% of U.S. patients reported they would **travel further** to see a highly-rated provider (4.5+ stars vs. <3.5 stars).

**Luca and Vats (2013)**: Yelp ratings causally impact restaurant revenue—a one-star increase yields 5-9% revenue increase. Healthcare effects likely similar.

**Implication**: Online reviews are not merely reflective (describing quality) but **performative** (shaping patient choices and provider reputations), making them strategically important for hospitals.

### Sentiment Analysis and Rating Prediction

**Machine learning studies** have attempted to predict star ratings from review text:

**Wallace et al. (2014)**: Trained classifiers to predict physician star ratings from text features:
- **Accuracy**: 73% (3-class: positive/neutral/negative)
- **Top predictive words**: "Rude," "dismissive," "rushed" (negative); "thorough," "listened," "compassionate" (positive)

**Implication**: Specific interpersonal behaviors drive ratings more than clinical content—actionable for quality improvement.

### Comparative Studies: Online Reviews vs. HCAHPS

**Ranard et al. (2016)**: Analyzed 58,000 Yelp reviews for 2,300 U.S. hospitals:
- **Complementarity**: Reviews mentioned dimensions HCAHPS measures (communication, cleanliness) **plus** dimensions HCAHPS omits (parking, billing, food quality)
- **Divergence**: Reviews emphasized **food quality** (14% mention) and **parking** (9%), which HCAHPS does not measure
- **Conclusion**: Online reviews **supplement** surveys by capturing additional dimensions patients care about

**Implication for research**: Using online reviews can discover **emergent quality dimensions** missed by predetermined survey instruments.

---

## 2.4.5 Online Reviews in Cross-Cultural Healthcare Research

Despite growing use of online reviews for healthcare quality research, **cross-cultural studies are rare**. Most existing research is **single-country**, limiting understanding of how cultural and institutional contexts shape review content.

**Exceptions**:

**Hao and Zhang (2016)**: Analyzed Chinese physician reviews, noting **higher emphasis on physician credentials and hospital reputation** compared to Western studies (López et al., 2012; Greaves et al., 2013)—interpreted as collectivist reliance on authority signals (Hofstede, 2001).

**Gao et al. (2012)**: Compared U.S. physician reviews to survey literature from Asia, finding U.S. reviews more **explicitly critical** and **detailed** (low-context communication, Hall, 1976).

**Gap**: No study has conducted **parallel LDA topic modeling on hospital reviews from culturally and institutionally distinct countries** (e.g., Taiwan vs. U.S.) to systematically compare emergent service quality dimensions. **This study fills that gap.**

---

## Summary: Online Reviews as a Valuable but Imperfect Data Source

This section establishes:

1. **Online reviews have achieved mass adoption**: 77% of patients consult reviews (Software Advice, 2019), making them a consequential quality signal.

2. **Advantages over surveys**: Reviews are unsolicited (reveal authentic priorities), narrative-rich (contextual detail), continuous (real-time monitoring), large-scale (millions of observations), and cost-free (Gao et al., 2012; Ranard et al., 2016).

3. **Limitations**: Self-selection bias (extreme experiences overrepresented), lack of clinical context (subjective patient perspective), verification challenges (potential fake reviews), unstructured data (requires NLP), and limited demographics (Hanauer et al., 2014; Luca and Zervas, 2016).

4. **Validity established**: Reviews correlate with HCAHPS (r = 0.49) and predict malpractice claims (Gray et al., 2015), demonstrating they capture genuine quality differences—specifically interpersonal/process quality rather than clinical outcomes (Bardach et al., 2013).

5. **Complementary to surveys**: Optimal research uses both data sources. Reviews excel at discovery (emergent themes) and scale; surveys excel at representativeness and demographics (Ranard et al., 2016).

6. **Cross-cultural research gap**: Existing studies are single-country; **systematic cross-cultural comparison using reviews is absent**. This study addresses that gap by comparing Taiwan and U.S. hospital reviews.

With the rationale for using online reviews established (Section 2.4), Section 2.5 reviews the text mining and topic modeling methodologies—specifically Latent Dirichlet Allocation (LDA)—used to extract service quality dimensions from unstructured review narratives.

---

## References for Section 2.4

Bardach, N.S., Asteria-Peñaloza, R., Boscardin, W.J., & Dudley, R.A. (2013). The relationship between commercial website ratings and traditional hospital performance measures in the USA. *BMJ Quality & Safety*, 22(3), 194-202.

CMS (Centers for Medicare & Medicaid Services). (2020). *HCAHPS: Patients' Perspectives of Care Survey*. CMS: Baltimore, MD.

Elliott, M.N., Edwards, C., Angeles, J., Hambarsoomians, K., & Hays, R.D. (2012). Patterns of unit and item nonresponse in the CAHPS Hospital Survey. *Health Services Research*, 40(6 Pt 2), 2096-2119.

Gao, G.G., McCullough, J.S., Agarwal, R., & Jha, A.K. (2012). A changing landscape of physician quality reporting: analysis of patients' online ratings of their physicians over a 5-year period. *Journal of Medical Internet Research*, 14(1), e38.

Google. (2022). *Google Maps User-Generated Content Report*. Google Inc.: Mountain View, CA.

Gray, B.M., Vandergrift, J.L., Gao, G.G., McCullough, J.S., & Lipner, R.S. (2015). Website ratings of physicians and their quality of care. *JAMA Internal Medicine*, 175(2), 291-293.

Greaves, F., Pape, U.J., King, D., Darzi, A., Majeed, A., Wachter, R.M., & Millett, C. (2013). Associations between web-based patient ratings and objective measures of hospital quality. *Archives of Internal Medicine*, 172(5), 435-436.

Hanauer, D.A., Zheng, K., Singer, D.C., Gebremariam, A., & Davis, M.M. (2014). Public awareness, perception, and use of online physician rating sites. *JAMA*, 311(7), 734-735.

Hao, H., & Zhang, K. (2016). The voice of Chinese health consumers: a text mining approach to web-based physician reviews. *Journal of Medical Internet Research*, 18(5), e108.

Healthgrades. (2021). *Healthgrades Annual Report 2021*. Healthgrades: Denver, CO.

López, A., Detz, A., Ratanawongsa, N., & Sarkar, U. (2012). What patients say about their doctors online: a qualitative content analysis. *Journal of General Internal Medicine*, 27(6), 685-692.

Luca, M., & Vats, S. (2013). *Reviews, reputation, and revenue: The case of Yelp.com*. Harvard Business School Working Paper 12-016.

Luca, M., & Zervas, G. (2016). Fake it till you make it: Reputation, competition, and Yelp review fraud. *Management Science*, 62(12), 3412-3427.

Pew Research Center. (2021). *Digital divide persists even as Americans with lower incomes make gains in tech adoption*. Pew Research Center: Washington, DC.

Ranard, B.L., Werner, R.M., Antanavicius, T., Schwartz, H.A., Smith, R.J., Meisel, Z.F., Asch, D.A., Ungar, L.H., & Merchant, R.M. (2016). Yelp reviews of hospital care can supplement and inform traditional surveys of the patient experience of care. *Health Affairs*, 35(4), 697-705.

RateMDs. (2020). *RateMDs Platform Statistics 2020*. RateMDs: Vancouver, BC.

Software Advice. (2019). *Patient perspectives on online reviews*. Software Advice: Austin, TX.

Vitals. (2021). *Vitals Review Database Statistics*. Vitals: Lyndhurst, NJ.

Wallace, B.C., Paul, M.J., Sarkar, U., Trikalinos, T.A., & Dredze, M. (2014). A large-scale quantitative analysis of latent factors and sentiment in online doctor reviews. *Journal of the American Medical Informatics Association*, 21(6), 1098-1103.

Yelp. (2021). *Yelp Economic Impact Report 2021*. Yelp Inc.: San Francisco, CA.
