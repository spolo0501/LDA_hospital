#!/usr/bin/env python3
"""
è‡ªå‹•ä¸‹è¼‰æ–‡ç» PDF
éœ€æ±‚: å·²é€é VPN é€£ç·šåˆ°å­¸æ ¡ç¶²è·¯
"""

import re
import os
import time
import requests
from pathlib import Path
from urllib.parse import quote

# è¨­å®š
PAPERS_DIR = Path(__file__).parent / "pdfs"
PAPERS_DIR.mkdir(exist_ok=True)

# è®€å– RIS æª”æ¡ˆ
def parse_ris_file(ris_path):
    """è§£æ RIS æª”æ¡ˆ"""
    with open(ris_path, 'r', encoding='utf-8') as f:
        content = f.read()

    records = content.split('ER  -\n')
    papers = []

    for record in records:
        if not record.strip():
            continue

        paper = {}

        # æå–è³‡è¨Š
        type_match = re.search(r'^TY  - (.+)$', record, re.MULTILINE)
        title_match = re.search(r'^TI  - (.+)$', record, re.MULTILINE)
        author_matches = re.findall(r'^AU  - (.+)$', record, re.MULTILINE)
        year_match = re.search(r'^PY  - (\d+)$', record, re.MULTILINE)
        journal_match = re.search(r'^JO  - (.+)$', record, re.MULTILINE)
        doi_match = re.search(r'^DO  - (.+)$', record, re.MULTILINE)
        url_match = re.search(r'^UR  - (.+)$', record, re.MULTILINE)
        volume_match = re.search(r'^VL  - (.+)$', record, re.MULTILINE)
        issue_match = re.search(r'^IS  - (.+)$', record, re.MULTILINE)

        if title_match:
            paper['type'] = type_match.group(1) if type_match else 'UNKNOWN'
            paper['title'] = title_match.group(1)
            paper['authors'] = author_matches if author_matches else []
            paper['year'] = year_match.group(1) if year_match else 'Unknown'
            paper['journal'] = journal_match.group(1) if journal_match else ''
            paper['doi'] = doi_match.group(1) if doi_match else ''
            paper['url'] = url_match.group(1) if url_match else ''
            paper['volume'] = volume_match.group(1) if volume_match else ''
            paper['issue'] = issue_match.group(1) if issue_match else ''

            # ç”Ÿæˆæª”æ¡ˆåç¨±
            first_author = author_matches[0].split(',')[0] if author_matches else 'Unknown'
            year = paper['year']
            safe_title = re.sub(r'[^\w\s-]', '', paper['title'][:50])
            safe_title = re.sub(r'[-\s]+', '_', safe_title)
            paper['filename'] = f"{first_author}_{year}_{safe_title}.pdf"

            papers.append(paper)

    return papers


def construct_doi_url(doi):
    """æ§‹å»º DOI URL"""
    if doi:
        return f"https://doi.org/{doi}"
    return None


def construct_scihub_url(doi=None, title=None):
    """æ§‹å»º Sci-Hub URL (å‚™ç”¨æ–¹æ¡ˆ)"""
    if doi:
        return f"https://sci-hub.se/{doi}"
    elif title:
        return f"https://sci-hub.se/{quote(title)}"
    return None


def construct_google_scholar_url(title, author=None, year=None):
    """æ§‹å»º Google Scholar æœå°‹ URL"""
    query = title
    if author:
        query += f" {author}"
    if year:
        query += f" {year}"
    return f"https://scholar.google.com/scholar?q={quote(query)}"


def download_via_doi(paper, session):
    """é€é DOI ä¸‹è¼‰"""
    if not paper.get('doi'):
        return False

    doi_url = construct_doi_url(paper['doi'])
    print(f"  å˜—è©¦é€é DOI: {doi_url}")

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        response = session.get(doi_url, headers=headers, timeout=10, allow_redirects=True)

        # æª¢æŸ¥æ˜¯å¦ç‚º PDF
        if 'application/pdf' in response.headers.get('Content-Type', ''):
            filepath = PAPERS_DIR / paper['filename']
            with open(filepath, 'wb') as f:
                f.write(response.content)
            print(f"  âœ… æˆåŠŸä¸‹è¼‰!")
            return True
        else:
            print(f"  â„¹ï¸  é‡å®šå‘åˆ°: {response.url}")
            # ä¿å­˜é‡å®šå‘ URL ä¾›å¾ŒçºŒä½¿ç”¨
            paper['publisher_url'] = response.url
            return False
    except Exception as e:
        print(f"  âŒ å¤±æ•—: {e}")
        return False


def generate_download_list(papers):
    """ç”Ÿæˆä¸‹è¼‰æ¸…å–® (ä¾›æ‰‹å‹•ä¸‹è¼‰)"""
    list_file = PAPERS_DIR / "_download_list.txt"

    with open(list_file, 'w', encoding='utf-8') as f:
        f.write("# æ–‡ç»ä¸‹è¼‰æ¸…å–®\n")
        f.write("# å»ºè­°é€éå­¸æ ¡åœ–æ›¸é¤¨ä»£ç†æˆ– Google Scholar ä¸‹è¼‰\n\n")

        for i, paper in enumerate(papers, 1):
            f.write(f"\n{'='*80}\n")
            f.write(f"[{i}/{len(papers)}] {paper['filename']}\n")
            f.write(f"{'='*80}\n\n")

            # åŸºæœ¬è³‡è¨Š
            f.write(f"Title: {paper['title']}\n")
            if paper['authors']:
                f.write(f"Authors: {', '.join(paper['authors'][:3])}")
                if len(paper['authors']) > 3:
                    f.write(f" et al.")
                f.write("\n")
            f.write(f"Year: {paper['year']}\n")
            if paper['journal']:
                f.write(f"Journal: {paper['journal']}\n")
            if paper['volume']:
                f.write(f"Volume: {paper['volume']}\n")
            if paper['issue']:
                f.write(f"Issue: {paper['issue']}\n")

            f.write("\nä¸‹è¼‰é€£çµ:\n")

            # DOI é€£çµ
            if paper.get('doi'):
                f.write(f"  DOI: {construct_doi_url(paper['doi'])}\n")

            # Google Scholar é€£çµ
            first_author = paper['authors'][0].split(',')[0] if paper['authors'] else None
            scholar_url = construct_google_scholar_url(paper['title'], first_author, paper['year'])
            f.write(f"  Google Scholar: {scholar_url}\n")

            # Sci-Hub é€£çµ (å‚™ç”¨)
            if paper.get('doi'):
                scihub_url = construct_scihub_url(doi=paper['doi'])
            else:
                scihub_url = construct_scihub_url(title=paper['title'])
            f.write(f"  Sci-Hub (å‚™ç”¨): {scihub_url}\n")

            # å‡ºç‰ˆå•†é€£çµ
            if paper.get('publisher_url'):
                f.write(f"  Publisher: {paper['publisher_url']}\n")

            f.write("\n")

    print(f"\nğŸ“ ä¸‹è¼‰æ¸…å–®å·²ç”Ÿæˆ: {list_file}")
    return list_file


def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 80)
    print("æ–‡ç»è‡ªå‹•ä¸‹è¼‰ç¨‹å¼")
    print("=" * 80)

    # è§£æ RIS æª”æ¡ˆ
    ris_file = Path(__file__).parent / "all_references.ris"
    print(f"\nğŸ“– è®€å–æ–‡ç»æ¸…å–®: {ris_file}")
    papers = parse_ris_file(ris_file)

    # åªè™•ç†æœŸåˆŠè«–æ–‡
    journal_papers = [p for p in papers if p['type'] == 'JOUR']
    print(f"âœ… æ‰¾åˆ° {len(journal_papers)} ç¯‡æœŸåˆŠè«–æ–‡")

    # å»ºç«‹ session
    session = requests.Session()

    # çµ±è¨ˆ
    success_count = 0
    failed_papers = []

    print(f"\né–‹å§‹ä¸‹è¼‰åˆ°: {PAPERS_DIR}\n")

    # å˜—è©¦ä¸‹è¼‰
    for i, paper in enumerate(journal_papers, 1):
        print(f"[{i}/{len(journal_papers)}] {paper['filename']}")
        print(f"  Title: {paper['title'][:60]}...")

        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
        filepath = PAPERS_DIR / paper['filename']
        if filepath.exists():
            print(f"  â­ï¸  å·²å­˜åœ¨ï¼Œè·³é")
            success_count += 1
            continue

        # å˜—è©¦é€é DOI ä¸‹è¼‰
        if download_via_doi(paper, session):
            success_count += 1
        else:
            failed_papers.append(paper)

        # é¿å…è«‹æ±‚éå¿«
        time.sleep(1)

    # ç”Ÿæˆä¸‹è¼‰æ¸…å–®
    print("\n" + "=" * 80)
    print("ä¸‹è¼‰å®Œæˆ!")
    print("=" * 80)
    print(f"âœ… æˆåŠŸä¸‹è¼‰: {success_count} ç¯‡")
    print(f"â¸ï¸  éœ€è¦æ‰‹å‹•ä¸‹è¼‰: {len(failed_papers)} ç¯‡")

    if failed_papers:
        print(f"\næ­£åœ¨ç”Ÿæˆä¸‹è¼‰æ¸…å–®...")
        list_file = generate_download_list(failed_papers)
        print(f"\nğŸ’¡ æç¤º: è«‹é–‹å•Ÿ {list_file.name} æŸ¥çœ‹ä¸‹è¼‰é€£çµ")
        print(f"   å»ºè­°é€éå­¸æ ¡åœ–æ›¸é¤¨ç¶²ç«™æˆ– Google Scholar ä¸‹è¼‰")

    print(f"\nğŸ“ PDF å„²å­˜ä½ç½®: {PAPERS_DIR}")


if __name__ == "__main__":
    main()
