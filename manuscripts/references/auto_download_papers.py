#!/usr/bin/env python3
"""
æ™ºèƒ½æ‰¹æ¬¡ä¸‹è¼‰æ–‡ç» - ä½¿ç”¨å­¸æ ¡ VPN
ç‰¹é»ï¼š
1. é€é Google Scholar æ‰¾ PDF
2. æ”¯æ´å¤šç¨®ä¸‹è¼‰æº
3. è‡ªå‹•é‡å‘½åæª”æ¡ˆ
4. ç”Ÿæˆä¸‹è¼‰å ±å‘Š
"""

import json
import time
import requests
from pathlib import Path
from urllib.parse import quote
import subprocess

# è¨­å®š
BASE_DIR = Path(__file__).parent
PAPERS_DIR = BASE_DIR / "pdfs"
PAPERS_DIR.mkdir(exist_ok=True)

PRIORITY_LIST = BASE_DIR / "priority_papers.json"
DOWNLOAD_REPORT = PAPERS_DIR / "download_report.txt"

# User Agent
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

def load_priority_papers():
    """è¼‰å…¥å„ªå…ˆä¸‹è¼‰æ¸…å–®"""
    with open(PRIORITY_LIST, 'r', encoding='utf-8') as f:
        return json.load(f)

def open_in_browser(url):
    """åœ¨ç€è¦½å™¨ä¸­é–‹å•Ÿ URL"""
    subprocess.run(['open', url], check=False)

def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 80)
    print("æ™ºèƒ½æ‰¹æ¬¡ä¸‹è¼‰æ–‡ç»")
    print("=" * 80)

    # è¼‰å…¥æ¸…å–®
    papers = load_priority_papers()
    print(f"\nğŸ“š å„ªå…ˆä¸‹è¼‰æ¸…å–®ï¼š{len(papers)} ç¯‡æ ¸å¿ƒæ–‡ç»\n")

    downloaded = []
    failed = []

    # æª¢æŸ¥å·²ä¸‹è¼‰
    print("æª¢æŸ¥å·²ä¸‹è¼‰çš„æ–‡ç»...")
    existing_files = list(PAPERS_DIR.glob("*.pdf"))
    print(f"âœ… å·²æœ‰ {len(existing_files)} ç¯‡ PDF\n")

    for paper in papers:
        priority = paper['priority']
        title = paper['title']
        filename = paper['filename']
        filepath = PAPERS_DIR / filename

        print(f"\n{'='*80}")
        print(f"[{priority}/10] {filename}")
        print(f"{'='*80}")
        print(f"Title: {title[:60]}...")

        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if filepath.exists():
            print("  âœ… å·²å­˜åœ¨ï¼Œè·³é")
            downloaded.append(paper)
            continue

        # æ§‹å»º Google Scholar æœå°‹ URL
        if 'doi' in paper:
            search_url = f"https://scholar.google.com/scholar?q={quote(paper['doi'])}"
        else:
            search_url = f"https://scholar.google.com/scholar?q={quote(paper['search_query'])}"

        print(f"\n  ğŸ” é–‹å•Ÿ Google Scholar æœå°‹...")
        print(f"  URL: {search_url}")

        # åœ¨ç€è¦½å™¨ä¸­é–‹å•Ÿ
        open_in_browser(search_url)

        print(f"\n  ğŸ’¡ è«‹åœ¨ç€è¦½å™¨ä¸­ï¼š")
        print(f"     1. æ‰¾åˆ°æ­£ç¢ºçš„æ–‡ç« ")
        print(f"     2. é»æ“Š [PDF] é€£çµä¸‹è¼‰")
        print(f"     3. ä¸‹è¼‰å®Œæˆå¾Œï¼ŒæŒ‰ Enter ç¹¼çºŒ...")

        input()

        # æª¢æŸ¥ Downloads è³‡æ–™å¤¾ä¸­æœ€æ–°çš„ PDF
        downloads_dir = Path.home() / "Downloads"
        pdf_files = sorted(downloads_dir.glob("*.pdf"), key=lambda x: x.stat().st_mtime, reverse=True)

        if pdf_files:
            latest_pdf = pdf_files[0]
            print(f"\n  ğŸ“„ æ‰¾åˆ°æœ€æ–°ä¸‹è¼‰ï¼š{latest_pdf.name}")

            # ç¢ºèªæ˜¯å¦ç‚ºæ­£ç¢ºçš„æ–‡ä»¶
            confirm = input(f"  æ˜¯å¦ç‚º {filename}ï¼Ÿ(y/n): ").lower()

            if confirm == 'y':
                # ç§»å‹•ä¸¦é‡å‘½å
                latest_pdf.rename(filepath)
                print(f"  âœ… å·²å„²å­˜ç‚ºï¼š{filename}")
                downloaded.append(paper)
            else:
                print(f"  â­ï¸  è·³éæ­¤æ–‡ä»¶")
                failed.append(paper)
        else:
            print(f"  âŒ Downloads è³‡æ–™å¤¾ä¸­æ²’æœ‰æ‰¾åˆ° PDF")
            failed.append(paper)

        # é¿å…éå¿«è«‹æ±‚
        time.sleep(2)

    # ç”Ÿæˆå ±å‘Š
    print(f"\n{'='*80}")
    print("ä¸‹è¼‰å®Œæˆå ±å‘Š")
    print(f"{'='*80}\n")

    print(f"âœ… æˆåŠŸä¸‹è¼‰ï¼š{len(downloaded)} ç¯‡")
    print(f"âŒ å¤±æ•—/è·³éï¼š{len(failed)} ç¯‡")

    # å¯«å…¥å ±å‘Š
    with open(DOWNLOAD_REPORT, 'w', encoding='utf-8') as f:
        f.write("æ–‡ç»ä¸‹è¼‰å ±å‘Š\n")
        f.write("=" * 80 + "\n\n")

        f.write(f"ä¸‹è¼‰æ™‚é–“ï¼š{time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ç¸½æ•¸ï¼š{len(papers)} ç¯‡\n")
        f.write(f"æˆåŠŸï¼š{len(downloaded)} ç¯‡\n")
        f.write(f"å¤±æ•—ï¼š{len(failed)} ç¯‡\n\n")

        if downloaded:
            f.write("æˆåŠŸä¸‹è¼‰æ¸…å–®ï¼š\n")
            f.write("-" * 80 + "\n")
            for paper in downloaded:
                f.write(f"[{paper['priority']}] {paper['filename']}\n")
                f.write(f"    {paper['title']}\n\n")

        if failed:
            f.write("\néœ€è¦æ‰‹å‹•ä¸‹è¼‰ï¼š\n")
            f.write("-" * 80 + "\n")
            for paper in failed:
                f.write(f"[{paper['priority']}] {paper['filename']}\n")
                f.write(f"    {paper['title']}\n")
                if 'doi' in paper:
                    f.write(f"    DOI: {paper['doi']}\n")
                f.write(f"    æœå°‹ï¼š{paper.get('search_query', '')}\n\n")

    print(f"\nğŸ“ å ±å‘Šå·²å„²å­˜ï¼š{DOWNLOAD_REPORT}")
    print(f"\nğŸ“ PDF ä½ç½®ï¼š{PAPERS_DIR}")

    if failed:
        print(f"\nğŸ’¡ æç¤ºï¼š{len(failed)} ç¯‡æ–‡ç»éœ€è¦æ‰‹å‹•ä¸‹è¼‰")
        print(f"   è«‹æŸ¥çœ‹ _download_list.txt ç²å–ä¸‹è¼‰é€£çµ")

if __name__ == "__main__":
    main()
